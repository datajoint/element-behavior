{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Element DeepLabCut for Pose Estimation", "text": "<p>DataJoint Element for markerless pose estimation with DeepLabCut.  DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments.  Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline.</p> <p></p> <p>Element DeepLabCut runs DeepLabCut which uses image recognition machine learning models to generate animal position estimates from consumer grade video equipment.  The Element is composed of two schemas for storing data and running analysis:</p> <ul> <li><code>train</code> - Manages model training</li> </ul> <ul> <li><code>model</code> - Manages models and launches pose estimation</li> </ul> <p>Visit the Concepts page for more information on pose estimation and Element DeepLabCut.  To get started with building your data pipeline visit the Tutorials page.</p>"}, {"location": "changelog/", "title": "Changelog", "text": "<p>Observes Semantic Versioning standard and  Keep a Changelog convention.</p>"}, {"location": "changelog/#028-2023-08-07", "title": "0.2.8 - 2023-08-07", "text": "<ul> <li>Update - GitHub Actions with new reusable workflows</li> <li>Update - Readme instructions</li> </ul>"}, {"location": "changelog/#027-2023-08-04", "title": "0.2.7 - 2023-08-04", "text": "<ul> <li>Fix - Update the project path in the pose config file to train the model</li> </ul>"}, {"location": "changelog/#026-2023-05-22", "title": "0.2.6 - 2023-05-22", "text": "<ul> <li>Add - DeepLabCut, NWB, and DANDI citations</li> <li>Update - mkdocs.yaml</li> </ul>"}, {"location": "changelog/#025-2023-05-11", "title": "0.2.5 - 2023-05-11", "text": "<ul> <li>Fix - <code>.ipynb</code> dark mode output for all notebooks.</li> <li>Fix - Remove <code>GOOGLE_ANALYTICS_KEY</code> from <code>u24_element_release_call.yml</code>.</li> </ul>"}, {"location": "changelog/#024-2023-04-28", "title": "0.2.4 - 2023-04-28", "text": "<ul> <li>Fix - <code>.ipynb</code> output in tutorials is not visible in dark mode.</li> </ul>"}, {"location": "changelog/#023-2023-02-28", "title": "0.2.3 - 2023-02-28", "text": "<ul> <li>Fix - For cases of multiple subjects/sessions with same recording_id</li> <li>Update - Use posix to handle DLC project path</li> </ul>"}, {"location": "changelog/#022-2023-01-17", "title": "0.2.2 - 2023-01-17", "text": "<ul> <li>Fix - improve function to auto generate PoseEstimationTask</li> <li>Update - loading DLC results handles multiple DLC output files</li> <li>Update - Adopt lazy import strategy for imported DeepLabCut functions</li> </ul>"}, {"location": "changelog/#021-2022-12-16", "title": "0.2.1 - 2022-12-16", "text": "<ul> <li>Update - Requirements to accommodate DLC package requirements changing</li> <li>Fix - Typos in docstrings</li> <li>Update - Docstrings for mkdocs deployment</li> <li>Update - Doc website styling including logos, navigation, social icons</li> <li>Add - Flow diagram in svg and drawio formats</li> </ul>"}, {"location": "changelog/#020-2022-10-10", "title": "0.2.0 - 2022-10-10", "text": "<ul> <li>Update - Remove direct dependency (<code>element-interface</code>) for PyPI release.</li> <li>Update - Docstring PEP257 compliance #24 </li> <li>Update - Explicit handling of KeyboardInterrupt #26</li> <li>Update - Streamline insert_new_params #27</li> <li>Update - Relocate module imports to the top of the files</li> <li>Update - Missing f for formatted string in read_yaml</li> <li>Change - Rename datajoint-saved config to <code>dj_dlc_config.yaml</code></li> <li>Add - Call reusable CICD</li> <li>Add - NWB export</li> <li>Add - mkdocs deployment with workflow API docs</li> </ul>"}, {"location": "changelog/#011-2022-06-10", "title": "0.1.1 - 2022-06-10", "text": "<ul> <li>Fix - Replace lazy imports</li> <li>Fix - Project path in the model.Model</li> </ul>"}, {"location": "changelog/#010-2022-05-10", "title": "0.1.0 - 2022-05-10", "text": "<ul> <li>Add - Adopted black formatting into code base</li> <li>Add - Table for RecordingInfo</li> <li>Add - File ID for tracking updatable secondary key filepaths</li> <li>Add - <code>make</code> functions for Computed/Imported tables</li> </ul>"}, {"location": "changelog/#000a-2021-11-15", "title": "0.0.0a - 2021-11-15", "text": "<ul> <li>Add - Drafts from a collection of precursor pipelines, including   DataJoint_Demo_DeepLabCut   graciously provided by the Mathis Lab.</li> <li>Add - Support for 2d single-animal models</li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses the following resources, please cite the respective manuscript and/or Research Resource Identifier (RRID):</p> <ul> <li>DataJoint Element DeepLabCut - Version 0.2.8      + Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D,      Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for      Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358<p>+ RRID:SCR_021894</p> </li> </ul> <ul> <li>DeepLabCut       + Manuscripts</li> </ul> <ul> <li>NWB<ul> <li>Manuscript</li> </ul> </li> </ul> <ul> <li>DANDI<ul> <li>Citation options</li> </ul> </li> </ul>"}, {"location": "concepts/", "title": "Concepts", "text": ""}, {"location": "concepts/#pose-estimation-in-neurophysiology", "title": "Pose Estimation in Neurophysiology", "text": "<p>Studying the inner workings of the brain requires understanding the relationship between neural activity and environmental stimuli, natural behavior, or inferred cognitive states. Pose estimation is a computer vision method to track the position, and thereby behavior, of the subject over the course of an experiment, which can then be paired with neuronal recordings to answer scientific questions about the brain.</p> <p>Previous pose estimation methods required reflective markers placed on a subject, as well as multiple expensive high-frame-rate infrared cameras to triangulate position within a limited field. Recent advancements in machine learning have facilitated dramatic advancements in capturing pose data with a video camera alone. In particular, DeepLabCut (DLC) facilitates the use of pre-trained machine learning models for 2-D and 3-D non-invasive markerless pose estimation.</p> <p>DeepLabCut offers the ability to continue training an exisiting object detection model to further specialize in videos in the training data set. In other words, researchers can take a well-known generalizable machine learning model and apply it to their experimental setup, making it relatively easy to produce pose estimation inferences for subsequent experimental sessions.</p> <p>While some alternative tools are either species-specific (e.g., DeepFly3D) or uniquely 2D (e.g., DeepPoseKit), DLC highlights a diversity of use-cases via a Model Zoo. Even compared to tools with similar functionality (e.g., SLEAP and dannce), DLC has more users, as measured by either GitHub forks or more citations (1600 vs. 900). DLC's trajectory toward an industry standard is attributable to continued funding, extensive documentation and both creator- and peer-support. Other comparable tools include mmpose, idtracker.ai, TREBA, B-KinD, VAME, and MARS.</p>"}, {"location": "concepts/#key-partnerships", "title": "Key Partnerships", "text": "<p>Mackenzie Mathis (Swiss Federal Institute of Technology Lausanne) is both a lead developer of DLC and a key advisor on DataJoint open source development as a member of the Scientific Steering Committee.</p> <p>DataJoint is also partnered with a number of groups who use DLC as part of broader workflows. In these collaborations, members of the DataJoint team have interviewed the scientists to understand their needs in experimental setup, pipeline design, and interfaces.</p> <p>These teams include:</p> <ul> <li>Moser Group (Norwegian University of Science and Technology) - see pipeline   design</li> </ul> <ul> <li>Mesoscale Activity Project (Janelia Research Campus/Baylor College of Medicine/New   York University)</li> </ul> <ul> <li>Hui-Chen Lu Lab (Indiana University)</li> </ul> <ul> <li>Tobias Rose Lab (University of Bonn)</li> </ul> <ul> <li>James Cotton Lab (Northwestern University)</li> </ul>"}, {"location": "concepts/#element-features", "title": "Element Features", "text": "<p>Development of the Element began with an  open source repository shared by the Mathis team. We further identified common needs across our respective partnerships to offer the following features for single-camera 2D models:</p> <ul> <li>Manage training data and configuration parameters</li> <li>Launch model training</li> <li>Evaluate models automatically and directly compare models</li> <li>Manage model metadata</li> <li>Launch inference video analysis</li> <li>Capture pose estimation output for each session</li> </ul>"}, {"location": "concepts/#element-architecture", "title": "Element Architecture", "text": "<p>Each node in the following diagram represents the analysis code in the workflow and the corresponding tables in the database.  Within the workflow, Element DeepLabCut connects to upstream Elements including Lab, Animal, and Session.  For more detailed documentation on each table, see the API docs for the respective schemas.</p> <p></p>"}, {"location": "concepts/#lab-schema-api-docs", "title": "<code>lab</code> schema (API docs)", "text": "Table Description Device Camera metadata"}, {"location": "concepts/#subject-schema-api-docs", "title": "<code>subject</code> schema (API docs)", "text": "<ul> <li>Although not required, most choose to connect the <code>Session</code> table to a <code>Subject</code> table.</li> </ul> Table Description Subject Basic information of the research subject"}, {"location": "concepts/#session-schema-api-docs", "title": "<code>session</code> schema (API docs)", "text": "Table Description Session Unique experimental session identifier"}, {"location": "concepts/#train-schema-api-docs", "title": "<code>train</code> schema (API docs)", "text": "<ul> <li>Optional tables related to model training.</li> </ul> Table Description VideoSet Set of files corresponding to a training dataset. TrainingParamSet A collection of model training parameters, represented by an index. TrainingTask A set of tasks specifying model training methods. ModelTraining A record of training iterations launched by <code>TrainingTask</code>."}, {"location": "concepts/#model-schema-api", "title": "<code>model</code> schema (API)", "text": "<ul> <li>Tables related to DeepLabCut models and pose estimation. The <code>model</code> schema can be   used without the <code>train</code> schema.</li> </ul> Table Description VideoRecording Video(s) from one recording session, for pose estimation. BodyPart Unique body parts (a.k.a. joints) and descriptions thereof. Model A central table for storing unique models. ModelEvaluation Evaluation results for each model. PoseEstimationTask A series of pose estimation tasks to be completed. Pairings of video recordings with models to be use for pose estimation. PoseEstimation Results of pose estimation using a given model."}, {"location": "concepts/#data-export-and-publishing", "title": "Data Export and Publishing", "text": "<p>Element DeepLabCut includes an export function that saves the outputs as a Neurodata Without Borders (NWB) file.  By running a single command, the data from an experimental session is saved to a NWB file.</p> <p>For more details on the export function, see the Tutorials page.</p> <p>Once NWB files are generated they can be readily shared with collaborators and published on DANDI Archive.  The DataJoint Elements ecosystem includes a function to upload the NWB files to DANDI (see Element Interface).</p> <pre><code>dlc_session_to_nwb(pose_key, use_element_session, session_kwargs)\n</code></pre>"}, {"location": "concepts/#roadmap", "title": "Roadmap", "text": "<p>Further development of this Element is community driven.  Upon user requests and based on guidance from the Scientific Steering Group we will add the following features to this Element:</p> <ul> <li>Support for multi-animal or multi-camera models</li> <li>Tools to label training data</li> </ul>"}, {"location": "api/element_deeplabcut/model/", "title": "model.py", "text": "<p>Code adapted from the Mathis Lab MIT License Copyright (c) 2022 Mackenzie Mathis DataJoint Schema for DeepLabCut 2.x, Supports 2D and 3D DLC via triangulation.</p>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.activate", "title": "<code>activate(model_schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>model_schema_name</code> <code>str</code> <p>schema name on the database server</p> required <code>create_schema</code> <code>bool</code> <p>when True (default), create schema in the database if it                 does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>when True (default), create schema tables in the database                  if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>a module (or name) containing the required dependencies.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to VideoRecording, identifying a recording session. Equipment: A parent table to VideoRecording, identifying a recording device.</p> Functions <p>get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies)                          with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed                               data. Defaults to session video subfolder.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>def activate(\n    model_schema_name: str,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: bool = None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        model_schema_name (str): schema name on the database server\n        create_schema (bool): when True (default), create schema in the database if it\n                            does not yet exist.\n        create_tables (bool): when True (default), create schema tables in the database\n                             if they do not yet exist.\n        linking_module (str): a module (or name) containing the required dependencies.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to VideoRecording, identifying a recording session.\n        Equipment: A parent table to VideoRecording, identifying a recording device.\n    Functions:\n        get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies)\n                                 with all behavioral recordings, as (list of) string(s).\n        get_dlc_processed_data_dir(): Optional. Returns absolute path for processed\n                                      data. Defaults to session video subfolder.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n    assert hasattr(\n        linking_module, \"get_dlc_root_data_dir\"\n    ), \"The linking module must specify a lookup function for a root data directory\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    # activate\n    schema.activate(\n        model_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.get_dlc_root_data_dir", "title": "<code>get_dlc_root_data_dir()</code>", "text": "<p>Pulls relevant func from parent namespace to specify root data dir(s).</p> <p>It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strings for possible root data directories.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>def get_dlc_root_data_dir() -&gt; list:\n\"\"\"Pulls relevant func from parent namespace to specify root data dir(s).\n\n    It is recommended that all paths in DataJoint Elements stored as relative\n    paths, with respect to some user-configured \"root\" director(y/ies). The\n    root(s) may vary between data modalities and user machines. Returns a full path\n    string or list of strings for possible root data directories.\n    \"\"\"\n    root_directories = _linking_module.get_dlc_root_data_dir()\n    if isinstance(root_directories, (str, Path)):\n        root_directories = [root_directories]\n\n    if (\n        hasattr(_linking_module, \"get_dlc_processed_data_dir\")\n        and get_dlc_processed_data_dir() not in root_directories\n    ):\n        root_directories.append(_linking_module.get_dlc_processed_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.get_dlc_processed_data_dir", "title": "<code>get_dlc_processed_data_dir()</code>", "text": "<p>Pulls relevant func from parent namespace. Defaults to DLC's project /videos/.</p> <p>Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>def get_dlc_processed_data_dir() -&gt; Optional[str]:\n\"\"\"Pulls relevant func from parent namespace. Defaults to DLC's project /videos/.\n\n    Method in parent namespace should provide a string to a directory where DLC output\n    files will be stored. If unspecified, output files will be stored in the\n    session directory 'videos' folder, per DeepLabCut default.\n    \"\"\"\n    if hasattr(_linking_module, \"get_dlc_processed_data_dir\"):\n        return _linking_module.get_dlc_processed_data_dir()\n    else:\n        return None\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.VideoRecording", "title": "<code>VideoRecording</code>", "text": "<p>             Bases: <code>dj.Manual</code></p> <p>Set of video recordings for DLC inferences.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Session primary key.</p> <code>recording_id</code> <code>int</code> <p>Unique recording ID.</p> <code>Device</code> <code>foreign key</code> <p>Device table primary key, used for default output directory path information.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@schema\nclass VideoRecording(dj.Manual):\n\"\"\"Set of video recordings for DLC inferences.\n\n    Attributes:\n        Session (foreign key): Session primary key.\n        recording_id (int): Unique recording ID.\n        Device (foreign key): Device table primary key, used for default output\n            directory path information.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Session\n    recording_id: int\n    ---\n    -&gt; Device\n    \"\"\"\n\n    class File(dj.Part):\n\"\"\"File IDs and paths associated with a given recording_id\n\n        Attributes:\n            VideoRecording (foreign key): Video recording primary key.\n            file_path ( varchar(255) ): file path of video, relative to root data dir.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        file_id: int\n        ---\n        file_path: varchar(255)  # filepath of video, relative to root data directory\n        \"\"\"\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.VideoRecording.File", "title": "<code>File</code>", "text": "<p>             Bases: <code>dj.Part</code></p> <p>File IDs and paths associated with a given recording_id</p> <p>Attributes:</p> Name Type Description <code>VideoRecording</code> <code>foreign key</code> <p>Video recording primary key.</p> <code>file_path</code> <code> varchar(255) </code> <p>file path of video, relative to root data dir.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>class File(dj.Part):\n\"\"\"File IDs and paths associated with a given recording_id\n\n    Attributes:\n        VideoRecording (foreign key): Video recording primary key.\n        file_path ( varchar(255) ): file path of video, relative to root data dir.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    file_id: int\n    ---\n    file_path: varchar(255)  # filepath of video, relative to root data directory\n    \"\"\"\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.RecordingInfo", "title": "<code>RecordingInfo</code>", "text": "<p>             Bases: <code>dj.Imported</code></p> <p>Automated table with video file metadata.</p> <p>Attributes:</p> Name Type Description <code>VideoRecording</code> <code>foreign key</code> <p>Video recording key.</p> <code>px_height</code> <code>smallint</code> <p>Height in pixels.</p> <code>px_width</code> <code>smallint</code> <p>Width in pixels.</p> <code>nframes</code> <code>int</code> <p>Number of frames.</p> <code>fps</code> <code>int</code> <p>Optional. Frames per second, Hz.</p> <code>recording_datetime</code> <code>datetime</code> <p>Optional. Datetime for the start of recording.</p> <code>recording_duration</code> <code>float</code> <p>video duration (s) from nframes / fps.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@schema\nclass RecordingInfo(dj.Imported):\n\"\"\"Automated table with video file metadata.\n\n    Attributes:\n        VideoRecording (foreign key): Video recording key.\n        px_height (smallint): Height in pixels.\n        px_width (smallint): Width in pixels.\n        nframes (int): Number of frames.\n        fps (int): Optional. Frames per second, Hz.\n        recording_datetime (datetime): Optional. Datetime for the start of recording.\n        recording_duration (float): video duration (s) from nframes / fps.\"\"\"\n\n    definition = \"\"\"\n    -&gt; VideoRecording\n    ---\n    px_height                 : smallint  # height in pixels\n    px_width                  : smallint  # width in pixels\n    nframes                   : int  # number of frames \n    fps = NULL                : int       # (Hz) frames per second\n    recording_datetime = NULL : datetime  # Datetime for the start of the recording\n    recording_duration        : float     # video duration (s) from nframes / fps\n    \"\"\"\n\n    @property\n    def key_source(self):\n\"\"\"Defines order of keys for make function when called via `populate()`\"\"\"\n        return VideoRecording &amp; VideoRecording.File\n\n    def make(self, key):\n\"\"\"Populates table with video metadata using CV2.\"\"\"\n        file_paths = (VideoRecording.File &amp; key).fetch(\"file_path\")\n\n        nframes = 0\n        px_height, px_width, fps = None, None, None\n\n        for file_path in file_paths:\n            file_path = (find_full_path(get_dlc_root_data_dir(), file_path)).as_posix()\n\n            cap = cv2.VideoCapture(file_path)\n            info = (\n                int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n                int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n                int(cap.get(cv2.CAP_PROP_FPS)),\n            )\n            if px_height is not None:\n                assert (px_height, px_width, fps) == info\n            px_height, px_width, fps = info\n            nframes += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            cap.release()\n\n        self.insert1(\n            {\n                **key,\n                \"px_height\": px_height,\n                \"px_width\": px_width,\n                \"nframes\": nframes,\n                \"fps\": fps,\n                \"recording_duration\": nframes / fps,\n            }\n        )\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.RecordingInfo.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Defines order of keys for make function when called via <code>populate()</code></p>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.RecordingInfo.make", "title": "<code>make(key)</code>", "text": "<p>Populates table with video metadata using CV2.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>def make(self, key):\n\"\"\"Populates table with video metadata using CV2.\"\"\"\n    file_paths = (VideoRecording.File &amp; key).fetch(\"file_path\")\n\n    nframes = 0\n    px_height, px_width, fps = None, None, None\n\n    for file_path in file_paths:\n        file_path = (find_full_path(get_dlc_root_data_dir(), file_path)).as_posix()\n\n        cap = cv2.VideoCapture(file_path)\n        info = (\n            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n            int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n            int(cap.get(cv2.CAP_PROP_FPS)),\n        )\n        if px_height is not None:\n            assert (px_height, px_width, fps) == info\n        px_height, px_width, fps = info\n        nframes += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        cap.release()\n\n    self.insert1(\n        {\n            **key,\n            \"px_height\": px_height,\n            \"px_width\": px_width,\n            \"nframes\": nframes,\n            \"fps\": fps,\n            \"recording_duration\": nframes / fps,\n        }\n    )\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>             Bases: <code>dj.Lookup</code></p> <p>Body parts tracked by DeepLabCut models</p> <p>Attributes:</p> Name Type Description <code>body_part</code> <code> varchar(32) </code> <p>Body part short name.</p> <code>body_part_description</code> <code> varchar(1000),optional </code> <p>Full description</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@schema\nclass BodyPart(dj.Lookup):\n\"\"\"Body parts tracked by DeepLabCut models\n\n    Attributes:\n        body_part ( varchar(32) ): Body part short name.\n        body_part_description ( varchar(1000),optional ): Full description\n\n    \"\"\"\n\n    definition = \"\"\"\n    body_part                : varchar(32)\n    ---\n    body_part_description='' : varchar(1000)\n    \"\"\"\n\n    @classmethod\n    def extract_new_body_parts(cls, dlc_config: dict, verbose: bool = True):\n\"\"\"Returns list of body parts present in dlc config, but not BodyPart table.\n\n        Args:\n            dlc_config (str or dict):  path to a config.y*ml, or dict of such contents.\n            verbose (bool): Default True. Print both existing and new items to console.\n        \"\"\"\n        if not isinstance(dlc_config, dict):\n            dlc_config_fp = find_full_path(get_dlc_root_data_dir(), Path(dlc_config))\n            assert dlc_config_fp.exists() and dlc_config_fp.suffix in (\n                \".yml\",\n                \".yaml\",\n            ), f\"dlc_config is neither dict nor filepath\\n Check: {dlc_config_fp}\"\n            if dlc_config_fp.suffix in (\".yml\", \".yaml\"):\n                with open(dlc_config_fp, \"rb\") as f:\n                    dlc_config = yaml.safe_load(f)\n        # -- Check and insert new BodyPart --\n        assert \"bodyparts\" in dlc_config, f\"Found no bodyparts section in {dlc_config}\"\n        tracked_body_parts = cls.fetch(\"body_part\")\n        new_body_parts = np.setdiff1d(dlc_config[\"bodyparts\"], tracked_body_parts)\n        if verbose:  # Added to silence duplicate prompt during `insert_new_model`\n            print(f\"Existing body parts: {tracked_body_parts}\")\n            print(f\"New body parts: {new_body_parts}\")\n        return new_body_parts\n\n    @classmethod\n    def insert_from_config(\n        cls, dlc_config: dict, descriptions: list = None, prompt=True\n    ):\n\"\"\"Insert all body parts from a config file.\n\n        Args:\n            dlc_config (str or dict):  path to a config.y*ml, or dict of such contents.\n            descriptions (list): Optional. List of strings describing new body parts.\n            prompt (bool): Optional, default True. Prompt for confirmation before insert.\n        \"\"\"\n\n        # handle dlc_config being a yaml file\n        new_body_parts = cls.extract_new_body_parts(dlc_config, verbose=False)\n        if new_body_parts is not None:  # Required bc np.array is ambiguous as bool\n            if descriptions:\n                assert len(descriptions) == len(new_body_parts), (\n                    \"Descriptions list does not match \"\n                    + \" the number of new_body_parts\"\n                )\n                print(f\"New descriptions: {descriptions}\")\n            if descriptions is None:\n                descriptions = [\"\" for x in range(len(new_body_parts))]\n\n            if (\n                prompt\n                and dj.utils.user_choice(\n                    f\"Insert {len(new_body_parts)} new body \" + \"part(s)?\"\n                )\n                != \"yes\"\n            ):\n                print(\"Canceled insert.\")\n                return\n            cls.insert(\n                [\n                    {\"body_part\": b, \"body_part_description\": d}\n                    for b, d in zip(new_body_parts, descriptions)\n                ]\n            )\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.BodyPart.extract_new_body_parts", "title": "<code>extract_new_body_parts(dlc_config, verbose=True)</code>  <code>classmethod</code>", "text": "<p>Returns list of body parts present in dlc config, but not BodyPart table.</p> <p>Parameters:</p> Name Type Description Default <code>dlc_config</code> <code>str or dict</code> <p>path to a config.y*ml, or dict of such contents.</p> required <code>verbose</code> <code>bool</code> <p>Default True. Print both existing and new items to console.</p> <code>True</code> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@classmethod\ndef extract_new_body_parts(cls, dlc_config: dict, verbose: bool = True):\n\"\"\"Returns list of body parts present in dlc config, but not BodyPart table.\n\n    Args:\n        dlc_config (str or dict):  path to a config.y*ml, or dict of such contents.\n        verbose (bool): Default True. Print both existing and new items to console.\n    \"\"\"\n    if not isinstance(dlc_config, dict):\n        dlc_config_fp = find_full_path(get_dlc_root_data_dir(), Path(dlc_config))\n        assert dlc_config_fp.exists() and dlc_config_fp.suffix in (\n            \".yml\",\n            \".yaml\",\n        ), f\"dlc_config is neither dict nor filepath\\n Check: {dlc_config_fp}\"\n        if dlc_config_fp.suffix in (\".yml\", \".yaml\"):\n            with open(dlc_config_fp, \"rb\") as f:\n                dlc_config = yaml.safe_load(f)\n    # -- Check and insert new BodyPart --\n    assert \"bodyparts\" in dlc_config, f\"Found no bodyparts section in {dlc_config}\"\n    tracked_body_parts = cls.fetch(\"body_part\")\n    new_body_parts = np.setdiff1d(dlc_config[\"bodyparts\"], tracked_body_parts)\n    if verbose:  # Added to silence duplicate prompt during `insert_new_model`\n        print(f\"Existing body parts: {tracked_body_parts}\")\n        print(f\"New body parts: {new_body_parts}\")\n    return new_body_parts\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.BodyPart.insert_from_config", "title": "<code>insert_from_config(dlc_config, descriptions=None, prompt=True)</code>  <code>classmethod</code>", "text": "<p>Insert all body parts from a config file.</p> <p>Parameters:</p> Name Type Description Default <code>dlc_config</code> <code>str or dict</code> <p>path to a config.y*ml, or dict of such contents.</p> required <code>descriptions</code> <code>list</code> <p>Optional. List of strings describing new body parts.</p> <code>None</code> <code>prompt</code> <code>bool</code> <p>Optional, default True. Prompt for confirmation before insert.</p> <code>True</code> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@classmethod\ndef insert_from_config(\n    cls, dlc_config: dict, descriptions: list = None, prompt=True\n):\n\"\"\"Insert all body parts from a config file.\n\n    Args:\n        dlc_config (str or dict):  path to a config.y*ml, or dict of such contents.\n        descriptions (list): Optional. List of strings describing new body parts.\n        prompt (bool): Optional, default True. Prompt for confirmation before insert.\n    \"\"\"\n\n    # handle dlc_config being a yaml file\n    new_body_parts = cls.extract_new_body_parts(dlc_config, verbose=False)\n    if new_body_parts is not None:  # Required bc np.array is ambiguous as bool\n        if descriptions:\n            assert len(descriptions) == len(new_body_parts), (\n                \"Descriptions list does not match \"\n                + \" the number of new_body_parts\"\n            )\n            print(f\"New descriptions: {descriptions}\")\n        if descriptions is None:\n            descriptions = [\"\" for x in range(len(new_body_parts))]\n\n        if (\n            prompt\n            and dj.utils.user_choice(\n                f\"Insert {len(new_body_parts)} new body \" + \"part(s)?\"\n            )\n            != \"yes\"\n        ):\n            print(\"Canceled insert.\")\n            return\n        cls.insert(\n            [\n                {\"body_part\": b, \"body_part_description\": d}\n                for b, d in zip(new_body_parts, descriptions)\n            ]\n        )\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.Model", "title": "<code>Model</code>", "text": "<p>             Bases: <code>dj.Manual</code></p> <p>DeepLabCut Models applied to generate pose estimations.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code> varchar(64) </code> <p>User-friendly model name.</p> <code>task</code> <code> varchar(32) </code> <p>Task in the config yaml.</p> <code>date</code> <code> varchar(16) </code> <p>Date in the config yaml.</p> <code>iteration</code> <code>int</code> <p>Iteration/version of this model.</p> <code>snapshotindex</code> <code>int</code> <p>Which snapshot for prediction (if -1, latest).</p> <code>shuffle</code> <code>int</code> <p>Which shuffle of the training dataset.</p> <code>trainingsetindex</code> <code>int</code> <p>Which training set fraction to generate model.</p> <code>scorer</code> <code> varchar(64) </code> <p>Scorer/network name - DLC's GetScorerName().</p> <code>config_template</code> <code>longblob</code> <p>Dictionary of the config for analyze_videos().</p> <code>project_path</code> <code> varchar(255) </code> <p>DLC's project_path in config relative to root.</p> <code>model_prefix</code> <code> varchar(32) </code> <p>Optional. Prefix for model files.</p> <code>model_description</code> <code> varchar(1000) </code> <p>Optional. User-entered description.</p> <code>TrainingParamSet</code> <code>foreign key</code> <p>Optional. Training parameters primary key.</p> Note <p>Models are uniquely identified by the union of task, date, iteration, shuffle, snapshotindex, and trainingsetindex.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@schema\nclass Model(dj.Manual):\n\"\"\"DeepLabCut Models applied to generate pose estimations.\n\n    Attributes:\n        model_name ( varchar(64) ): User-friendly model name.\n        task ( varchar(32) ): Task in the config yaml.\n        date ( varchar(16) ): Date in the config yaml.\n        iteration (int): Iteration/version of this model.\n        snapshotindex (int): Which snapshot for prediction (if -1, latest).\n        shuffle (int): Which shuffle of the training dataset.\n        trainingsetindex (int): Which training set fraction to generate model.\n        scorer ( varchar(64) ): Scorer/network name - DLC's GetScorerName().\n        config_template (longblob): Dictionary of the config for analyze_videos().\n        project_path ( varchar(255) ): DLC's project_path in config relative to root.\n        model_prefix ( varchar(32) ): Optional. Prefix for model files.\n        model_description ( varchar(1000) ): Optional. User-entered description.\n        TrainingParamSet (foreign key): Optional. Training parameters primary key.\n\n    Note:\n        Models are uniquely identified by the union of task, date, iteration, shuffle,\n        snapshotindex, and trainingsetindex.\n    \"\"\"\n\n    definition = \"\"\"\n    model_name           : varchar(64)  # User-friendly model name\n    ---\n    task                 : varchar(32)  # Task in the config yaml\n    date                 : varchar(16)  # Date in the config yaml\n    iteration            : int          # Iteration/version of this model\n    snapshotindex        : int          # which snapshot for prediction (if -1, latest)\n    shuffle              : int          # Shuffle (1) or not (0)\n    trainingsetindex     : int          # Index of training fraction list in config.yaml\n    unique index (task, date, iteration, shuffle, snapshotindex, trainingsetindex)\n    scorer               : varchar(64)  # Scorer/network name - DLC's GetScorerName()\n    config_template      : longblob     # Dictionary of the config for analyze_videos()\n    project_path         : varchar(255) # DLC's project_path in config relative to root\n    model_prefix=''      : varchar(32)\n    model_description='' : varchar(300)\n    -&gt; [nullable] train.TrainingParamSet\n    \"\"\"\n    # project_path is the only item required downstream in the pose schema\n\n    class BodyPart(dj.Part):\n\"\"\"Body parts associated with a given model\n\n        Attributes:\n            body_part ( varchar(32) ): Short name. Also called joint.\n            body_part_description ( varchar(1000) ): Optional. Longer description.\"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; BodyPart\n        \"\"\"\n\n    @classmethod\n    def insert_new_model(\n        cls,\n        model_name: str,\n        dlc_config,\n        *,\n        shuffle: int,\n        trainingsetindex,\n        project_path=None,\n        model_description=\"\",\n        model_prefix=\"\",\n        paramset_idx: int = None,\n        prompt=True,\n        params=None,\n    ):\n\"\"\"Insert new model into the dlc.Model table.\n\n        Args:\n            model_name (str): User-friendly name for this model.\n            dlc_config (str or dict):  path to a config.y*ml, or dict of such contents.\n            shuffle (int): Shuffled or not as 1 or 0.\n            trainingsetindex (int): Index of training fraction list in config.yaml.\n            model_description (str): Optional. Description of this model.\n            model_prefix (str): Optional. Filename prefix used across DLC project\n            paramset_idx (int): Optional. Index from the TrainingParamSet table\n            prompt (bool): Optional. Prompt the user with all info before inserting.\n            params (dict): Optional. If dlc_config is path, dict of override items\n        \"\"\"\n        from deeplabcut.utils.auxiliaryfunctions import GetScorerName  # isort:skip\n\n        # handle dlc_config being a yaml file\n        if not isinstance(dlc_config, dict):\n            dlc_config_fp = find_full_path(get_dlc_root_data_dir(), Path(dlc_config))\n            assert dlc_config_fp.exists(), (\n                \"dlc_config is neither dict nor filepath\" + f\"\\n Check: {dlc_config_fp}\"\n            )\n            if dlc_config_fp.suffix in (\".yml\", \".yaml\"):\n                with open(dlc_config_fp, \"rb\") as f:\n                    dlc_config = yaml.safe_load(f)\n            if isinstance(params, dict):\n                dlc_config.update(params)\n\n        # ---- Get and resolve project path ----\n        project_path = find_full_path(\n            get_dlc_root_data_dir(), dlc_config.get(\"project_path\", project_path)\n        )\n        dlc_config[\"project_path\"] = project_path.as_posix()  # update if different\n        root_dir = find_root_directory(get_dlc_root_data_dir(), project_path)\n\n        # ---- Verify config ----\n        needed_attributes = [\n            \"Task\",\n            \"date\",\n            \"iteration\",\n            \"snapshotindex\",\n            \"TrainingFraction\",\n        ]\n        for attribute in needed_attributes:\n            assert attribute in dlc_config, f\"Couldn't find {attribute} in config\"\n\n        # ---- Get scorer name ----\n        # \"or 'f'\" below covers case where config returns None. str_to_bool handles else\n        scorer_legacy = str_to_bool(dlc_config.get(\"scorer_legacy\", \"f\"))\n\n        dlc_scorer = GetScorerName(\n            cfg=dlc_config,\n            shuffle=shuffle,\n            trainFraction=dlc_config[\"TrainingFraction\"][int(trainingsetindex)],\n            modelprefix=model_prefix,\n        )[scorer_legacy]\n        if dlc_config[\"snapshotindex\"] == -1:\n            dlc_scorer = \"\".join(dlc_scorer.split(\"_\")[:-1])\n\n        # ---- Insert ----\n        model_dict = {\n            \"model_name\": model_name,\n            \"model_description\": model_description,\n            \"scorer\": dlc_scorer,\n            \"task\": dlc_config[\"Task\"],\n            \"date\": dlc_config[\"date\"],\n            \"iteration\": dlc_config[\"iteration\"],\n            \"snapshotindex\": dlc_config[\"snapshotindex\"],\n            \"shuffle\": shuffle,\n            \"trainingsetindex\": int(trainingsetindex),\n            \"project_path\": project_path.relative_to(root_dir).as_posix(),\n            \"paramset_idx\": paramset_idx,\n            \"config_template\": dlc_config,\n        }\n\n        # -- prompt for confirmation --\n        if prompt:\n            print(\"--- DLC Model specification to be inserted ---\")\n            for k, v in model_dict.items():\n                if k != \"config_template\":\n                    print(\"\\t{}: {}\".format(k, v))\n                else:\n                    print(\"\\t-- Template/Contents of config.yaml --\")\n                    for k, v in model_dict[\"config_template\"].items():\n                        print(\"\\t\\t{}: {}\".format(k, v))\n\n        if (\n            prompt\n            and dj.utils.user_choice(\"Proceed with new DLC model insert?\") != \"yes\"\n        ):\n            print(\"Canceled insert.\")\n            return\n        # ---- Save DJ-managed config ----\n        _ = dlc_reader.save_yaml(project_path, dlc_config)\n        # ____ Insert into table ----\n        with cls.connection.transaction:\n            cls.insert1(model_dict)\n            # Returns array, so check size for unambiguous truth value\n            if BodyPart.extract_new_body_parts(dlc_config, verbose=False).size &gt; 0:\n                BodyPart.insert_from_config(dlc_config, prompt=prompt)\n            cls.BodyPart.insert((model_name, bp) for bp in dlc_config[\"bodyparts\"])\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.Model.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>             Bases: <code>dj.Part</code></p> <p>Body parts associated with a given model</p> <p>Attributes:</p> Name Type Description <code>body_part</code> <code> varchar(32) </code> <p>Short name. Also called joint.</p> <code>body_part_description</code> <code> varchar(1000) </code> <p>Optional. Longer description.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>class BodyPart(dj.Part):\n\"\"\"Body parts associated with a given model\n\n    Attributes:\n        body_part ( varchar(32) ): Short name. Also called joint.\n        body_part_description ( varchar(1000) ): Optional. Longer description.\"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; BodyPart\n    \"\"\"\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.Model.insert_new_model", "title": "<code>insert_new_model(model_name, dlc_config, *, shuffle, trainingsetindex, project_path=None, model_description='', model_prefix='', paramset_idx=None, prompt=True, params=None)</code>  <code>classmethod</code>", "text": "<p>Insert new model into the dlc.Model table.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>User-friendly name for this model.</p> required <code>dlc_config</code> <code>str or dict</code> <p>path to a config.y*ml, or dict of such contents.</p> required <code>shuffle</code> <code>int</code> <p>Shuffled or not as 1 or 0.</p> required <code>trainingsetindex</code> <code>int</code> <p>Index of training fraction list in config.yaml.</p> required <code>model_description</code> <code>str</code> <p>Optional. Description of this model.</p> <code>''</code> <code>model_prefix</code> <code>str</code> <p>Optional. Filename prefix used across DLC project</p> <code>''</code> <code>paramset_idx</code> <code>int</code> <p>Optional. Index from the TrainingParamSet table</p> <code>None</code> <code>prompt</code> <code>bool</code> <p>Optional. Prompt the user with all info before inserting.</p> <code>True</code> <code>params</code> <code>dict</code> <p>Optional. If dlc_config is path, dict of override items</p> <code>None</code> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@classmethod\ndef insert_new_model(\n    cls,\n    model_name: str,\n    dlc_config,\n    *,\n    shuffle: int,\n    trainingsetindex,\n    project_path=None,\n    model_description=\"\",\n    model_prefix=\"\",\n    paramset_idx: int = None,\n    prompt=True,\n    params=None,\n):\n\"\"\"Insert new model into the dlc.Model table.\n\n    Args:\n        model_name (str): User-friendly name for this model.\n        dlc_config (str or dict):  path to a config.y*ml, or dict of such contents.\n        shuffle (int): Shuffled or not as 1 or 0.\n        trainingsetindex (int): Index of training fraction list in config.yaml.\n        model_description (str): Optional. Description of this model.\n        model_prefix (str): Optional. Filename prefix used across DLC project\n        paramset_idx (int): Optional. Index from the TrainingParamSet table\n        prompt (bool): Optional. Prompt the user with all info before inserting.\n        params (dict): Optional. If dlc_config is path, dict of override items\n    \"\"\"\n    from deeplabcut.utils.auxiliaryfunctions import GetScorerName  # isort:skip\n\n    # handle dlc_config being a yaml file\n    if not isinstance(dlc_config, dict):\n        dlc_config_fp = find_full_path(get_dlc_root_data_dir(), Path(dlc_config))\n        assert dlc_config_fp.exists(), (\n            \"dlc_config is neither dict nor filepath\" + f\"\\n Check: {dlc_config_fp}\"\n        )\n        if dlc_config_fp.suffix in (\".yml\", \".yaml\"):\n            with open(dlc_config_fp, \"rb\") as f:\n                dlc_config = yaml.safe_load(f)\n        if isinstance(params, dict):\n            dlc_config.update(params)\n\n    # ---- Get and resolve project path ----\n    project_path = find_full_path(\n        get_dlc_root_data_dir(), dlc_config.get(\"project_path\", project_path)\n    )\n    dlc_config[\"project_path\"] = project_path.as_posix()  # update if different\n    root_dir = find_root_directory(get_dlc_root_data_dir(), project_path)\n\n    # ---- Verify config ----\n    needed_attributes = [\n        \"Task\",\n        \"date\",\n        \"iteration\",\n        \"snapshotindex\",\n        \"TrainingFraction\",\n    ]\n    for attribute in needed_attributes:\n        assert attribute in dlc_config, f\"Couldn't find {attribute} in config\"\n\n    # ---- Get scorer name ----\n    # \"or 'f'\" below covers case where config returns None. str_to_bool handles else\n    scorer_legacy = str_to_bool(dlc_config.get(\"scorer_legacy\", \"f\"))\n\n    dlc_scorer = GetScorerName(\n        cfg=dlc_config,\n        shuffle=shuffle,\n        trainFraction=dlc_config[\"TrainingFraction\"][int(trainingsetindex)],\n        modelprefix=model_prefix,\n    )[scorer_legacy]\n    if dlc_config[\"snapshotindex\"] == -1:\n        dlc_scorer = \"\".join(dlc_scorer.split(\"_\")[:-1])\n\n    # ---- Insert ----\n    model_dict = {\n        \"model_name\": model_name,\n        \"model_description\": model_description,\n        \"scorer\": dlc_scorer,\n        \"task\": dlc_config[\"Task\"],\n        \"date\": dlc_config[\"date\"],\n        \"iteration\": dlc_config[\"iteration\"],\n        \"snapshotindex\": dlc_config[\"snapshotindex\"],\n        \"shuffle\": shuffle,\n        \"trainingsetindex\": int(trainingsetindex),\n        \"project_path\": project_path.relative_to(root_dir).as_posix(),\n        \"paramset_idx\": paramset_idx,\n        \"config_template\": dlc_config,\n    }\n\n    # -- prompt for confirmation --\n    if prompt:\n        print(\"--- DLC Model specification to be inserted ---\")\n        for k, v in model_dict.items():\n            if k != \"config_template\":\n                print(\"\\t{}: {}\".format(k, v))\n            else:\n                print(\"\\t-- Template/Contents of config.yaml --\")\n                for k, v in model_dict[\"config_template\"].items():\n                    print(\"\\t\\t{}: {}\".format(k, v))\n\n    if (\n        prompt\n        and dj.utils.user_choice(\"Proceed with new DLC model insert?\") != \"yes\"\n    ):\n        print(\"Canceled insert.\")\n        return\n    # ---- Save DJ-managed config ----\n    _ = dlc_reader.save_yaml(project_path, dlc_config)\n    # ____ Insert into table ----\n    with cls.connection.transaction:\n        cls.insert1(model_dict)\n        # Returns array, so check size for unambiguous truth value\n        if BodyPart.extract_new_body_parts(dlc_config, verbose=False).size &gt; 0:\n            BodyPart.insert_from_config(dlc_config, prompt=prompt)\n        cls.BodyPart.insert((model_name, bp) for bp in dlc_config[\"bodyparts\"])\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.ModelEvaluation", "title": "<code>ModelEvaluation</code>", "text": "<p>             Bases: <code>dj.Computed</code></p> <p>Performance characteristics model calculated by <code>deeplabcut.evaluate_network</code></p> <p>Attributes:</p> Name Type Description <code>Model</code> <code>foreign key</code> <p>Model name.</p> <code>train_iterations</code> <code>int</code> <p>Training iterations.</p> <code>train_error</code> <code>float</code> <p>Optional. Train error (px).</p> <code>test_error</code> <code>float</code> <p>Optional. Test error (px).</p> <code>p_cutoff</code> <code>float</code> <p>Optional. p-cutoff used.</p> <code>train_error_p</code> <code>float</code> <p>Optional. Train error with p-cutoff.</p> <code>test_error_p</code> <code>float</code> <p>Optional. Test error with p-cutoff.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@schema\nclass ModelEvaluation(dj.Computed):\n\"\"\"Performance characteristics model calculated by `deeplabcut.evaluate_network`\n\n    Attributes:\n        Model (foreign key): Model name.\n        train_iterations (int): Training iterations.\n        train_error (float): Optional. Train error (px).\n        test_error (float): Optional. Test error (px).\n        p_cutoff (float): Optional. p-cutoff used.\n        train_error_p (float): Optional. Train error with p-cutoff.\n        test_error_p (float): Optional. Test error with p-cutoff.\"\"\"\n\n    definition = \"\"\"\n    -&gt; Model\n    ---\n    train_iterations   : int   # Training iterations\n    train_error=null   : float # Train error (px)\n    test_error=null    : float # Test error (px)\n    p_cutoff=null      : float # p-cutoff used\n    train_error_p=null : float # Train error with p-cutoff\n    test_error_p=null  : float # Test error with p-cutoff\n    \"\"\"\n\n    def make(self, key):\n        from deeplabcut import evaluate_network  # isort:skip\n        from deeplabcut.utils.auxiliaryfunctions import (\n            get_evaluation_folder,\n        )  # isort:skip\n\n\"\"\".populate() method will launch evaluation for each unique entry in Model.\"\"\"\n        dlc_config, project_path, model_prefix, shuffle, trainingsetindex = (\n            Model &amp; key\n        ).fetch1(\n            \"config_template\",\n            \"project_path\",\n            \"model_prefix\",\n            \"shuffle\",\n            \"trainingsetindex\",\n        )\n\n        project_path = find_full_path(get_dlc_root_data_dir(), project_path)\n        yml_path, _ = dlc_reader.read_yaml(project_path)\n\n        evaluate_network(\n            yml_path,\n            Shuffles=[shuffle],  # this needs to be a list\n            trainingsetindex=trainingsetindex,\n            comparisonbodyparts=\"all\",\n        )\n\n        eval_folder = get_evaluation_folder(\n            trainFraction=dlc_config[\"TrainingFraction\"][trainingsetindex],\n            shuffle=shuffle,\n            cfg=dlc_config,\n            modelprefix=model_prefix,\n        )\n        eval_path = project_path / eval_folder\n        assert eval_path.exists(), f\"Couldn't find evaluation folder:\\n{eval_path}\"\n\n        eval_csvs = list(eval_path.glob(\"*csv\"))\n        max_modified_time = 0\n        for eval_csv in eval_csvs:\n            modified_time = os.path.getmtime(eval_csv)\n            if modified_time &gt; max_modified_time:\n                eval_csv_latest = eval_csv\n        with open(eval_csv_latest, newline=\"\") as f:\n            results = list(csv.DictReader(f, delimiter=\",\"))[0]\n        # in testing, test_error_p returned empty string\n        self.insert1(\n            dict(\n                key,\n                train_iterations=results[\"Training iterations:\"],\n                train_error=results[\" Train error(px)\"],\n                test_error=results[\" Test error(px)\"],\n                p_cutoff=results[\"p-cutoff used\"],\n                train_error_p=results[\"Train error with p-cutoff\"],\n                test_error_p=results[\"Test error with p-cutoff\"],\n            )\n        )\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimationTask", "title": "<code>PoseEstimationTask</code>", "text": "<p>             Bases: <code>dj.Manual</code></p> <p>Staging table for pairing of video recording and model before inference.</p> <p>Attributes:</p> Name Type Description <code>VideoRecording</code> <code>foreign key</code> <p>Video recording key.</p> <code>Model</code> <code>foreign key</code> <p>Model name.</p> <code>task_mode</code> <code>load or trigger</code> <p>Optional. Default load. Or trigger computation.</p> <code>pose_estimation_output_dir</code> <code> varchar(255) </code> <p>Optional. Output dir relative to                                          get_dlc_root_data_dir.</p> <code>pose_estimation_params</code> <code>longblob</code> <p>Optional. Params for DLC's analyze_videos                                params, if not default.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@schema\nclass PoseEstimationTask(dj.Manual):\n\"\"\"Staging table for pairing of video recording and model before inference.\n\n    Attributes:\n        VideoRecording (foreign key): Video recording key.\n        Model (foreign key): Model name.\n        task_mode (load or trigger): Optional. Default load. Or trigger computation.\n        pose_estimation_output_dir ( varchar(255) ): Optional. Output dir relative to\n                                                     get_dlc_root_data_dir.\n        pose_estimation_params (longblob): Optional. Params for DLC's analyze_videos\n                                           params, if not default.\"\"\"\n\n    definition = \"\"\"\n    -&gt; VideoRecording                           # Session -&gt; Recording + File part table\n    -&gt; Model                                    # Must specify a DLC project_path\n    ---\n    task_mode='load' : enum('load', 'trigger')  # load results or trigger computation\n    pose_estimation_output_dir='': varchar(255) # output dir relative to the root dir\n    pose_estimation_params=null  : longblob     # analyze_videos params, if not default\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key: dict, relative: bool = False, mkdir: bool = False):\n\"\"\"Return the expected pose_estimation_output_dir.\n\n        Spaces in model name are replaced with hyphens.\n        Based on convention: / video_dir / Device_{}_Recording_{}_Model_{}\n\n        Args:\n            key: DataJoint key specifying a pairing of VideoRecording and Model.\n            relative (bool): Report directory relative to get_dlc_processed_data_dir().\n            mkdir (bool): Default False. Make directory if it doesn't exist.\n        \"\"\"\n        video_filepath = find_full_path(\n            get_dlc_root_data_dir(),\n            (VideoRecording.File &amp; key).fetch(\"file_path\", limit=1)[0],\n        )\n        root_dir = find_root_directory(get_dlc_root_data_dir(), video_filepath.parent)\n        recording_key = VideoRecording &amp; key\n        device = \"-\".join(\n            str(v)\n            for v in (_linking_module.Device &amp; recording_key).fetch1(\"KEY\").values()\n        )\n        if get_dlc_processed_data_dir():\n            processed_dir = Path(get_dlc_processed_data_dir())\n        else:  # if processed not provided, default to where video is\n            processed_dir = root_dir\n\n        output_dir = (\n            processed_dir\n            / video_filepath.parent.relative_to(root_dir)\n            / (\n                f'device_{device}_recording_{key[\"recording_id\"]}_model_'\n                + key[\"model_name\"].replace(\" \", \"-\")\n            )\n        )\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def generate(\n        cls,\n        video_recording_key: dict,\n        model_name: str,\n        *,\n        task_mode: str = None,\n        analyze_videos_params: dict = None,\n    ):\n\"\"\"Insert PoseEstimationTask in inferred output dir.\n\n        Based on the convention / video_dir / device_{}_recording_{}_model_{}\n\n        Args:\n            video_recording_key (dict): DataJoint key specifying a VideoRecording.\n\n            model_name (str): Name of DLC model (from Model table) to be used for inference.\n            task_mode (str): Default 'trigger' computation. Or 'load' existing results.\n            analyze_videos_params (dict): Optional. Parameters passed to DLC's analyze_videos:\n                videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n                dynamic, robust_nframes, allow_growth, use_shelve\n        \"\"\"\n        processed_dir = get_dlc_processed_data_dir()\n        output_dir = cls.infer_output_dir(\n            {**video_recording_key, \"model_name\": model_name},\n            relative=False,\n            mkdir=True,\n        )\n\n        if task_mode is None:\n            try:\n                _ = dlc_reader.PoseEstimation(output_dir)\n            except FileNotFoundError:\n                task_mode = \"trigger\"\n            else:\n                task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **video_recording_key,\n                \"model_name\": model_name,\n                \"task_mode\": task_mode,\n                \"pose_estimation_params\": analyze_videos_params,\n                \"pose_estimation_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n            }\n        )\n\n    insert_estimation_task = generate\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimationTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Return the expected pose_estimation_output_dir.</p> <p>Spaces in model name are replaced with hyphens. Based on convention: / video_dir / Device_{}Recording{}Model{}</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>DataJoint key specifying a pairing of VideoRecording and Model.</p> required <code>relative</code> <code>bool</code> <p>Report directory relative to get_dlc_processed_data_dir().</p> <code>False</code> <code>mkdir</code> <code>bool</code> <p>Default False. Make directory if it doesn't exist.</p> <code>False</code> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key: dict, relative: bool = False, mkdir: bool = False):\n\"\"\"Return the expected pose_estimation_output_dir.\n\n    Spaces in model name are replaced with hyphens.\n    Based on convention: / video_dir / Device_{}_Recording_{}_Model_{}\n\n    Args:\n        key: DataJoint key specifying a pairing of VideoRecording and Model.\n        relative (bool): Report directory relative to get_dlc_processed_data_dir().\n        mkdir (bool): Default False. Make directory if it doesn't exist.\n    \"\"\"\n    video_filepath = find_full_path(\n        get_dlc_root_data_dir(),\n        (VideoRecording.File &amp; key).fetch(\"file_path\", limit=1)[0],\n    )\n    root_dir = find_root_directory(get_dlc_root_data_dir(), video_filepath.parent)\n    recording_key = VideoRecording &amp; key\n    device = \"-\".join(\n        str(v)\n        for v in (_linking_module.Device &amp; recording_key).fetch1(\"KEY\").values()\n    )\n    if get_dlc_processed_data_dir():\n        processed_dir = Path(get_dlc_processed_data_dir())\n    else:  # if processed not provided, default to where video is\n        processed_dir = root_dir\n\n    output_dir = (\n        processed_dir\n        / video_filepath.parent.relative_to(root_dir)\n        / (\n            f'device_{device}_recording_{key[\"recording_id\"]}_model_'\n            + key[\"model_name\"].replace(\" \", \"-\")\n        )\n    )\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimationTask.generate", "title": "<code>generate(video_recording_key, model_name, *, task_mode=None, analyze_videos_params=None)</code>  <code>classmethod</code>", "text": "<p>Insert PoseEstimationTask in inferred output dir.</p> <p>Based on the convention / video_dir / device_{}recording{}model{}</p> <p>Parameters:</p> Name Type Description Default <code>video_recording_key</code> <code>dict</code> <p>DataJoint key specifying a VideoRecording.</p> required <code>model_name</code> <code>str</code> <p>Name of DLC model (from Model table) to be used for inference.</p> required <code>task_mode</code> <code>str</code> <p>Default 'trigger' computation. Or 'load' existing results.</p> <code>None</code> <code>analyze_videos_params</code> <code>dict</code> <p>Optional. Parameters passed to DLC's analyze_videos: videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve</p> <code>None</code> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@classmethod\ndef generate(\n    cls,\n    video_recording_key: dict,\n    model_name: str,\n    *,\n    task_mode: str = None,\n    analyze_videos_params: dict = None,\n):\n\"\"\"Insert PoseEstimationTask in inferred output dir.\n\n    Based on the convention / video_dir / device_{}_recording_{}_model_{}\n\n    Args:\n        video_recording_key (dict): DataJoint key specifying a VideoRecording.\n\n        model_name (str): Name of DLC model (from Model table) to be used for inference.\n        task_mode (str): Default 'trigger' computation. Or 'load' existing results.\n        analyze_videos_params (dict): Optional. Parameters passed to DLC's analyze_videos:\n            videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference,\n            dynamic, robust_nframes, allow_growth, use_shelve\n    \"\"\"\n    processed_dir = get_dlc_processed_data_dir()\n    output_dir = cls.infer_output_dir(\n        {**video_recording_key, \"model_name\": model_name},\n        relative=False,\n        mkdir=True,\n    )\n\n    if task_mode is None:\n        try:\n            _ = dlc_reader.PoseEstimation(output_dir)\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **video_recording_key,\n            \"model_name\": model_name,\n            \"task_mode\": task_mode,\n            \"pose_estimation_params\": analyze_videos_params,\n            \"pose_estimation_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n        }\n    )\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimation", "title": "<code>PoseEstimation</code>", "text": "<p>             Bases: <code>dj.Computed</code></p> <p>Results of pose estimation.</p> <p>Attributes:</p> Name Type Description <code>PoseEstimationTask</code> <code>foreign key</code> <p>Pose Estimation Task key.</p> <code>post_estimation_time</code> <code>datetime</code> <p>time of generation of this set of DLC results.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@schema\nclass PoseEstimation(dj.Computed):\n\"\"\"Results of pose estimation.\n\n    Attributes:\n        PoseEstimationTask (foreign key): Pose Estimation Task key.\n        post_estimation_time (datetime): time of generation of this set of DLC results.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PoseEstimationTask\n    ---\n    pose_estimation_time: datetime  # time of generation of this set of DLC results\n    \"\"\"\n\n    class BodyPartPosition(dj.Part):\n\"\"\"Position of individual body parts by frame index\n\n        Attributes:\n            PoseEstimation (foreign key): Pose Estimation key.\n            Model.BodyPart (foreign key): Body Part key.\n            frame_index (longblob): Frame index in model.\n            x_pos (longblob): X position.\n            y_pos (longblob): Y position.\n            z_pos (longblob): Optional. Z position.\n            likelihood (longblob): Model confidence.\"\"\"\n\n        definition = \"\"\" # uses DeepLabCut h5 output for body part position\n        -&gt; master\n        -&gt; Model.BodyPart\n        ---\n        frame_index : longblob     # frame index in model\n        x_pos       : longblob\n        y_pos       : longblob\n        z_pos=null  : longblob\n        likelihood  : longblob\n        \"\"\"\n\n    def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n        # ID model and directories\n        dlc_model = (Model &amp; key).fetch1()\n        task_mode, output_dir = (PoseEstimationTask &amp; key).fetch1(\n            \"task_mode\", \"pose_estimation_output_dir\"\n        )\n\n        output_dir = find_full_path(get_dlc_root_data_dir(), output_dir)\n\n        # Triger PoseEstimation\n        if task_mode == \"trigger\":\n            # Triggering dlc for pose estimation required:\n            # - project_path: full path to the directory containing the trained model\n            # - video_filepaths: full paths to the video files for inference\n            # - analyze_video_params: optional parameters to analyze video\n            project_path = find_full_path(\n                get_dlc_root_data_dir(), dlc_model[\"project_path\"]\n            )\n            video_filepaths = [\n                find_full_path(get_dlc_root_data_dir(), fp).as_posix()\n                for fp in (VideoRecording.File &amp; key).fetch(\"file_path\")\n            ]\n            analyze_video_params = (PoseEstimationTask &amp; key).fetch1(\n                \"pose_estimation_params\"\n            ) or {}\n\n            dlc_reader.do_pose_estimation(\n                key,\n                video_filepaths,\n                dlc_model,\n                project_path,\n                output_dir,\n                **analyze_video_params,\n            )\n\n        dlc_result = dlc_reader.PoseEstimation(output_dir)\n        creation_time = datetime.fromtimestamp(dlc_result.creation_time).strftime(\n            \"%Y-%m-%d %H:%M:%S\"\n        )\n\n        body_parts = [\n            {\n                **key,\n                \"body_part\": k,\n                \"frame_index\": np.arange(dlc_result.nframes),\n                \"x_pos\": v[\"x\"],\n                \"y_pos\": v[\"y\"],\n                \"z_pos\": v.get(\"z\"),\n                \"likelihood\": v[\"likelihood\"],\n            }\n            for k, v in dlc_result.data.items()\n        ]\n\n        self.insert1({**key, \"pose_estimation_time\": creation_time})\n        self.BodyPartPosition.insert(body_parts)\n\n    @classmethod\n    def get_trajectory(cls, key: dict, body_parts: list = \"all\") -&gt; pd.DataFrame:\n\"\"\"Returns a pandas dataframe of coordinates of the specified body_part(s)\n\n        Args:\n            key (dict): A DataJoint query specifying one PoseEstimation entry.\n            body_parts (list, optional): Body parts as a list. If \"all\", all joints\n\n        Returns:\n            df: multi index pandas dataframe with DLC scorer names, body_parts\n                and x/y coordinates of each joint name for a camera_id, similar to\n                 output of DLC dataframe. If 2D, z is set of zeros\n        \"\"\"\n        model_name = key[\"model_name\"]\n\n        if body_parts == \"all\":\n            body_parts = (cls.BodyPartPosition &amp; key).fetch(\"body_part\")\n        elif not isinstance(body_parts, list):\n            body_parts = list(body_parts)\n\n        df = None\n        for body_part in body_parts:\n            x_pos, y_pos, z_pos, likelihood = (\n                cls.BodyPartPosition &amp; {\"body_part\": body_part}\n            ).fetch1(\"x_pos\", \"y_pos\", \"z_pos\", \"likelihood\")\n            if not z_pos:\n                z_pos = np.zeros_like(x_pos)\n\n            a = np.vstack((x_pos, y_pos, z_pos, likelihood))\n            a = a.T\n            pdindex = pd.MultiIndex.from_product(\n                [[model_name], [body_part], [\"x\", \"y\", \"z\", \"likelihood\"]],\n                names=[\"scorer\", \"bodyparts\", \"coords\"],\n            )\n            frame = pd.DataFrame(a, columns=pdindex, index=range(0, a.shape[0]))\n            df = pd.concat([df, frame], axis=1)\n        return df\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimation.BodyPartPosition", "title": "<code>BodyPartPosition</code>", "text": "<p>             Bases: <code>dj.Part</code></p> <p>Position of individual body parts by frame index</p> <p>Attributes:</p> Name Type Description <code>PoseEstimation</code> <code>foreign key</code> <p>Pose Estimation key.</p> <code>Model.BodyPart</code> <code>foreign key</code> <p>Body Part key.</p> <code>frame_index</code> <code>longblob</code> <p>Frame index in model.</p> <code>x_pos</code> <code>longblob</code> <p>X position.</p> <code>y_pos</code> <code>longblob</code> <p>Y position.</p> <code>z_pos</code> <code>longblob</code> <p>Optional. Z position.</p> <code>likelihood</code> <code>longblob</code> <p>Model confidence.</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>class BodyPartPosition(dj.Part):\n\"\"\"Position of individual body parts by frame index\n\n    Attributes:\n        PoseEstimation (foreign key): Pose Estimation key.\n        Model.BodyPart (foreign key): Body Part key.\n        frame_index (longblob): Frame index in model.\n        x_pos (longblob): X position.\n        y_pos (longblob): Y position.\n        z_pos (longblob): Optional. Z position.\n        likelihood (longblob): Model confidence.\"\"\"\n\n    definition = \"\"\" # uses DeepLabCut h5 output for body part position\n    -&gt; master\n    -&gt; Model.BodyPart\n    ---\n    frame_index : longblob     # frame index in model\n    x_pos       : longblob\n    y_pos       : longblob\n    z_pos=null  : longblob\n    likelihood  : longblob\n    \"\"\"\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimation.make", "title": "<code>make(key)</code>", "text": "<p>.populate() method will launch training for each PoseEstimationTask</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>def make(self, key):\n\"\"\".populate() method will launch training for each PoseEstimationTask\"\"\"\n    # ID model and directories\n    dlc_model = (Model &amp; key).fetch1()\n    task_mode, output_dir = (PoseEstimationTask &amp; key).fetch1(\n        \"task_mode\", \"pose_estimation_output_dir\"\n    )\n\n    output_dir = find_full_path(get_dlc_root_data_dir(), output_dir)\n\n    # Triger PoseEstimation\n    if task_mode == \"trigger\":\n        # Triggering dlc for pose estimation required:\n        # - project_path: full path to the directory containing the trained model\n        # - video_filepaths: full paths to the video files for inference\n        # - analyze_video_params: optional parameters to analyze video\n        project_path = find_full_path(\n            get_dlc_root_data_dir(), dlc_model[\"project_path\"]\n        )\n        video_filepaths = [\n            find_full_path(get_dlc_root_data_dir(), fp).as_posix()\n            for fp in (VideoRecording.File &amp; key).fetch(\"file_path\")\n        ]\n        analyze_video_params = (PoseEstimationTask &amp; key).fetch1(\n            \"pose_estimation_params\"\n        ) or {}\n\n        dlc_reader.do_pose_estimation(\n            key,\n            video_filepaths,\n            dlc_model,\n            project_path,\n            output_dir,\n            **analyze_video_params,\n        )\n\n    dlc_result = dlc_reader.PoseEstimation(output_dir)\n    creation_time = datetime.fromtimestamp(dlc_result.creation_time).strftime(\n        \"%Y-%m-%d %H:%M:%S\"\n    )\n\n    body_parts = [\n        {\n            **key,\n            \"body_part\": k,\n            \"frame_index\": np.arange(dlc_result.nframes),\n            \"x_pos\": v[\"x\"],\n            \"y_pos\": v[\"y\"],\n            \"z_pos\": v.get(\"z\"),\n            \"likelihood\": v[\"likelihood\"],\n        }\n        for k, v in dlc_result.data.items()\n    ]\n\n    self.insert1({**key, \"pose_estimation_time\": creation_time})\n    self.BodyPartPosition.insert(body_parts)\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimation.get_trajectory", "title": "<code>get_trajectory(key, body_parts='all')</code>  <code>classmethod</code>", "text": "<p>Returns a pandas dataframe of coordinates of the specified body_part(s)</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>A DataJoint query specifying one PoseEstimation entry.</p> required <code>body_parts</code> <code>list</code> <p>Body parts as a list. If \"all\", all joints</p> <code>'all'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>multi index pandas dataframe with DLC scorer names, body_parts and x/y coordinates of each joint name for a camera_id, similar to  output of DLC dataframe. If 2D, z is set of zeros</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>@classmethod\ndef get_trajectory(cls, key: dict, body_parts: list = \"all\") -&gt; pd.DataFrame:\n\"\"\"Returns a pandas dataframe of coordinates of the specified body_part(s)\n\n    Args:\n        key (dict): A DataJoint query specifying one PoseEstimation entry.\n        body_parts (list, optional): Body parts as a list. If \"all\", all joints\n\n    Returns:\n        df: multi index pandas dataframe with DLC scorer names, body_parts\n            and x/y coordinates of each joint name for a camera_id, similar to\n             output of DLC dataframe. If 2D, z is set of zeros\n    \"\"\"\n    model_name = key[\"model_name\"]\n\n    if body_parts == \"all\":\n        body_parts = (cls.BodyPartPosition &amp; key).fetch(\"body_part\")\n    elif not isinstance(body_parts, list):\n        body_parts = list(body_parts)\n\n    df = None\n    for body_part in body_parts:\n        x_pos, y_pos, z_pos, likelihood = (\n            cls.BodyPartPosition &amp; {\"body_part\": body_part}\n        ).fetch1(\"x_pos\", \"y_pos\", \"z_pos\", \"likelihood\")\n        if not z_pos:\n            z_pos = np.zeros_like(x_pos)\n\n        a = np.vstack((x_pos, y_pos, z_pos, likelihood))\n        a = a.T\n        pdindex = pd.MultiIndex.from_product(\n            [[model_name], [body_part], [\"x\", \"y\", \"z\", \"likelihood\"]],\n            names=[\"scorer\", \"bodyparts\", \"coords\"],\n        )\n        frame = pd.DataFrame(a, columns=pdindex, index=range(0, a.shape[0]))\n        df = pd.concat([df, frame], axis=1)\n    return df\n</code></pre>"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.str_to_bool", "title": "<code>str_to_bool(value)</code>", "text": "<p>Return whether the provided string represents true. Otherwise false.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>any</code> <p>Any input</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if value in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\")</p> Source code in <code>element_deeplabcut/model.py</code> <pre><code>def str_to_bool(value) -&gt; bool:\n\"\"\"Return whether the provided string represents true. Otherwise false.\n\n    Args:\n        value (any): Any input\n\n    Returns:\n        bool (bool): True if value in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\")\n    \"\"\"\n    # Due to distutils equivalent depreciation in 3.10\n    # Adopted from github.com/PostHog/posthog/blob/master/posthog/utils.py\n    if not value:\n        return False\n    return str(value).lower() in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\")\n</code></pre>"}, {"location": "api/element_deeplabcut/train/", "title": "train.py", "text": "<p>Code adapted from the Mathis Lab MIT License Copyright (c) 2022 Mackenzie Mathis DataJoint Schema for DeepLabCut 2.x, Supports 2D and 3D DLC via triangulation.</p>"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.activate", "title": "<code>activate(train_schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>train_schema_name</code> <code>str</code> <p>schema name on the database server</p> required <code>create_schema</code> <code>bool</code> <p>when True (default), create schema in the database if it                 does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>when True (default), create schema tables in the database                  if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>a module (or name) containing the required dependencies.</p> <code>None</code> <p>Dependencies:</p> Functions <p>get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies)                          with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed                               data. Defaults to session video subfolder.</p> Source code in <code>element_deeplabcut/train.py</code> <pre><code>def activate(\n    train_schema_name: str,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activate this schema.\n\n    Args:\n        train_schema_name (str): schema name on the database server\n        create_schema (bool): when True (default), create schema in the database if it\n                            does not yet exist.\n        create_tables (bool): when True (default), create schema tables in the database\n                             if they do not yet exist.\n        linking_module (str): a module (or name) containing the required dependencies.\n\n    Dependencies:\n    Functions:\n        get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies)\n                                 with all behavioral recordings, as (list of) string(s).\n        get_dlc_processed_data_dir(): Optional. Returns absolute path for processed\n                                      data. Defaults to session video subfolder.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n    assert hasattr(\n        linking_module, \"get_dlc_root_data_dir\"\n    ), \"The linking module must specify a lookup function for a root data directory\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    # activate\n    schema.activate(\n        train_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.get_dlc_root_data_dir", "title": "<code>get_dlc_root_data_dir()</code>", "text": "<p>Pulls relevant func from parent namespace to specify root data dir(s).</p> <p>It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strings for possible root data directories.</p> Source code in <code>element_deeplabcut/train.py</code> <pre><code>def get_dlc_root_data_dir() -&gt; list:\n\"\"\"Pulls relevant func from parent namespace to specify root data dir(s).\n\n    It is recommended that all paths in DataJoint Elements stored as relative\n    paths, with respect to some user-configured \"root\" director(y/ies). The\n    root(s) may vary between data modalities and user machines. Returns a full path\n    string or list of strings for possible root data directories.\n    \"\"\"\n    root_directories = _linking_module.get_dlc_root_data_dir()\n    if isinstance(root_directories, (str, Path)):\n        root_directories = [root_directories]\n\n    if (\n        hasattr(_linking_module, \"get_dlc_processed_data_dir\")\n        and get_dlc_processed_data_dir() not in root_directories\n    ):\n        root_directories.append(_linking_module.get_dlc_processed_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.get_dlc_processed_data_dir", "title": "<code>get_dlc_processed_data_dir()</code>", "text": "<p>Pulls relevant func from parent namespace. Defaults to DLC's project /videos/.</p> <p>Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default.</p> Source code in <code>element_deeplabcut/train.py</code> <pre><code>def get_dlc_processed_data_dir() -&gt; str:\n\"\"\"Pulls relevant func from parent namespace. Defaults to DLC's project /videos/.\n\n    Method in parent namespace should provide a string to a directory where DLC output\n    files will be stored. If unspecified, output files will be stored in the\n    session directory 'videos' folder, per DeepLabCut default.\n    \"\"\"\n    if hasattr(_linking_module, \"get_dlc_processed_data_dir\"):\n        return _linking_module.get_dlc_processed_data_dir()\n    else:\n        return get_dlc_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.VideoSet", "title": "<code>VideoSet</code>", "text": "<p>             Bases: <code>dj.Manual</code></p> <p>Collection of videos included in a given training set.</p> <p>Attributes:</p> Name Type Description <code>video_set_id</code> <code>int</code> <p>Unique ID for collection of videos.</p> Source code in <code>element_deeplabcut/train.py</code> <pre><code>@schema\nclass VideoSet(dj.Manual):\n\"\"\"Collection of videos included in a given training set.\n\n    Attributes:\n        video_set_id (int): Unique ID for collection of videos.\"\"\"\n\n    definition = \"\"\" # Set of vids in training set\n    video_set_id: int\n    \"\"\"\n\n    class File(dj.Part):\n\"\"\"File IDs and paths in a given VideoSet\n\n        Attributes:\n            VideoSet (foreign key): VideoSet key.\n            file_path ( varchar(255) ): Path to file on disk relative to root.\"\"\"\n\n        definition = \"\"\" # Paths of training files (e.g., labeled pngs, CSV or video)\n        -&gt; master\n        file_id: int\n        ---\n        file_path: varchar(255)\n        \"\"\"\n</code></pre>"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.VideoSet.File", "title": "<code>File</code>", "text": "<p>             Bases: <code>dj.Part</code></p> <p>File IDs and paths in a given VideoSet</p> <p>Attributes:</p> Name Type Description <code>VideoSet</code> <code>foreign key</code> <p>VideoSet key.</p> <code>file_path</code> <code> varchar(255) </code> <p>Path to file on disk relative to root.</p> Source code in <code>element_deeplabcut/train.py</code> <pre><code>class File(dj.Part):\n\"\"\"File IDs and paths in a given VideoSet\n\n    Attributes:\n        VideoSet (foreign key): VideoSet key.\n        file_path ( varchar(255) ): Path to file on disk relative to root.\"\"\"\n\n    definition = \"\"\" # Paths of training files (e.g., labeled pngs, CSV or video)\n    -&gt; master\n    file_id: int\n    ---\n    file_path: varchar(255)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.TrainingParamSet", "title": "<code>TrainingParamSet</code>", "text": "<p>             Bases: <code>dj.Lookup</code></p> <p>Parameters used to train a model</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>smallint</code> <p>Index uniqely identifying paramset.</p> <code>paramset_desc</code> <code> varchar(128) </code> <p>Description of paramset.</p> <code>param_set_hash</code> <code>uuid</code> <p>Hash identifying this paramset.</p> <code>params</code> <code>longblob</code> <p>Dictionary of all applicable parameters.</p> <code>Note</code> <code>longblob</code> <p>param_set_hash must be unique.</p> Source code in <code>element_deeplabcut/train.py</code> <pre><code>@schema\nclass TrainingParamSet(dj.Lookup):\n\"\"\"Parameters used to train a model\n\n    Attributes:\n        paramset_idx (smallint): Index uniqely identifying paramset.\n        paramset_desc ( varchar(128) ): Description of paramset.\n        param_set_hash (uuid): Hash identifying this paramset.\n        params (longblob): Dictionary of all applicable parameters.\n        Note: param_set_hash must be unique.\"\"\"\n\n    definition = \"\"\"\n    # Parameters to specify a DLC model training instance\n    # For DLC \u2264 v2.0, include scorer_legacy = True in params\n    paramset_idx                  : smallint\n    ---\n    paramset_desc: varchar(128)\n    param_set_hash                : uuid      # hash identifying this parameterset\n    unique index (param_set_hash)\n    params                        : longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    required_parameters = (\"shuffle\", \"trainingsetindex\")\n    skipped_parameters = (\"project_path\", \"video_sets\")\n\n    @classmethod\n    def insert_new_params(\n        cls, paramset_desc: str, params: dict, paramset_idx: int = None\n    ):\n\"\"\"\n        Insert a new set of training parameters into dlc.TrainingParamSet.\n\n        Args:\n            paramset_desc (str): Description of parameter set to be inserted\n            params (dict): Dictionary including all settings to specify model training.\n                        Must include shuffle &amp; trainingsetindex b/c not in config.yaml.\n                        project_path and video_sets will be overwritten by config.yaml.\n                        Note that trainingsetindex is 0-indexed\n            paramset_idx (int): optional, integer to represent parameters.\n        \"\"\"\n\n        for required_param in cls.required_parameters:\n            assert required_param in params, (\n                \"Missing required parameter: \" + required_param\n            )\n        for skipped_param in cls.skipped_parameters:\n            if skipped_param in params:\n                params.pop(skipped_param)\n\n        if paramset_idx is None:\n            paramset_idx = (\n                dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n            ) + 1\n\n        param_dict = {\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n        # If the specified param-set already exists\n        if param_query:\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if existing_paramset_idx == int(paramset_idx):  # If existing_idx same:\n                return  # job done\n        else:\n            cls.insert1(param_dict)  # if duplicate, will raise duplicate error\n</code></pre>"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.TrainingParamSet.insert_new_params", "title": "<code>insert_new_params(paramset_desc, params, paramset_idx=None)</code>  <code>classmethod</code>", "text": "<p>Insert a new set of training parameters into dlc.TrainingParamSet.</p> <p>Parameters:</p> Name Type Description Default <code>paramset_desc</code> <code>str</code> <p>Description of parameter set to be inserted</p> required <code>params</code> <code>dict</code> <p>Dictionary including all settings to specify model training.         Must include shuffle &amp; trainingsetindex b/c not in config.yaml.         project_path and video_sets will be overwritten by config.yaml.         Note that trainingsetindex is 0-indexed</p> required <code>paramset_idx</code> <code>int</code> <p>optional, integer to represent parameters.</p> <code>None</code> Source code in <code>element_deeplabcut/train.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls, paramset_desc: str, params: dict, paramset_idx: int = None\n):\n\"\"\"\n    Insert a new set of training parameters into dlc.TrainingParamSet.\n\n    Args:\n        paramset_desc (str): Description of parameter set to be inserted\n        params (dict): Dictionary including all settings to specify model training.\n                    Must include shuffle &amp; trainingsetindex b/c not in config.yaml.\n                    project_path and video_sets will be overwritten by config.yaml.\n                    Note that trainingsetindex is 0-indexed\n        paramset_idx (int): optional, integer to represent parameters.\n    \"\"\"\n\n    for required_param in cls.required_parameters:\n        assert required_param in params, (\n            \"Missing required parameter: \" + required_param\n        )\n    for skipped_param in cls.skipped_parameters:\n        if skipped_param in params:\n            params.pop(skipped_param)\n\n    if paramset_idx is None:\n        paramset_idx = (\n            dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n        ) + 1\n\n    param_dict = {\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n    # If the specified param-set already exists\n    if param_query:\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if existing_paramset_idx == int(paramset_idx):  # If existing_idx same:\n            return  # job done\n    else:\n        cls.insert1(param_dict)  # if duplicate, will raise duplicate error\n</code></pre>"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.TrainingTask", "title": "<code>TrainingTask</code>", "text": "<p>             Bases: <code>dj.Manual</code></p> <p>Staging table for pairing videosets and training parameter sets</p> <p>Attributes:</p> Name Type Description <code>VideoSet</code> <code>foreign key</code> <p>VideoSet Key.</p> <code>TrainingParamSet</code> <code>foreign key</code> <p>TrainingParamSet key.</p> <code>training_id</code> <code>int</code> <p>Unique ID for training task.</p> <code>model_prefix</code> <code> varchar(32) </code> <p>Optional. Prefix for model files.</p> <code>project_path</code> <code> varchar(255) </code> <p>Optional. DLC's project_path in config relative                            to get_dlc_root_data_dir</p> Source code in <code>element_deeplabcut/train.py</code> <pre><code>@schema\nclass TrainingTask(dj.Manual):\n\"\"\"Staging table for pairing videosets and training parameter sets\n\n    Attributes:\n        VideoSet (foreign key): VideoSet Key.\n        TrainingParamSet (foreign key): TrainingParamSet key.\n        training_id (int): Unique ID for training task.\n        model_prefix ( varchar(32) ): Optional. Prefix for model files.\n        project_path ( varchar(255) ): Optional. DLC's project_path in config relative\n                                       to get_dlc_root_data_dir\n    \"\"\"\n\n    definition = \"\"\"      # Specification for a DLC model training instance\n    -&gt; VideoSet           # labeled video(s) for training\n    -&gt; TrainingParamSet\n    training_id     : int\n    ---\n    model_prefix='' : varchar(32)\n    project_path='' : varchar(255) # DLC's project_path in config relative to root\n    \"\"\"\n</code></pre>"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.ModelTraining", "title": "<code>ModelTraining</code>", "text": "<p>             Bases: <code>dj.Computed</code></p> <p>Automated Model training information.</p> <p>Attributes:</p> Name Type Description <code>TrainingTask</code> <code>foreign key</code> <p>TrainingTask key.</p> <code>latest_snapshot</code> <code>int unsigned</code> <p>Latest exact snapshot index (i.e., never -1).</p> <code>config_template</code> <code>longblob</code> <p>Stored full config file.</p> Source code in <code>element_deeplabcut/train.py</code> <pre><code>@schema\nclass ModelTraining(dj.Computed):\n\"\"\"Automated Model training information.\n\n    Attributes:\n        TrainingTask (foreign key): TrainingTask key.\n        latest_snapshot (int unsigned): Latest exact snapshot index (i.e., never -1).\n        config_template (longblob): Stored full config file.\"\"\"\n\n    definition = \"\"\"\n    -&gt; TrainingTask\n    ---\n    latest_snapshot: int unsigned # latest exact snapshot index (i.e., never -1)\n    config_template: longblob     # stored full config file\n    \"\"\"\n\n    # To continue from previous training snapshot, devs suggest editing pose_cfg.yml\n    # https://github.com/DeepLabCut/DeepLabCut/issues/70\n\n    def make(self, key):\n        from deeplabcut import train_network  # isort:skip\n\n        try:\n            from deeplabcut.utils.auxiliaryfunctions import (\n                get_model_folder,\n                edit_config,\n            )  # isort:skip\n        except ImportError:\n            from deeplabcut.utils.auxiliaryfunctions import (\n                GetModelFolder as get_model_folder,\n            )  # isort:skip\n\n\"\"\"Launch training for each train.TrainingTask training_id via `.populate()`.\"\"\"\n        project_path, model_prefix = (TrainingTask &amp; key).fetch1(\n            \"project_path\", \"model_prefix\"\n        )\n\n        project_path = find_full_path(get_dlc_root_data_dir(), project_path)\n\n        # ---- Build and save DLC configuration (yaml) file ----\n        _, dlc_config = dlc_reader.read_yaml(project_path)  # load existing\n        dlc_config.update((TrainingParamSet &amp; key).fetch1(\"params\"))\n        dlc_config.update(\n            {\n                \"project_path\": project_path.as_posix(),\n                \"modelprefix\": model_prefix,\n                \"train_fraction\": dlc_config[\"TrainingFraction\"][\n                    int(dlc_config[\"trainingsetindex\"])\n                ],\n                \"training_filelist_datajoint\": [  # don't overwrite origin video_sets\n                    find_full_path(get_dlc_root_data_dir(), fp).as_posix()\n                    for fp in (VideoSet.File &amp; key).fetch(\"file_path\")\n                ],\n            }\n        )\n        # Write dlc config file to base project folder\n        dlc_cfg_filepath = dlc_reader.save_yaml(project_path, dlc_config)\n\n        # ---- Update the project path in the DLC pose configuration (yaml) files ----\n        model_folder = get_model_folder(\n            trainFraction=dlc_config[\"train_fraction\"],\n            shuffle=dlc_config[\"shuffle\"],\n            cfg=dlc_config,\n            modelprefix=dlc_config[\"modelprefix\"],\n        )\n        model_train_folder = project_path / model_folder / \"train\"\n\n        edit_config(\n            model_train_folder / \"pose_cfg.yaml\",\n            {\"project_path\": project_path.as_posix()},\n        )\n\n        # ---- Trigger DLC model training job ----\n        train_network_input_args = list(inspect.signature(train_network).parameters)\n        train_network_kwargs = {\n            k: int(v) if k in (\"shuffle\", \"trainingsetindex\", \"maxiters\") else v\n            for k, v in dlc_config.items()\n            if k in train_network_input_args\n        }\n        for k in [\"shuffle\", \"trainingsetindex\", \"maxiters\"]:\n            train_network_kwargs[k] = int(train_network_kwargs[k])\n\n        try:\n            train_network(dlc_cfg_filepath, **train_network_kwargs)\n        except KeyboardInterrupt:  # Instructions indicate to train until interrupt\n            print(\"DLC training stopped via Keyboard Interrupt\")\n\n        snapshots = list(model_train_folder.glob(\"*index*\"))\n        max_modified_time = 0\n        # DLC goes by snapshot magnitude when judging 'latest' for evaluation\n        # Here, we mean most recently generated\n        for snapshot in snapshots:\n            modified_time = os.path.getmtime(snapshot)\n            if modified_time &gt; max_modified_time:\n                latest_snapshot = int(snapshot.stem[9:])\n                max_modified_time = modified_time\n\n        self.insert1(\n            {**key, \"latest_snapshot\": latest_snapshot, \"config_template\": dlc_config}\n        )\n</code></pre>"}, {"location": "api/element_deeplabcut/version/", "title": "version.py", "text": "<p>Package metadata</p>"}, {"location": "api/element_deeplabcut/export/nwb/", "title": "nwb.py", "text": "<p>Portions of code adapted from DeepLabCut/DLC2NWB MIT License Copyright (c) 2022 Alexander Mathis DataJoint export methods for DeepLabCut 2.x</p>"}, {"location": "api/element_deeplabcut/export/nwb/#element_deeplabcut.export.nwb.dlc_session_to_nwb", "title": "<code>dlc_session_to_nwb(keys, use_element_session=True, session_kwargs=None)</code>", "text": "<p>Using keys from PoseEstimation table, save DLC's h5 output to NWB.</p> <p>Calls DLC2NWB to export NWB file using current h5 on disk. If use_element_session, calls NWB export function from Elements for lab, animal and session, passing session_kwargs. Saves output based on naming convention in DLC2NWB. If output path already exists, returns output path without making changes to the file. NOTE: does not support multianimal exports</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list</code> <p>One or more keys from model.PoseEstimation</p> required <code>use_element_session</code> <code>bool</code> <p>Optional. If True, call NWB export from Element Session</p> <code>True</code> <code>session_kwargs</code> <code>dict</code> <p>Optional. Additional keyword args for Element Session export</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Output path of saved file</p> Source code in <code>element_deeplabcut/export/nwb.py</code> <pre><code>def dlc_session_to_nwb(\n    keys: list, use_element_session: bool = True, session_kwargs: dict = None\n) -&gt; str:\n\"\"\"Using keys from PoseEstimation table, save DLC's h5 output to NWB.\n\n    Calls DLC2NWB to export NWB file using current h5 on disk. If use_element_session,\n    calls NWB export function from Elements for lab, animal and session, passing\n    session_kwargs. Saves output based on naming convention in DLC2NWB. If output path\n    already exists, returns output path without making changes to the file.\n    NOTE: does not support multianimal exports\n\n    Args:\n        keys: One or more keys from model.PoseEstimation\n        use_element_session: Optional. If True, call NWB export from Element Session\n        session_kwargs: Optional. Additional keyword args for Element Session export\n\n    Returns:\n        Output path of saved file\n    \"\"\"\n    if not isinstance(keys, abc.Sequence):  # Ensure list for following loop\n        keys = [keys]\n\n    for key in keys:\n        write_file = True\n        subject_id = key[\"subject\"]\n        output_dir = model.PoseEstimationTask.infer_output_dir(key)\n        config_file = str(output_dir / \"dj_dlc_config.yaml\")\n        video_name = Path((model.VideoRecording.File &amp; key).fetch1(\"file_path\")).stem\n        h5file = next(output_dir.glob(f\"{video_name}*h5\"))\n        output_path = h5file.replace(\".h5\", f\"_{subject_id}.nwb\")  # DLC2NWB convention\n\n        if Path(output_path).exists():\n            logger.warning(f\"Skipping {subject_id}. NWB already exists.\")\n            write_file = False\n\n        # Use standard DLC2NWB export\n        if write_file and not use_element_session:\n            output_path = convert_h5_to_nwb(config_file, h5file, subject_id)\n\n        # Pass Element Session export items in export\n        if write_file and use_element_session:\n            from element_session.export.nwb import session_to_nwb\n\n            session_nwb = session_to_nwb(key, **session_kwargs)  # call session export\n            dlc_nwb = write_subject_to_nwb(session_nwb, h5file, subject_id, config_file)\n            # warnings filter from DLC2NWB\n            with warnings.catch_warnings(), NWBHDF5IO(output_path, mode=\"w\") as io:\n                warnings.filterwarnings(\"ignore\", category=DtypeConversionWarning)\n                io.write(dlc_nwb)\n\n    return output_path\n</code></pre>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/", "title": "dlc_reader.py", "text": ""}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation", "title": "<code>PoseEstimation</code>", "text": "<p>Class for handling DLC pose estimation files.</p> Source code in <code>element_deeplabcut/readers/dlc_reader.py</code> <pre><code>class PoseEstimation:\n\"\"\"Class for handling DLC pose estimation files.\"\"\"\n\n    def __init__(\n        self,\n        dlc_dir: str = None,\n        pkl_path: str = None,\n        h5_path: str = None,\n        yml_path: str = None,\n        filename_prefix: str = \"\",\n    ):\n        if dlc_dir is None:\n            assert pkl_path and h5_path and yml_path, (\n                'If \"dlc_dir\" is not provided, then pkl_path, h5_path, and yml_path '\n                + \"must be provided\"\n            )\n        else:\n            self.dlc_dir = Path(dlc_dir)\n            if not self.dlc_dir.exists():\n                raise FileNotFoundError(f\"Unable to find {dlc_dir}\")\n\n        # meta file: pkl - info about this DLC run (input video, configuration, etc.)\n        if pkl_path is None:\n            self.pkl_paths = sorted(\n                self.dlc_dir.rglob(f\"{filename_prefix}*meta.pickle\")\n            )\n            if not len(self.pkl_paths) &gt; 0:\n                raise FileNotFoundError(\n                    f\"No meta file (.pickle) found in: {self.dlc_dir}\"\n                )\n        else:\n            pkl_path = Path(pkl_path)\n            if not pkl_path.exists():\n                raise FileNotFoundError(f\"{pkl_path} not found\")\n            self.pkl_paths = [pkl_path]\n\n        # data file: h5 - body part outputs from the DLC post estimation step\n        if h5_path is None:\n            self.h5_paths = sorted(self.dlc_dir.rglob(f\"{filename_prefix}*.h5\"))\n            if not len(self.h5_paths) &gt; 0:\n                raise FileNotFoundError(\n                    f\"No DLC output file (.h5) found in: {self.dlc_dir}\"\n                )\n        else:\n            h5_path = Path(h5_path)\n            if not h5_path.exists():\n                raise FileNotFoundError(f\"{h5_path} not found\")\n            self.h5_paths = [h5_path]\n\n        # validate number of files\n        assert len(self.h5_paths) == len(\n            self.pkl_paths\n        ), f\"Unequal number of .h5 files ({len(self.h5_paths)}) and .pickle files ({len(self.pkl_paths)})\"\n\n        assert (\n            self.pkl_paths[0].stem == self.h5_paths[0].stem + \"_meta\"\n        ), f\"Mismatching h5 ({self.h5_paths[0].stem}) and pickle {self.pkl_paths[0].stem}\"\n\n        # config file: yaml - configuration for invoking the DLC post estimation step\n        if yml_path is None:\n            yml_paths = list(self.dlc_dir.glob(f\"{filename_prefix}*.y*ml\"))\n            # If multiple, defer to the one we save.\n            if len(yml_paths) &gt; 1:\n                yml_paths = [val for val in yml_paths if val.stem == \"dj_dlc_config\"]\n            if len(yml_paths) != 1:\n                raise FileNotFoundError(\n                    f\"Unable to find one unique .yaml file in: {dlc_dir} - Found: {len(yml_paths)}\"\n                )\n            self.yml_path = yml_paths[0]\n        else:\n            self.yml_path = Path(yml_path)\n            if not self.yml_path.exists():\n                raise FileNotFoundError(f\"{self.yml_path} not found\")\n\n        self._pkl = None\n        self._rawdata = None\n        self._yml = None\n        self._data = None\n\n        train_idx = np.where(\n            (np.array(self.yml[\"TrainingFraction\"]) * 100).astype(int)\n            == int(self.pkl[\"training set fraction\"] * 100)\n        )[0][0]\n        train_iter = int(self.pkl[\"Scorer\"].split(\"_\")[-1])\n\n        self.model = {\n            \"Scorer\": self.pkl[\"Scorer\"],\n            \"Task\": self.yml[\"Task\"],\n            \"date\": self.yml[\"date\"],\n            \"iteration\": self.pkl[\"iteration (active-learning)\"],\n            \"shuffle\": int(re.search(r\"shuffle(\\d+)\", self.pkl[\"Scorer\"]).groups()[0]),\n            \"snapshotindex\": self.yml[\"snapshotindex\"],\n            \"trainingsetindex\": train_idx,\n            \"training_iteration\": train_iter,\n        }\n\n        self.fps = self.pkl[\"fps\"]\n        self.nframes = self.pkl[\"nframes\"]\n        self.creation_time = self.h5_paths[0].stat().st_mtime\n\n    @property\n    def pkl(self):\n\"\"\"Pickle file contents\"\"\"\n        if self._pkl is None:\n            nframes = 0\n            meta_hash = None\n            for fp in self.pkl_paths:\n                with open(fp, \"rb\") as f:\n                    meta = pickle.load(f)\n                nframes += meta[\"data\"].pop(\"nframes\")\n\n                # remove variable fields\n                for k in (\"start\", \"stop\", \"run_duration\"):\n                    meta[\"data\"].pop(k)\n\n                # confirm identical setting in all .pickle files\n                if meta_hash is None:\n                    meta_hash = dict_to_uuid(meta)\n                else:\n                    assert meta_hash == dict_to_uuid(\n                        meta\n                    ), f\"Inconsistent DLC-model-config file used: {fp}\"\n\n            self._pkl = meta[\"data\"]\n            self._pkl[\"nframes\"] = nframes\n        return self._pkl\n\n    @property\n    def yml(self):\n\"\"\"json-structured config.yaml file contents\"\"\"\n        if self._yml is None:\n            with open(self.yml_path, \"rb\") as f:\n                self._yml = yaml.safe_load(f)\n        return self._yml\n\n    @property\n    def rawdata(self):\n\"\"\"Raw data from h5 file\"\"\"\n        if self._rawdata is None:\n            self._rawdata = pd.concat([pd.read_hdf(fp) for fp in self.h5_paths])\n        return self._rawdata\n\n    @property\n    def data(self):\n\"\"\"Data from the h5 file, restructured as a dict\"\"\"\n        if self._data is None:\n            self._data = self.reformat_rawdata()\n        return self._data\n\n    @property\n    def df(self):\n\"\"\"Data as dataframe\"\"\"\n        top_level = self.rawdata.columns.levels[0][0]\n        return self.rawdata.get(top_level)\n\n    @property\n    def body_parts(self):\n\"\"\"Set of body parts present in data file\"\"\"\n        return self.df.columns.levels[0]\n\n    def reformat_rawdata(self):\n\"\"\"Transform raw h5 data into dict\"\"\"\n        error_message = (\n            f\"Total frames from .h5 file ({len(self.rawdata)}) differs \"\n            + f'from .pickle ({self.pkl[\"nframes\"]})'\n        )\n        assert len(self.rawdata) == self.pkl[\"nframes\"], error_message\n\n        body_parts_position = {}\n        for body_part in self.body_parts:\n            body_parts_position[body_part] = {\n                c: self.df.get(body_part).get(c).values\n                for c in self.df.get(body_part).columns\n            }\n\n        return body_parts_position\n</code></pre>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.pkl", "title": "<code>pkl</code>  <code>property</code>", "text": "<p>Pickle file contents</p>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.yml", "title": "<code>yml</code>  <code>property</code>", "text": "<p>json-structured config.yaml file contents</p>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.rawdata", "title": "<code>rawdata</code>  <code>property</code>", "text": "<p>Raw data from h5 file</p>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.data", "title": "<code>data</code>  <code>property</code>", "text": "<p>Data from the h5 file, restructured as a dict</p>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.df", "title": "<code>df</code>  <code>property</code>", "text": "<p>Data as dataframe</p>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.body_parts", "title": "<code>body_parts</code>  <code>property</code>", "text": "<p>Set of body parts present in data file</p>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.reformat_rawdata", "title": "<code>reformat_rawdata()</code>", "text": "<p>Transform raw h5 data into dict</p> Source code in <code>element_deeplabcut/readers/dlc_reader.py</code> <pre><code>def reformat_rawdata(self):\n\"\"\"Transform raw h5 data into dict\"\"\"\n    error_message = (\n        f\"Total frames from .h5 file ({len(self.rawdata)}) differs \"\n        + f'from .pickle ({self.pkl[\"nframes\"]})'\n    )\n    assert len(self.rawdata) == self.pkl[\"nframes\"], error_message\n\n    body_parts_position = {}\n    for body_part in self.body_parts:\n        body_parts_position[body_part] = {\n            c: self.df.get(body_part).get(c).values\n            for c in self.df.get(body_part).columns\n        }\n\n    return body_parts_position\n</code></pre>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.read_yaml", "title": "<code>read_yaml(fullpath, filename='*')</code>", "text": "<p>Return contents of yml in fullpath. If available, defer to DJ-saved version</p> <p>Parameters:</p> Name Type Description Default <code>fullpath</code> <code>str</code> <p>String or pathlib path. Directory with yaml files</p> required <code>filename</code> <code>str</code> <p>Filename, no extension. Permits wildcards.</p> <code>'*'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (a) filepath as pathlib.PosixPath and (b) file contents as dict</p> Source code in <code>element_deeplabcut/readers/dlc_reader.py</code> <pre><code>def read_yaml(fullpath: str, filename: str = \"*\") -&gt; tuple:\n\"\"\"Return contents of yml in fullpath. If available, defer to DJ-saved version\n\n    Args:\n        fullpath (str): String or pathlib path. Directory with yaml files\n        filename (str, optional): Filename, no extension. Permits wildcards.\n\n    Returns:\n        Tuple of (a) filepath as pathlib.PosixPath and (b) file contents as dict\n    \"\"\"\n    from deeplabcut.utils.auxiliaryfunctions import read_config\n\n    # Take the DJ-saved if there. If not, return list of available\n    yml_paths = list(Path(fullpath).glob(\"dj_dlc_config.yaml\")) or sorted(\n        list(Path(fullpath).glob(f\"{filename}.y*ml\"))\n    )\n\n    assert (  # If more than 1 and not DJ-saved,\n        len(yml_paths) == 1\n    ), f\"Found more yaml files than expected: {len(yml_paths)}\\n{fullpath}\"\n\n    return yml_paths[0], read_config(yml_paths[0])\n</code></pre>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.save_yaml", "title": "<code>save_yaml(output_dir, config_dict, filename='dj_dlc_config', mkdir=True)</code>", "text": "<p>Save config_dict to output_path as filename.yaml. By default, preserves original.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>where to save yaml file</p> required <code>config_dict</code> <code>str</code> <p>dict of config params or element-deeplabcut model.Model dict</p> required <code>filename</code> <code>str</code> <p>default 'dj_dlc_config' or preserve original 'config' Set to 'config' to overwrite original file. If extension is included, removed and replaced with \"yaml\".</p> <code>'dj_dlc_config'</code> <code>mkdir</code> <code>bool</code> <p>Optional, True. Make new directory if output_dir not exist</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>path of saved file as string - due to DLC func preference for strings</p> Source code in <code>element_deeplabcut/readers/dlc_reader.py</code> <pre><code>def save_yaml(\n    output_dir: str,\n    config_dict: dict,\n    filename: str = \"dj_dlc_config\",\n    mkdir: bool = True,\n) -&gt; str:\n\"\"\"Save config_dict to output_path as filename.yaml. By default, preserves original.\n\n    Args:\n        output_dir (str): where to save yaml file\n        config_dict (str): dict of config params or element-deeplabcut model.Model dict\n        filename (str, optional): default 'dj_dlc_config' or preserve original 'config'\n            Set to 'config' to overwrite original file.\n            If extension is included, removed and replaced with \"yaml\".\n        mkdir (bool): Optional, True. Make new directory if output_dir not exist\n\n    Returns:\n        path of saved file as string - due to DLC func preference for strings\n    \"\"\"\n    from deeplabcut.utils.auxiliaryfunctions import write_config\n\n    if \"config_template\" in config_dict:  # if passed full model.Model dict\n        config_dict = config_dict[\"config_template\"]\n    if mkdir:\n        output_dir.mkdir(exist_ok=True)\n    if \".\" in filename:  # if user provided extension, remove\n        filename = filename.split(\".\")[0]\n\n    output_filepath = Path(output_dir) / f\"{filename}.yaml\"\n    write_config(output_filepath, config_dict)\n    return str(output_filepath)\n</code></pre>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.do_pose_estimation", "title": "<code>do_pose_estimation(key, video_filepaths, dlc_model, project_path, output_dir, videotype='', gputouse=None, save_as_csv=False, batchsize=None, cropping=None, TFGPUinference=True, dynamic=(False, 0.5, 10), robust_nframes=False, allow_growth=False, use_shelve=False)</code>", "text": "<p>Launch DLC's analyze_videos within element-deeplabcut.</p> <p>Also saves a copy of the current config in the output dir, with ensuring analyzed videos in the video_set. NOTE: Config-specificed cropping not supported when adding to config in this manner.</p> <p>Parameters:</p> Name Type Description Default <code>video_filepaths</code> <code>list</code> <p>list of videos to analyze</p> required <code>dlc_model</code> <code>dict</code> <p>element-deeplabcut dlc.Model</p> required <code>project_path</code> <code>str</code> <p>path to project config.yml</p> required <code>output_dir</code> <code>str</code> <p>where to save output</p> required <code>videotype</code> <code>str, optional, default=\"\"</code> <p>Checks for the extension of the video in case the input to the video is a directory. Only videos with this extension are analyzed. If unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.</p> <code>''</code> <code>gputouse</code> <code>int or None, optional, default=None</code> <p>Indicates the GPU to use (see number in <code>nvidia-smi</code>). If none, <code>None</code>. See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries</p> <code>None</code> <code>save_as_csv</code> <code>bool, optional, default=False</code> <p>Saves the predictions in a .csv file.</p> <code>False</code> <code>batchsize</code> <code>int or None, optional, default=None</code> <p>Change batch size for inference; if given overwrites <code>pose_cfg.yaml</code></p> <code>None</code> <code>cropping</code> <code>list or None, optional, default=None</code> <p>List of cropping coordinates as [x1, x2, y1, y2]. Note that the same cropping parameters will then be used for all videos. If different video crops are desired, run <code>analyze_videos</code> on individual videos with the corresponding cropping coordinates.</p> <code>None</code> <code>TFGPUinference</code> <code>bool, optional, default=True</code> <p>Perform inference on GPU with TensorFlow code. Introduced in \"Pretraining boosts out-of-domain robustness for pose estimation\" by Alexander Mathis, Mert Y\u00fcksekg\u00f6n\u00fcl, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis. Source https://arxiv.org/abs/1909.11229</p> <code>True</code> <code>dynamic</code> <code>tuple(bool, float, int) triple (state, detectiontreshold, margin</code> <p>If the state is true, then dynamic cropping will be performed. That means that if an object is detected (i.e. any body part &gt; detectiontreshold), then object boundaries are computed according to the smallest/largest x position and smallest/largest y position of all body parts. This  window is expanded by the margin and from then on only the posture within this crop is analyzed (until the object is lost, i.e. &lt;detectiontreshold). The current position is utilized for updating the crop window for the next frame (this is why the margin is important and should be set large enough given the movement of the animal).</p> <code>(False, 0.5, 10)</code> <code>robust_nframes</code> <code>bool, optional, default=False</code> <p>Evaluate a video's number of frames in a robust manner. This option is slower (as the whole video is read frame-by-frame), but does not rely on metadata, hence its robustness against file corruption.</p> <code>False</code> <code>allow_growth</code> <code>bool, optional, default=False.</code> <p>For some smaller GPUs the memory issues happen. If <code>True</code>, the memory allocator does not pre-allocate the entire specified GPU memory region, instead starting small and growing as needed. See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2</p> <code>False</code> <code>use_shelve</code> <code>bool, optional, default=False</code> <p>By default, data are dumped in a pickle file at the end of the video analysis. Otherwise, data are written to disk on the fly using a \"shelf\"; i.e., a pickle-based, persistent, database-like object by default, resulting in constant memory footprint.</p> <code>False</code> Source code in <code>element_deeplabcut/readers/dlc_reader.py</code> <pre><code>def do_pose_estimation(\n    key: dict,\n    video_filepaths: list,\n    dlc_model: dict,\n    project_path: str,\n    output_dir: str,\n    videotype=\"\",\n    gputouse=None,\n    save_as_csv=False,\n    batchsize=None,\n    cropping=None,\n    TFGPUinference=True,\n    dynamic=(False, 0.5, 10),\n    robust_nframes=False,\n    allow_growth=False,\n    use_shelve=False,\n):\n\"\"\"Launch DLC's analyze_videos within element-deeplabcut.\n\n    Also saves a copy of the current config in the output dir, with ensuring analyzed\n    videos in the video_set. NOTE: Config-specificed cropping not supported when adding\n    to config in this manner.\n\n    Args:\n        video_filepaths (list): list of videos to analyze\n        dlc_model (dict): element-deeplabcut dlc.Model\n        project_path (str): path to project config.yml\n        output_dir (str): where to save output\n            # BELOW FROM DLC'S DOCSTRING\n\n        videotype (str, optional, default=\"\"):\n            Checks for the extension of the video in case the input to the video is a\n            directory. Only videos with this extension are analyzed. If unspecified,\n            videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.\n        gputouse (int or None, optional, default=None):\n            Indicates the GPU to use (see number in ``nvidia-smi``). If none, ``None``.\n            See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n        save_as_csv (bool, optional, default=False):\n            Saves the predictions in a .csv file.\n        batchsize (int or None, optional, default=None):\n            Change batch size for inference; if given overwrites ``pose_cfg.yaml``\n        cropping (list or None, optional, default=None):\n            List of cropping coordinates as [x1, x2, y1, y2].\n            Note that the same cropping parameters will then be used for all videos.\n            If different video crops are desired, run ``analyze_videos`` on individual\n            videos with the corresponding cropping coordinates.\n        TFGPUinference (bool, optional, default=True):\n            Perform inference on GPU with TensorFlow code. Introduced in \"Pretraining\n            boosts out-of-domain robustness for pose estimation\" by Alexander Mathis,\n            Mert Y\u00fcksekg\u00f6n\u00fcl, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis.\n            Source https://arxiv.org/abs/1909.11229\n        dynamic (tuple(bool, float, int) triple (state, detectiontreshold, margin)):\n            If the state is true, then dynamic cropping will be performed. That means\n            that if an object is detected (i.e. any body part &gt; detectiontreshold),\n            then object boundaries are computed according to the smallest/largest x\n            position and smallest/largest y position of all body parts. This  window is\n            expanded by the margin and from then on only the posture within this crop\n            is analyzed (until the object is lost, i.e. &lt;detectiontreshold). The\n            current position is utilized for updating the crop window for the next\n            frame (this is why the margin is important and should be set large enough\n            given the movement of the animal).\n        robust_nframes (bool, optional, default=False):\n            Evaluate a video's number of frames in a robust manner.\n            This option is slower (as the whole video is read frame-by-frame),\n            but does not rely on metadata, hence its robustness against file corruption.\n        allow_growth (bool, optional, default=False.):\n            For some smaller GPUs the memory issues happen. If ``True``, the memory\n            allocator does not pre-allocate the entire specified GPU memory region,\n            instead starting small and growing as needed.\n            See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2\n        use_shelve (bool, optional, default=False):\n            By default, data are dumped in a pickle file at the end of the video\n            analysis. Otherwise, data are written to disk on the fly using a \"shelf\";\n            i.e., a pickle-based, persistent, database-like object by default,\n            resulting in constant memory footprint.\n\n    \"\"\"\n    from deeplabcut.pose_estimation_tensorflow import analyze_videos\n\n    # ---- Build and save DLC configuration (yaml) file ----\n    dlc_config = dlc_model[\"config_template\"]\n    dlc_project_path = Path(project_path)\n    dlc_config[\"project_path\"] = dlc_project_path.as_posix()\n\n    # ---- Add current video to config ---\n    for video_filepath in video_filepaths:\n        if video_filepath not in dlc_config[\"video_sets\"]:\n            try:\n                px_width, px_height = (model.RecordingInfo &amp; key).fetch1(\n                    \"px_width\", \"px_height\"\n                )\n            except DataJointError:\n                logger.warn(\n                    f\"Could not find RecordingInfo for {video_filepath.stem}\"\n                    + \"\\n\\tUsing zeros for crop value in config.\"\n                )\n                px_height, px_width = 0, 0\n            dlc_config[\"video_sets\"].update(\n                {str(video_filepath): {\"crop\": f\"0, {px_width}, 0, {px_height}\"}}\n            )\n\n    # ---- Write config files ----\n    # To output dir: Important for loading/parsing output in datajoint\n    _ = save_yaml(output_dir, dlc_config)\n    # To project dir: Required by DLC to run the analyze_videos\n    if dlc_project_path != output_dir:\n        config_filepath = save_yaml(dlc_project_path, dlc_config)\n\n    # ---- Trigger DLC prediction job ----\n    analyze_videos(\n        config=config_filepath,\n        videos=video_filepaths,\n        shuffle=dlc_model[\"shuffle\"],\n        trainingsetindex=dlc_model[\"trainingsetindex\"],\n        destfolder=output_dir,\n        modelprefix=dlc_model[\"model_prefix\"],\n        videotype=videotype,\n        gputouse=gputouse,\n        save_as_csv=save_as_csv,\n        batchsize=batchsize,\n        cropping=cropping,\n        TFGPUinference=TFGPUinference,\n        dynamic=dynamic,\n        robust_nframes=robust_nframes,\n        allow_growth=allow_growth,\n        use_shelve=use_shelve,\n    )\n</code></pre>"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.do_pose_estimation--below-from-dlcs-docstring", "title": "BELOW FROM DLC'S DOCSTRING", "text": ""}, {"location": "api/workflow_deeplabcut/ingest/", "title": "ingest.py", "text": ""}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_subjects", "title": "<code>ingest_subjects(subject_csv_path='./user_data/subjects.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Inserts ./user_data/subject.csv data into corresponding subject schema tables</p> <p>Parameters:</p> Name Type Description Default <code>subject_csv_path</code> <code>str</code> <p>relative path of subject csv</p> <code>'./user_data/subjects.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Display number of entries inserted when ingesting</p> <code>True</code> Source code in <code>workflow_deeplabcut/ingest.py</code> <pre><code>def ingest_subjects(\n    subject_csv_path=\"./user_data/subjects.csv\",\n    skip_duplicates=True,\n    verbose=True,\n):\n\"\"\"Inserts ./user_data/subject.csv data into corresponding subject schema tables\n\n    Args:\n        subject_csv_path (str): relative path of subject csv\n        skip_duplicates (bool): Default True. Passed to DataJoint insert\n        verbose (bool): Display number of entries inserted when ingesting\n    \"\"\"\n    csvs = [subject_csv_path]\n    tables = [subject.Subject()]\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_sessions", "title": "<code>ingest_sessions(session_csv_path='./user_data/sessions.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingests to session schema from ./user_data/sessions.csv</p> <p>Parameters:</p> Name Type Description Default <code>session_csv_path</code> <code>str</code> <p>relative path of session csv</p> <code>'./user_data/sessions.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Default True. Display number of entries inserted when ingesting</p> <code>True</code> Source code in <code>workflow_deeplabcut/ingest.py</code> <pre><code>def ingest_sessions(\n    session_csv_path=\"./user_data/sessions.csv\", skip_duplicates=True, verbose=True\n):\n\"\"\"Ingests to session schema from ./user_data/sessions.csv\n\n    Args:\n        session_csv_path (str): relative path of session csv\n        skip_duplicates (bool): Default True. Passed to DataJoint insert\n        verbose (bool): Default True. Display number of entries inserted when ingesting\n    \"\"\"\n    csvs = [session_csv_path, session_csv_path, session_csv_path]\n    tables = [session.Session(), session.SessionDirectory(), session.SessionNote()]\n\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_train_params", "title": "<code>ingest_train_params(config_params_csv_path, skip_duplicates=True, verbose=True)</code>", "text": "<p>Use provided path to load TrainingParamSet with relative path to config.yaml</p> <p>Parameters:</p> Name Type Description Default <code>config_params_csv_path</code> <code>str</code> <p>relative path of csv with config parameters</p> required <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Default True. Display number of entries inserted when ingesting</p> <code>True</code> Source code in <code>workflow_deeplabcut/ingest.py</code> <pre><code>def ingest_train_params(config_params_csv_path, skip_duplicates=True, verbose=True):\n\"\"\"Use provided path to load TrainingParamSet with relative path to config.yaml\n\n    Args:\n        config_params_csv_path (str): relative path of csv with config parameters\n        skip_duplicates (bool):  Default True. Passed to DataJoint insert\n        verbose (bool): Default True. Display number of entries inserted when ingesting\n    \"\"\"\n    if verbose:\n        previous_length = len(train.TrainingParamSet.fetch())\n    with open(config_params_csv_path, newline=\"\") as f:\n        config_csv = list(csv.DictReader(f, delimiter=\",\"))\n    for line in config_csv:\n        paramset_idx = line.pop(\"paramset_idx\")\n        if skip_duplicates and (\n            paramset_idx in list(train.TrainingParamSet.fetch(\"paramset_idx\"))\n        ):\n            continue\n        paramset_desc = line.pop(\"paramset_desc\")\n        try:\n            config_path = find_full_path(\n                get_dlc_root_data_dir(), line.pop(\"config_path\")\n            )\n        except FileNotFoundError as e:\n            if verbose:\n                print(f\"Skipping {paramset_desc}:\\n\\t{e}\")\n            continue\n        with open(config_path, \"rb\") as y:\n            params = yaml.safe_load(y)\n        params.update({**line})\n\n        train.TrainingParamSet.insert_new_params(\n            paramset_idx=paramset_idx, paramset_desc=paramset_desc, params=params\n        )\n    if verbose:\n        insert_length = len(train.TrainingParamSet.fetch()) - previous_length\n        print(\n            f\"\\n---- Inserting {insert_length} entry(s) into #model_training_param_set \"\n            + \"----\"\n        )\n</code></pre>"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_train_vids", "title": "<code>ingest_train_vids(train_video_csv_path, verbose=False, **kwargs)</code>", "text": "<p>Use provided CSV to insert into train.VideoSet and train.VideoSet.File</p> <p>Parameters:</p> Name Type Description Default <code>train_video_csv_path</code> <code>str</code> <p>relative path of csv with training video info</p> required <code>verbose</code> <code>bool</code> <p>Default False. Display number of entries inserted when ingesting</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Optional. Unused dict of keyword arguments.</p> <code>{}</code> Source code in <code>workflow_deeplabcut/ingest.py</code> <pre><code>def ingest_train_vids(train_video_csv_path: str, verbose: bool = False, **kwargs: dict):\n\"\"\"Use provided CSV to insert into train.VideoSet and train.VideoSet.File\n\n    Args:\n        train_video_csv_path (str): relative path of csv with training video info\n        verbose (bool): Default False. Display number of entries inserted when ingesting\n        **kwargs (dict): Optional. Unused dict of keyword arguments.\n    \"\"\"\n    csvs = [train_video_csv_path, train_video_csv_path]\n    tables = [train.VideoSet(), train.VideoSet.File()]\n    # With current CSV organization, must skip vids, as primary key is duplicated\n    ingest_csv_to_table(csvs, tables, skip_duplicates=True, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_model_vids", "title": "<code>ingest_model_vids(model_video_csv_path, skip_duplicates=True, verbose=False)</code>", "text": "<p>Use provided CSV to insert into model.VideoRecording and VideoRecording.File</p> <p>Parameters:</p> Name Type Description Default <code>model_video_csv_path</code> <code>str</code> <p>relative path of csv with model video info</p> required <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Default False. Display number of entries inserted when ingesting</p> <code>False</code> Source code in <code>workflow_deeplabcut/ingest.py</code> <pre><code>def ingest_model_vids(\n    model_video_csv_path: str, skip_duplicates: bool = True, verbose: bool = False\n):\n\"\"\"Use provided CSV to insert into model.VideoRecording and VideoRecording.File\n\n    Args:\n        model_video_csv_path (str): relative path of csv with model video info\n        skip_duplicates (bool): Default True. Passed to DataJoint insert\n        verbose (bool): Default False. Display number of entries inserted when ingesting\n    \"\"\"\n    csvs = [model_video_csv_path, model_video_csv_path]\n    tables = [model.VideoRecording(), model.VideoRecording.File()]\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_model", "title": "<code>ingest_model(model_model_csv_path, skip_duplicates=True, verbose=False)</code>", "text": "<p>Use provided CSV to insert into model.Model table</p> <p>Parameters:</p> Name Type Description Default <code>model_model_csv_path</code> <code>str</code> <p>relative path of csv with DLC Model info</p> required <code>skip_duplicates</code> <code>bool</code> <p>Default True. Passed to DataJoint insert</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Default False. Display number of entries inserted when ingesting</p> <code>False</code> Source code in <code>workflow_deeplabcut/ingest.py</code> <pre><code>def ingest_model(\n    model_model_csv_path: str, skip_duplicates: bool = True, verbose: bool = False\n):\n\"\"\"Use provided CSV to insert into model.Model table\n\n    Args:\n        model_model_csv_path (str): relative path of csv with DLC Model info\n        skip_duplicates (bool): Default True. Passed to DataJoint insert\n        verbose (bool): Default False. Display number of entries inserted when ingesting\n    \"\"\"\n    # NOTE: not included in ingest_dlc_items because not yet included in notebooks\n\n    with open(model_model_csv_path, newline=\"\") as f:\n        data = list(csv.DictReader(f, delimiter=\",\"))\n\n    if verbose:\n        prev_len = len(model.Model())\n\n    for model_row in data:  # replace relative path with full path\n        model_row[\"dlc_config\"] = find_full_path(\n            get_dlc_root_data_dir(), model_row.pop(\"config_relative_path\")\n        )\n        model_row[\"project_path\"] = Path(model_row[\"dlc_config\"]).parent\n        model_row[\"prompt\"] = str_to_bool(model_row[\"prompt\"])\n        model_name = model_row[\"model_name\"]\n        if skip_duplicates and model_name in model.Model.fetch(\"model_name\"):\n            if verbose:\n                print(f\"Skipping model, name already exists: {model_name}\")\n            continue\n        else:\n            model.Model.insert_new_model(**model_row)\n\n    if verbose:\n        insert_len = len(model.Model()) - prev_len\n        print(f\"\\n---- Inserting {insert_len} entry(s) into model ----\")\n</code></pre>"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_dlc_items", "title": "<code>ingest_dlc_items(config_params_csv_path='./user_data/config_params.csv', train_video_csv_path='./user_data/train_videosets.csv', model_video_csv_path='./user_data/model_videos.csv', skip_duplicates=False, verbose=True)</code>", "text": "<p>Ingests to DLC schema from CSVs</p> <p>Parameters:</p> Name Type Description Default <code>config_params_csv_path</code> <code>str</code> <p>Optional. Csv path for model training config and parameters</p> <code>'./user_data/config_params.csv'</code> <code>train_video_csv_path</code> <code>str</code> <p>Optional. Csv path for list of training videosets</p> <code>'./user_data/train_videosets.csv'</code> <code>model_video_csv_path</code> <code>str</code> <p>Optional. Csv path for list of modeling videos for pose estimation</p> <code>'./user_data/model_videos.csv'</code> Source code in <code>workflow_deeplabcut/ingest.py</code> <pre><code>def ingest_dlc_items(\n    config_params_csv_path: str = \"./user_data/config_params.csv\",\n    train_video_csv_path: str = \"./user_data/train_videosets.csv\",\n    model_video_csv_path: str = \"./user_data/model_videos.csv\",\n    skip_duplicates: bool = False,\n    verbose: bool = True,\n):\n\"\"\"Ingests to DLC schema from CSVs\n\n    Args:\n        config_params_csv_path (str): Optional. Csv path for model training config and\n            parameters\n        train_video_csv_path (str): Optional. Csv path for list of training videosets\n        model_video_csv_path (str): Optional. Csv path for list of modeling videos for\n            pose estimation\n    \"\"\"\n    ingest_train_params(\n        config_params_csv_path=config_params_csv_path,\n        skip_duplicates=skip_duplicates,\n        verbose=verbose,\n    )\n    ingest_train_vids(\n        train_video_csv_path=train_video_csv_path,\n        skip_duplicates=skip_duplicates,\n        verbose=verbose,\n    )\n    ingest_model_vids(\n        model_video_csv_path=model_video_csv_path,\n        skip_duplicates=skip_duplicates,\n        verbose=verbose,\n    )\n</code></pre>"}, {"location": "api/workflow_deeplabcut/load_demo_data/", "title": "load_demo_data.py", "text": ""}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.download_djarchive_dlc_data", "title": "<code>download_djarchive_dlc_data(target_directory='/tmp/test_data/')</code>", "text": "<p>Download DLC demo data from djarchive. Approx .3 GB</p> <p>Parameters:</p> Name Type Description Default <code>target_directory</code> <code>str</code> <p>Where to store the downloaded data.</p> <code>'/tmp/test_data/'</code> Source code in <code>workflow_deeplabcut/load_demo_data.py</code> <pre><code>def download_djarchive_dlc_data(target_directory: str = \"/tmp/test_data/\"):\n\"\"\"Download DLC demo data from djarchive. Approx .3 GB\n\n    Args:\n        target_directory (str, optional): Where to store the downloaded data.\n    \"\"\"\n    import djarchive_client\n\n    client = djarchive_client.client()\n    os.makedirs(target_directory, exist_ok=True)\n\n    client.download(\n        \"workflow-dlc-data\", target_directory=target_directory, revision=\"v1\"\n    )\n</code></pre>"}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.update_pose_cfg", "title": "<code>update_pose_cfg(project='from_top_tracking', net_type=None, update_snapshot=0)</code>", "text": "<p>Updates weight paths to absolute. If update_snapshot, changes weights to snap #</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>Default from 'from_top_tracking'. Poject name/folder in dlc_root_data_dir</p> <code>'from_top_tracking'</code> <code>net_type</code> <code>str</code> <p>Project net weights (e.g., resnet50) If project is 'from_top_tracking', 'mobilenet_v2_1.0'</p> <code>None</code> <code>update_snapshot</code> <code>int</code> <p>Default 0 = no. If -1, highest integer value available. If integer, look for that snapshot.</p> <code>0</code> Source code in <code>workflow_deeplabcut/load_demo_data.py</code> <pre><code>def update_pose_cfg(\n    project: str = \"from_top_tracking\", net_type: str = None, update_snapshot: int = 0\n):\n\"\"\"Updates weight paths to absolute. If update_snapshot, changes weights to snap #\n\n    Args:\n        project (str, optional): Default from 'from_top_tracking'. Poject name/folder in\n            dlc_root_data_dir\n        net_type (str, optional): Project net weights (e.g., resnet50)\n            If project is 'from_top_tracking', 'mobilenet_v2_1.0'\n        update_snapshot (int, optional): Default 0 = no. If -1, highest integer value\n            available. If integer, look for that snapshot.\n    \"\"\"\n    project_path = find_full_path(get_dlc_root_data_dir(), f\"{project}/\")\n    if project == \"from_top_tracking\":\n        net_type == \"mobilenet_v2_1.0\"\n    for phase in [\"test\", \"train\"]:\n        config_search = list(project_path.rglob(f\"{phase}/pose_cfg.yaml\"))\n        if not config_search:\n            print(f\"Couldn't find cfg for {phase}\")\n        config_path = config_search[0]\n        cfg = read_plainconfig(config_path)\n        if update_snapshot and phase == \"train\":\n            # Get available snapshots\n            snaps_on_disk = set(\n                [\n                    int(i.split(\"-\")[1])\n                    for i in [f.stem for f in list(project_path.rglob(\"snapshot-*\"))]\n                ]\n            )\n            # If -1, take most recent\n            if update_snapshot == -1:\n                update_snapshot = snaps_on_disk.pop()  # last in sorted set\n            else:\n                # Assert desired snapshot is available\n                assert (\n                    update_snapshot in snaps_on_disk\n                ), f\"Couldn't find snapshot {update_snapshot} in {config_path.parent}\"\n            # Set snaphot value\n            cfg[\"init_weights\"] = str(\n                config_path.parent / f\"snapshot-{update_snapshot}\"\n            )\n        else:\n            init_weights = Path(\n                cfg[\"init_weights\"]\n            )  # e.g., path/to/snapshot-1 (no ext)\n            cfg[\"init_weights\"] = str(\n                find_full_path(get_deeplabcut_path(), init_weights.parent)\n                / init_weights.name  # need parent/name bc it isn't on disk\n            )\n\n        if net_type:  # if net_type explicitly provided, update\n            cfg[\"net_type\"] = net_type\n\n        # For train, pull datatype_set for next function\n        if phase == \"train\":\n            augmenter_type = cfg.get(\"dataset_type\")\n\n        write_plainconfig(config_path, cfg)\n\n    return augmenter_type\n</code></pre>"}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.setup_bare_project", "title": "<code>setup_bare_project(project='from_top_tracking', net_type=None)</code>", "text": "<p>Adds absolute paths to config files and generates training-datasets folder</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>Default 'from_top_tracking'. DLC project folder</p> <code>'from_top_tracking'</code> <code>net_type</code> <code>str</code> <p>Project net (e.g., resnet50) passed to             creat_training_dataset. If 'from_top', 'mobilenet_v2_1.0'</p> <code>None</code> Source code in <code>workflow_deeplabcut/load_demo_data.py</code> <pre><code>def setup_bare_project(project: str = \"from_top_tracking\", net_type: str = None):\n\"\"\"Adds absolute paths to config files and generates training-datasets folder\n\n    Args:\n        project (str, optional): Default 'from_top_tracking'. DLC project folder\n        net_type (str, optional): Project net (e.g., resnet50) passed to\n                        creat_training_dataset. If 'from_top', 'mobilenet_v2_1.0'\n    \"\"\"\n    from deeplabcut import create_training_dataset\n\n    if \"from_top_tracking\" in project:  # set net_type for example data\n        net_type = \"mobilenet_v2_1.0\"\n\n    if os.path.isabs(project) and Path(project).exists():\n        project_path = Path(project)\n    else:\n        project_path = find_full_path(get_dlc_root_data_dir(), f\"{project}/\")\n\n    # ---- Write roots to project config ----\n    project_config_path = project_path / \"config.yaml\"\n    project_cfg = read_plainconfig(project_config_path)\n    project_cfg[\"project_path\"] = str(project_path)\n    cfg_videoset_paths = {}  # separate to not change during loop\n    for video, value in project_cfg[\"video_sets\"].items():  # add absolute video path\n        cfg_videoset_paths[os.path.join(project_path, video)] = value\n    project_cfg[\"video_sets\"] = cfg_videoset_paths  # save new fullpaths\n    write_plainconfig(project_config_path, project_cfg)\n    # Update train/test pose_cfg, return augmenter type\n    augmenter_type = update_pose_cfg(\n        project=project, net_type=net_type, update_snapshot=-1\n    )\n\n    # ---- Create training dataset ----\n    # Folder deleted from publicly available data to cut down on size\n    _ = create_training_dataset(\n        project_config_path,\n        num_shuffles=1,\n        net_type=net_type,\n        augmenter_type=augmenter_type,\n        posecfg_template=str(next(Path(project_path).rglob(\"train/pose_cfg.y?ml\"))),\n    )\n</code></pre>"}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.shorten_video", "title": "<code>shorten_video(vid_path='from_top_tracking/videos/test.mp4', output_path=None, first_n_sec=2)</code>", "text": "<p>Save the first 2 seconds of a video relative to dlc root dir.</p> <p>Parameters:</p> Name Type Description Default <code>vid_path</code> <code>str</code> <p>Default \"from_top_tracking/videos/test_full.mp4\". Path relative to get_dlc_root_data_dir() root directory</p> <code>'from_top_tracking/videos/test.mp4'</code> <code>output_path</code> <code>str</code> <p>Destination relative to vid_path root. If none, adds '-Ns' to filename, where N in first_n_sec</p> <code>None</code> <code>first_n_sec</code> <code>int</code> <p>Default 2. # of seconds to extract from beginning of video</p> <code>2</code> Source code in <code>workflow_deeplabcut/load_demo_data.py</code> <pre><code>def shorten_video(\n    vid_path: str = \"from_top_tracking/videos/test.mp4\",\n    output_path: str = None,\n    first_n_sec: int = 2,\n):\n\"\"\"Save the first 2 seconds of a video relative to dlc root dir.\n\n    Args:\n        vid_path (str, optional): Default \"from_top_tracking/videos/test_full.mp4\". Path\n            relative to get_dlc_root_data_dir() root directory\n        output_path (str, optional): Destination relative to vid_path root. If none,\n            adds '-Ns' to filename, where N in first_n_sec\n        first_n_sec (int, optional): Default 2. # of seconds to extract from beginning\n            of video\n    \"\"\"\n\n    if os.path.isabs(vid_path) and Path(vid_path).exists():\n        vid_path_full = Path(vid_path)\n    else:\n        vid_path_full = find_full_path(get_dlc_root_data_dir(), vid_path)\n\n    if not output_path:\n        output_path_full = vid_path_full.with_name(\n            vid_path_full.stem + f\"-{first_n_sec}s\" + vid_path_full.suffix\n        )\n    else:\n        output_path_full = (\n            find_root_directory(get_dlc_root_data_dir(), vid_path_full) / output_path\n        )\n\n    cmd = (  # adjust -ss 0 to start later\n        f\"ffmpeg -n -hide_banner -loglevel error -ss 0 -t {first_n_sec} -i \"\n        + f\"{vid_path_full} -vcodec copy -acodec copy {output_path_full}\"\n    )\n    _ = os.system(cmd)\n</code></pre>"}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.revert_checkpoint_file", "title": "<code>revert_checkpoint_file(project='from_top_tracking', original_checkpoint='checkpoint_orig')</code>", "text": "<p>Delete existing checkpoint file and replace with original_checkpoint</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>DLC project name. Defaults to \"from_top_tracking\".</p> <code>'from_top_tracking'</code> <code>original_checkpoint</code> <code>str</code> <p>Original checkpoint file ot use in the revert, in the same directory as the checkpoint file. Defaults to \"checkpoint_orig\".</p> <code>'checkpoint_orig'</code> Source code in <code>workflow_deeplabcut/load_demo_data.py</code> <pre><code>def revert_checkpoint_file(\n    project=\"from_top_tracking\", original_checkpoint=\"checkpoint_orig\"\n):\n\"\"\"Delete existing checkpoint file and replace with original_checkpoint\n\n    Args:\n        project (str, optional): DLC project name. Defaults to \"from_top_tracking\".\n        original_checkpoint (str, optional): Original checkpoint file ot use in the\n            revert, in the same directory as the checkpoint file. Defaults to\n            \"checkpoint_orig\".\n    \"\"\"\n    import shutil\n\n    project_path = find_full_path(get_dlc_root_data_dir(), f\"{project}/\")\n    original_checkpoint_path = list(project_path.rglob(original_checkpoint))\n    assert (\n        len(original_checkpoint_path) == 1\n    ), f\"Found more than one original checkpoint:\\n{original_checkpoint_path}\"\n    shutil.copy(\n        original_checkpoint_path[0], original_checkpoint_path[0].parent / \"checkpoint\"\n    )\n</code></pre>"}, {"location": "api/workflow_deeplabcut/paths/", "title": "paths.py", "text": ""}, {"location": "api/workflow_deeplabcut/paths/#workflow_deeplabcut.paths.get_dlc_root_data_dir", "title": "<code>get_dlc_root_data_dir()</code>", "text": "<p>Returns a list of root directories for Element DeepLabCut</p> Source code in <code>workflow_deeplabcut/paths.py</code> <pre><code>def get_dlc_root_data_dir() -&gt; list:\n\"\"\"Returns a list of root directories for Element DeepLabCut\"\"\"\n    dlc_root_dirs = dj.config.get(\"custom\", {}).get(\"dlc_root_data_dir\")\n    if not dlc_root_dirs:\n        return None\n    elif not isinstance(dlc_root_dirs, abc.Sequence):\n        return list(dlc_root_dirs)\n    else:\n        return dlc_root_dirs\n</code></pre>"}, {"location": "api/workflow_deeplabcut/paths/#workflow_deeplabcut.paths.get_dlc_processed_data_dir", "title": "<code>get_dlc_processed_data_dir()</code>", "text": "<p>Returns an output directory relative to custom 'dlc_output_dir' root</p> Source code in <code>workflow_deeplabcut/paths.py</code> <pre><code>def get_dlc_processed_data_dir() -&gt; str:\n\"\"\"Returns an output directory relative to custom 'dlc_output_dir' root\"\"\"\n    from pathlib import Path\n\n    dlc_output_dir = dj.config.get(\"custom\", {}).get(\"dlc_output_dir\")\n    if dlc_output_dir:\n        return Path(dlc_output_dir)\n    else:\n        return None\n</code></pre>"}, {"location": "api/workflow_deeplabcut/pipeline/", "title": "pipeline.py", "text": ""}, {"location": "api/workflow_deeplabcut/pipeline/#workflow_deeplabcut.pipeline.get_dlc_root_data_dir", "title": "<code>get_dlc_root_data_dir()</code>", "text": "<p>Returns a list of root directories for Element DeepLabCut</p> Source code in <code>workflow_deeplabcut/paths.py</code> <pre><code>def get_dlc_root_data_dir() -&gt; list:\n\"\"\"Returns a list of root directories for Element DeepLabCut\"\"\"\n    dlc_root_dirs = dj.config.get(\"custom\", {}).get(\"dlc_root_data_dir\")\n    if not dlc_root_dirs:\n        return None\n    elif not isinstance(dlc_root_dirs, abc.Sequence):\n        return list(dlc_root_dirs)\n    else:\n        return dlc_root_dirs\n</code></pre>"}, {"location": "api/workflow_deeplabcut/pipeline/#workflow_deeplabcut.pipeline.get_dlc_processed_data_dir", "title": "<code>get_dlc_processed_data_dir()</code>", "text": "<p>Returns an output directory relative to custom 'dlc_output_dir' root</p> Source code in <code>workflow_deeplabcut/paths.py</code> <pre><code>def get_dlc_processed_data_dir() -&gt; str:\n\"\"\"Returns an output directory relative to custom 'dlc_output_dir' root\"\"\"\n    from pathlib import Path\n\n    dlc_output_dir = dj.config.get(\"custom\", {}).get(\"dlc_output_dir\")\n    if dlc_output_dir:\n        return Path(dlc_output_dir)\n    else:\n        return None\n</code></pre>"}, {"location": "api/workflow_deeplabcut/pipeline/#workflow_deeplabcut.pipeline.Device", "title": "<code>Device</code>", "text": "<p>             Bases: <code>dj.Lookup</code></p> <p>Table for managing lab equipment.</p> <p>In Element DeepLabCut, this table is referenced by <code>model.VideoRecording</code>. The primary key is also used to generate inferred output directories when running pose estimation inference. Refer to the <code>definition</code> attribute for the table design.</p> <p>Attributes:</p> Name Type Description <code>device</code> <code> varchar(32) </code> <p>Device short name.</p> <code>modality</code> <code> varchar(64) </code> <p>Modality for which this device is used.</p> <code>description</code> <code> varchar(256) </code> <p>Optional. Description of device.</p> Source code in <code>workflow_deeplabcut/pipeline.py</code> <pre><code>@lab.schema\nclass Device(dj.Lookup):\n\"\"\"Table for managing lab equipment.\n\n    In Element DeepLabCut, this table is referenced by `model.VideoRecording`.\n    The primary key is also used to generate inferred output directories when\n    running pose estimation inference. Refer to the `definition` attribute\n    for the table design.\n\n    Attributes:\n        device ( varchar(32) ): Device short name.\n        modality ( varchar(64) ): Modality for which this device is used.\n        description ( varchar(256) ): Optional. Description of device.\n    \"\"\"\n\n    definition = \"\"\"\n    device             : varchar(32)\n    ---\n    modality           : varchar(64)\n    description=null   : varchar(256)\n    \"\"\"\n    contents = [\n        [\"Camera1\", \"Pose Estimation\", \"Panasonic HC-V380K\"],\n        [\"Camera2\", \"Pose Estimation\", \"Panasonic HC-V770K\"],\n    ]\n</code></pre>"}, {"location": "api/workflow_deeplabcut/process/", "title": "process.py", "text": ""}, {"location": "api/workflow_deeplabcut/process/#workflow_deeplabcut.process.QuietStdOut", "title": "<code>QuietStdOut</code>", "text": "<p>Context for suppressing standard output</p> Source code in <code>workflow_deeplabcut/process.py</code> <pre><code>class QuietStdOut:\n\"\"\"Context for suppressing standard output\"\"\"\n\n    def __enter__(self):\n        self._original_stdout = sys.stdout\n        sys.stdout = open(os.devnull, \"w\")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        sys.stdout.close()\n        sys.stdout = self._original_stdout\n</code></pre>"}, {"location": "api/workflow_deeplabcut/process/#workflow_deeplabcut.process.run", "title": "<code>run(verbose=True, display_progress=True, reserve_jobs=False, suppress_errors=False)</code>", "text": "<p>Run all <code>make</code> methods from element-deeplabcut</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Print which table is in being populated. Default True.</p> <code>True</code> <code>display_progress</code> <code>bool</code> <p>tqdm progress bar. Defaults to True.</p> <code>True</code> <code>reserve_jobs</code> <code>bool</code> <p>Reserves job to populate in asynchronous fashion. Defaults to False.</p> <code>False</code> <code>suppress_errors</code> <code>bool</code> <p>Suppress errors that would halt execution. Defaults to False.</p> <code>False</code> Source code in <code>workflow_deeplabcut/process.py</code> <pre><code>def run(\n    verbose: bool = True,\n    display_progress: bool = True,\n    reserve_jobs: bool = False,\n    suppress_errors: bool = False,\n):\n\"\"\"Run all `make` methods from element-deeplabcut\n\n    Args:\n        verbose (bool, optional): Print which table is in being populated. Default True.\n        display_progress (bool, optional): tqdm progress bar. Defaults to True.\n        reserve_jobs (bool, optional): Reserves job to populate in asynchronous fashion.\n            Defaults to False.\n        suppress_errors (bool, optional): Suppress errors that would halt execution.\n            Defaults to False.\n    \"\"\"\n    populate_settings = {\n        \"display_progress\": display_progress,\n        \"reserve_jobs\": reserve_jobs,\n        \"suppress_errors\": suppress_errors,\n    }\n\n    tables = [\n        train.ModelTraining(),\n        model.RecordingInfo(),\n        model.ModelEvaluation(),\n        model.PoseEstimation(),\n    ]\n\n    with nullcontext() if verbose else QuietStdOut():\n        for table in tables:\n            print(f\"\\n---- Populating {table.table_name} ----\")\n            table.populate(**populate_settings)\n</code></pre>"}, {"location": "api/workflow_deeplabcut/version/", "title": "version.py", "text": "<p>Package metadata Update the Docker image tag in <code>docker-compose.yaml</code> to match</p>"}, {"location": "tutorials/", "title": "Tutorials", "text": ""}, {"location": "tutorials/#installation", "title": "Installation", "text": "<p>Installation of the Element requires an integrated development environment and database. Instructions to setup each of the components can be found on the User Instructions page.  These instructions use the example workflow for Element DeepLabCut, which can be modified for a user's specific experimental requirements.  This example workflow uses four Elements (Lab, Animal, Session, and DeepLabCut) to construct a complete pipeline, and is able to ingest experimental metadata and run model training and inference.</p> <p>The DeepLabCut (DLC) website has a rich library of resources for downloading the software and understanding its various features. This includes getting started with their software (see links below).</p>"}, {"location": "tutorials/#steps-to-run-the-element", "title": "Steps to run the Element", "text": "<p>The Element assumes you:</p> <ol> <li>Have a DLC project folder on your machine. You can declare a project either    from the    DLC GUI    or via a    terminal.</li> <li>Have labeled data in your DLC project folder. Again, this can be done via    the GUI    or a    terminal.</li> </ol> <p>With these steps in place, you can then use the materials below to start training and pose estimation inferences. Training starts by configuring parameters in the <code>train</code> schema, and launching training in the <code>ModelTraining</code> table. When you're happy with the state of a model, you can insert it into the <code>Model</code> table, and pair it with videos to trigger pose estimation inferences via the <code>PoseEstimationTask</code> table in the <code>model</code> schema. See Element Architecture for a full list of table functions.</p>"}, {"location": "tutorials/#videos", "title": "Videos", "text": "<p>The Element DeepLabCut tutorial gives an overview of the workflow files and notebooks as well as core concepts related to DeepLabCut.</p> <p></p>"}, {"location": "tutorials/#notebooks", "title": "Notebooks", "text": "<p>Each of the notebooks in the workflow (download here steps through ways to interact with the Element itself. For convenience, these notebooks are also rendered as part of this site. To try out Elements notebooks in an online Jupyter environment with access to example data, visit CodeBook. (DeepLabCut notebooks coming soon!)</p> <ul> <li>Data Download    highlights how to use DataJoint tools to download a sample model for trying out the Element.</li> <li>Configure    helps configure your local DataJoint installation to point to the correct database.</li> <li>Workflow Structure demonstrates the table    architecture of the Element and key DataJoint basics for interacting with these    tables.</li> <li>Process steps through adding data to these tables and launching    key DeepLabCut features, like model training.</li> <li>Automate    highlights the same steps as above, but utilizing all built-in automation tools.</li> <li>Visualization    demonstrates how to fetch data from the Element to generate figures and label data.</li> <li>Drop schemas    provides the steps for dropping all the tables to start fresh.</li> <li><code>07-NWB-Export</code> (coming soon!) will describe how to export into NWB files. For now,   see below</li> <li>Alternate Dataset    does all of the above, but with a    dataset from DeepLabCut.</li> </ul>"}, {"location": "tutorials/#data-export-to-neurodata-without-borders-nwb", "title": "Data Export to Neurodata Without Borders (NWB)", "text": "<p>The <code>export/nwb.py</code> module calls DLC2NWB to save output generated by Element DeepLabCut as NWB files. The main function, <code>dlc_session_to_nwb</code>, contains a flag to control calling a parallel function in Element Session.</p> <p>Before using, please install DLC2NWB</p> <pre><code>pip install dlc2nwb\n</code></pre> <p>Then, call the export function using keys from the <code>PoseEstimation</code> table.</p> <pre><code>from element_deeplabcut import model\nfrom element_session import session\nfrom element_deeplabcut.export import dlc_session_to_nwb\n\nsession_key = (session.Session &amp; CONDITION)\npose_key = (model.PoseEstimation &amp; session_key).fetch1('KEY')\ndlc_session_to_nwb(pose_key, use_element_session=True, session_kwargs=SESSION_KWARGS)\n</code></pre> <p>Here, <code>CONDITION</code> should uniquely identify a session and <code>SESSION_KWARGS</code> can be any of the items described in the docstring of <code>element_session.export.nwb.session_to_nwb</code> as a dictionary.</p> <p>As DLC2NWB does not currently offer a separate function for generating <code>PoseEstimation</code> objects (see ndx-pose), the current solution is to allow DLC2NWB to write to disk, and optionally rewrite this file using metadata provided by the export function in Element Session.</p>"}, {"location": "tutorials/00-DataDownload_Optional/", "title": "Data Download", "text": "<p>These notebooks are built around data provided by DataJoint, including a well-trained model. For similar content using data from DeepLabCut, see 09-AlternateDataset.</p> <p>DataJoint provides various datasets via <code>djarchive</code>. To pip install...</p> In\u00a0[\u00a0]: Copied! <pre>pip install git+https://github.com/datajoint/djarchive-client.git\n</pre> pip install git+https://github.com/datajoint/djarchive-client.git In\u00a0[1]: Copied! <pre>import os; import djarchive_client\nclient = djarchive_client.client()\n</pre> import os; import djarchive_client client = djarchive_client.client() <p>We can browse available datasets:</p> In\u00a0[2]: Copied! <pre>list(client.datasets())\n</pre> list(client.datasets()) Out[2]: <pre>['t',\n 'workflow-array-ephys-benchmark',\n 'workflow-calcium-imaging-test-set',\n 'workflow-dlc-data',\n 'workflow-facemap',\n 'workflow-trial']</pre> <p>Datasets have different versions available:</p> In\u00a0[3]: Copied! <pre>list(client.revisions())\n</pre> list(client.revisions()) Out[3]: <pre>[('t', '1'),\n ('workflow-array-ephys-benchmark', '0.1.0a4'),\n ('workflow-array-ephys-benchmark', 'v1'),\n ('workflow-calcium-imaging-test-set', '0_1_0a2'),\n ('workflow-dlc-data', 'v1'),\n ('workflow-facemap', '0.0.0'),\n ('workflow-trial', '0.0.0b1')]</pre> <p>We can make a directory for downloading:</p> In\u00a0[6]: Copied! <pre>os.makedirs('/tmp/test_data', exist_ok=True)\n</pre> os.makedirs('/tmp/test_data', exist_ok=True) <p>Then run download for a given set and the revision:</p> In\u00a0[7]: Copied! <pre>client.download('workflow-dlc-data',\n                target_directory='/tmp/test_data/', \n                revision='v1')\n</pre> client.download('workflow-dlc-data',                 target_directory='/tmp/test_data/',                  revision='v1') Out[7]: <pre>(79, 0)</pre> <pre><code>/tmp/test_data/from_top_tracking/\n- config.yml\n- dlc-models/iteration-0/from_top_trackingFeb23-trainset95shuffle1/\n    - test/pose_cfg.yaml\n    - train/\n        - checkpoint\n        - checkpoint_orig\n        \u2500 learning_stats.csv\n        \u2500 log.txt\n        \u2500 pose_cfg.yaml\n        \u2500 snapshot-10300.data-00000-of-00001\n        \u2500 snapshot-10300.index\n        \u2500 snapshot-10300.meta   # same for 103000\n- labeled-data/\n    - train1/\n        - CollectedData_DJ.csv\n        - CollectedData_DJ.h5\n        - img00674.png          # and others\n    - train2/                   # similar to above\n- videos/\n    - test.mp4\n    - train1.mp4\n</code></pre> <p>We will use this dataset as an example across this series of notebooks. If you use another dataset, change the path accordingly.</p> <ul> <li><code>config.yaml</code> contains key parameters of the project</li> <li><code>labeled-data</code> includes pixel coordinates for each body part</li> <li><code>videos</code> includes the full training and inference videos</li> </ul> <p>This workflow contains additional functions for setting up this demo data, including adding absolute paths to config files and shortening the inference video to speed up pose estimation.</p> In\u00a0[1]: Copied! <pre>from workflow_deeplabcut.load_demo_data import setup_bare_project, shorten_video\n\nsetup_bare_project(project=\"/tmp/test_data/from_top_tracking\", \n                   net_type = \"mobilenet_v2_1.0\") # sets paths\nshorten_video(\"/tmp/test_data/from_top_tracking/videos/test.mp4\",\n              output_path=None,first_n_sec=2) # makes test-2s.mp4\n</pre> from workflow_deeplabcut.load_demo_data import setup_bare_project, shorten_video  setup_bare_project(project=\"/tmp/test_data/from_top_tracking\",                     net_type = \"mobilenet_v2_1.0\") # sets paths shorten_video(\"/tmp/test_data/from_top_tracking/videos/test.mp4\",               output_path=None,first_n_sec=2) # makes test-2s.mp4 <pre>Loading DLC 2.2.1.1...\nDLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n</pre> <p>For your own data, we recommend using the DLC gui to intitialize your project and label the data.</p> <p>In the next notebook, 01-Configure, we'll set up the DataJoint config file with a pointer to your root data directory.</p>"}, {"location": "tutorials/00-DataDownload_Optional/#datajoint-u24-workflow-deeplabcut", "title": "DataJoint U24 - Workflow DeepLabCut\u00b6", "text": ""}, {"location": "tutorials/00-DataDownload_Optional/#download-example-data", "title": "Download example data\u00b6", "text": ""}, {"location": "tutorials/00-DataDownload_Optional/#directory-organization", "title": "Directory organization\u00b6", "text": "<p>After downloading, the directory will be organized as follows:</p>"}, {"location": "tutorials/01-Configure/", "title": "Configure", "text": "<ul> <li><p>To run <code>workflow-deeplabcut</code>, we need to set up the DataJoint config file, called <code>dj_local_conf.json</code>, unique to each machine.</p> </li> <li><p>The config only needs to be set up once. If you already have one, skip to 02-Workflow-Structure.</p> </li> <li><p>By convention, we set a local config in the workflow directory. You may be interested in setting a global config.</p> </li> </ul> In\u00a0[1]: Copied! <pre>import os\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\nassert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"\n                                                              + \"workflow directory\")\n</pre> import os # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd())=='notebooks': os.chdir('..') assert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"                                                               + \"workflow directory\") <p>Now we can set up credentials following instructions here.</p> In\u00a0[2]: Copied! <pre>import datajoint as dj\nimport getpass\ndj.config['database.host'] = '{YOUR_HOST}'\ndj.config['database.user'] = '{YOUR_USERNAME}'\ndj.config['database.password'] = getpass.getpass() # enter the password securely\n</pre> import datajoint as dj import getpass dj.config['database.host'] = '{YOUR_HOST}' dj.config['database.user'] = '{YOUR_USERNAME}' dj.config['database.password'] = getpass.getpass() # enter the password securely <pre> \u00b7\u00b7\u00b7\u00b7\n</pre> <p>You should be able to connect to the database at this stage.</p> In\u00a0[\u00a0]: Copied! <pre>dj.conn()\n</pre> dj.conn() <p>A schema prefix can help manage privelages on a server. Teams who work on the same schemas should use the same prefix</p> <p>Setting the prefix to <code>neuro_</code> means that every schema we then create will start with <code>neuro_</code> (e.g. <code>neuro_lab</code>, <code>neuro_subject</code>, <code>neuro_model</code> etc.)</p> In\u00a0[\u00a0]: Copied! <pre>dj.config['custom'] = {'database.prefix': 'neuro_'}\n</pre> dj.config['custom'] = {'database.prefix': 'neuro_'} <p><code>dlc_root_data_dir</code> sets the root path(s) for the Element. Given multiple, the Element will always figure out which root to use based on the files it expects there. This should be the directory above your DeepLabCut project path.</p> In\u00a0[\u00a0]: Copied! <pre>dj.config['custom'] = {'dlc_root_data_dir' : ['/tmp/test_data/', '/tmp/example/']}\n\n# Check the connection with `find_full_path`\nfrom element_interface.utils import find_full_path\ndata_dir = find_full_path(dj.config['custom']['dlc_root_data_dir'],\n                          'from_top_tracking')\nassert data_dir.exists(), \"Please check the that you have the from_top_tracking folder\"\n</pre> dj.config['custom'] = {'dlc_root_data_dir' : ['/tmp/test_data/', '/tmp/example/']}  # Check the connection with `find_full_path` from element_interface.utils import find_full_path data_dir = find_full_path(dj.config['custom']['dlc_root_data_dir'],                           'from_top_tracking') assert data_dir.exists(), \"Please check the that you have the from_top_tracking folder\" In\u00a0[\u00a0]: Copied! <pre>dj.config.save_local()\n# dj.config.save_global()\n</pre> dj.config.save_local() # dj.config.save_global() <p>In the next notebook notebook, we'll explore the workflow structure.</p>"}, {"location": "tutorials/01-Configure/#datajoint-u24-workflow-deeplabcut", "title": "DataJoint U24 - Workflow DeepLabCut\u00b6", "text": ""}, {"location": "tutorials/01-Configure/#configure-datajoint", "title": "Configure DataJoint\u00b6", "text": ""}, {"location": "tutorials/01-Configure/#configure-database-host-address-and-credentials", "title": "Configure database host address and credentials\u00b6", "text": ""}, {"location": "tutorials/01-Configure/#configure-the-custom-field", "title": "Configure the <code>custom</code> field\u00b6", "text": ""}, {"location": "tutorials/01-Configure/#prefix", "title": "Prefix\u00b6", "text": ""}, {"location": "tutorials/01-Configure/#root-directory", "title": "Root directory\u00b6", "text": ""}, {"location": "tutorials/01-Configure/#save-the-config-as-a-json", "title": "Save the config as a json\u00b6", "text": "<p>Once set, the config can either be saved locally or globally.</p> <ul> <li>The local config would be saved as <code>dj_local_conf.json</code> in the workflow directory. This is usefull for managing multiple (demo) pipelines.</li> <li>A global config would be saved as <code>datajoint_config.json</code> in the home directory.</li> </ul> <p>When imported, DataJoint will first check for a local config. If none, it will check for a global config.</p>"}, {"location": "tutorials/02-WorkflowStructure_Optional/", "title": "Workflow Structure", "text": "<p>This notebook introduces some useful DataJoint for exploring pipelines featuring the DeepLabCut Element.</p> <ul> <li>DataJoint needs to be configured before running this notebook (see 01-Configure).</li> <li>Those familar with the structure of DataJoint workflows can skip to 03-Process.</li> <li>The playground tutorial on CodeBook provides a more  thorough introduction.</li> </ul> <p>To load the local config, we move to the package root.</p> In\u00a0[1]: Copied! <pre>import os\nif os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\nassert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"\n                                                              + \"workflow directory\")\n</pre> import os if os.path.basename(os.getcwd())=='notebooks': os.chdir('..') assert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"                                                               + \"workflow directory\") <p>Schemas are conceptually related sets of tables. By importing schemas from <code>workflow_deeplabcut.pipeline</code>, we'll declare the tables on the server with the prefix in the config (if we have permission to do so). If these tables are already declared, we'll gain access.</p> <ul> <li><code>dj.list_schemas()</code> lists all schemas a user has access to in the current database</li> <li><code>&lt;schema&gt;.schema.list_tables()</code> will provide names for each table in the format used under the hood.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import datajoint as dj\nfrom workflow_deeplabcut.pipeline import lab, subject, session, train, model\n\ndj.list_schemas()\n</pre> import datajoint as dj from workflow_deeplabcut.pipeline import lab, subject, session, train, model  dj.list_schemas() In\u00a0[4]: Copied! <pre>train.schema.list_tables()\n</pre> train.schema.list_tables() Out[4]: <pre>['#training_param_set',\n 'video_set',\n 'video_set__file',\n 'training_task',\n '__model_training']</pre> <p><code>dj.Diagram()</code> plots tables and dependencies in a schema. To see additional upstream or downstream connections, add <code>- N</code> or <code>+ N</code>.</p> <ul> <li><code>train</code>: Optional schema to manage model training within DataJoint</li> <li><code>model</code>: Schema to manage pose estimation</li> </ul> In\u00a0[3]: Copied! <pre>dj.Diagram(train)\n</pre> dj.Diagram(train) Out[3]: In\u00a0[6]: Copied! <pre>dj.Diagram(model) - 1\n</pre> dj.Diagram(model) - 1 Out[6]: <ul> <li><code>&lt;table&gt;()</code> show table contents</li> <li><code>heading</code> shows attribute definitions</li> <li><code>describe()</code> show table defintiion with foreign key references</li> </ul> In\u00a0[\u00a0]: Copied! <pre>model.VideoRecording.File()\n</pre> model.VideoRecording.File() <p>subject</p> <p>session_datetime</p> <p>recording_id</p> <p>file_id</p> <p>file_path</p> filepath of video, relative to root data directory <p>Total: 0</p> In\u00a0[\u00a0]: Copied! <pre>model.Model.heading\n</pre> model.Model.heading <pre># \nmodel_name           : varchar(64)                  # user-friendly model name\n---\ntask                 : varchar(32)                  # task in the config yaml\ndate                 : varchar(16)                  # date in the config yaml\niteration            : int                          # iteration/version of this model\nsnapshotindex        : int                          # which snapshot for prediction (if -1, latest)\nshuffle              : int                          # which shuffle of the training dataset\ntrainingsetindex     : int                          # which training set fraction to generate model\nscorer               : varchar(64)                  # scorer/network name - DLC's GetScorerName()\nconfig_template      : longblob                     # dictionary of the config for analyze_videos()\nproject_path         : varchar(255)                 # DLC's project_path in config relative to root\nmodel_prefix=\"\"      : varchar(32)                  # \nmodel_description=\"\" : varchar(1000)                # \nparamset_idx=null    : smallint                     # </pre> In\u00a0[\u00a0]: Copied! <pre>train.TrainingTask.describe()\n</pre> train.TrainingTask.describe() <pre># Specification for a DLC model training instance\n-&gt; train.VideoSet\n-&gt; train.TrainingParamSet\ntraining_id          : int                          \n---\nmodel_prefix=\"\"      : varchar(32)                  \nproject_path=\"\"      : varchar(255)                 # DLC's project_path in config relative to root\n\n</pre> <pre>'# Specification for a DLC model training instance\\n-&gt; train.VideoSet\\n-&gt; train.TrainingParamSet\\ntraining_id          : int                          \\n---\\nmodel_prefix=\"\"      : varchar(32)                  \\nproject_path=\"\"      : varchar(255)                 # DLC\\'s project_path in config relative to root\\n'</pre> In\u00a0[\u00a0]: Copied! <pre>dj.Diagram(lab) + dj.Diagram(subject) + dj.Diagram(session)\n</pre> dj.Diagram(lab) + dj.Diagram(subject) + dj.Diagram(session) In\u00a0[11]: Copied! <pre>session.Session.describe()\n</pre> session.Session.describe() <pre>-&gt; subject.Subject\nsession_datetime     : datetime                     \n\n</pre> Out[11]: <pre>'-&gt; subject.Subject\\nsession_datetime     : datetime                     \\n'</pre>"}, {"location": "tutorials/02-WorkflowStructure_Optional/#datajoint-u24-workflow-deeplabcut", "title": "DataJoint U24 - Workflow DeepLabCut\u00b6", "text": ""}, {"location": "tutorials/02-WorkflowStructure_Optional/#introduction", "title": "Introduction\u00b6", "text": ""}, {"location": "tutorials/02-WorkflowStructure_Optional/#schemas-diagrams-and-tables", "title": "Schemas, Diagrams and Tables\u00b6", "text": ""}, {"location": "tutorials/02-WorkflowStructure_Optional/#table-types", "title": "Table Types\u00b6", "text": "<ul> <li>Manual table: green box, manually inserted table, expect new entries daily, e.g. Subject, ProbeInsertion.</li> <li>Lookup table: gray box, pre inserted table, commonly used for general facts or parameters. e.g. Strain, ClusteringMethod, ClusteringParamSet.</li> <li>Imported table: blue oval, auto-processing table, the processing depends on the importing of external files. e.g. process of Clustering requires output files from kilosort2.</li> <li>Computed table: red circle, auto-processing table, the processing does not depend on files external to the database, commonly used for</li> <li>Part table: plain text, as an appendix to the master table, all the part entries of a given master entry represent a intact set of the master entry. e.g. Unit of a CuratedClustering.</li> </ul>"}, {"location": "tutorials/02-WorkflowStructure_Optional/#table-links", "title": "Table Links\u00b6", "text": "<ul> <li>One-to-one primary: thick solid line, share the exact same primary key, meaning the child table inherits all the primary key fields from the parent table as its own primary key.</li> <li>One-to-many primary: thin solid line, inherit the primary key from the parent table, but have additional field(s) as part of the primary key as well</li> <li>Secondary dependency: dashed line, the child table inherits the primary key fields from parent table as its own secondary attribute.</li> </ul>"}, {"location": "tutorials/02-WorkflowStructure_Optional/#common-table-functions", "title": "Common Table Functions\u00b6", "text": ""}, {"location": "tutorials/02-WorkflowStructure_Optional/#other-elements-installed-with-the-workflow", "title": "Other Elements installed with the workflow\u00b6", "text": "<ul> <li><code>lab</code>: lab management related information, such as Lab, User, Project, Protocol, Source.</li> <li><code>subject</code>: general animal information, User, Genetic background, Death etc.</li> <li><code>session</code>: General information of experimental sessions.</li> </ul> <p>For more information about these Elements, see workflow session.</p>"}, {"location": "tutorials/02-WorkflowStructure_Optional/#summary-and-next-step", "title": "Summary and next step\u00b6", "text": "<ul> <li><p>This notebook introduced the overall structures of the schemas and tables in the workflow and relevant tools to explore the schema structure and table definitions.</p> </li> <li><p>The next notebook will introduce the detailed steps to run through <code>workflow-deeplabcut</code>.</p> </li> </ul>"}, {"location": "tutorials/03-Process/", "title": "Process", "text": "<p>The workflow requires a DeepLabCut project with labeled data.</p> <ul> <li>If you don't have data, refer to 00-DataDownload and 01-Configure.</li> <li>For an overview of the schema, refer to 02-WorkflowStructure.</li> <li>For a more automated approach, refer to 03-Automate.</li> </ul> <p>Let's change the directory to load the local config, <code>dj_local_conf.json</code>.</p> In\u00a0[1]: Copied! <pre>import os\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\nassert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"\n                                                              + \"workflow directory\")\n</pre> import os # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd())=='notebooks': os.chdir('..') assert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"                                                               + \"workflow directory\") <p><code>Pipeline.py</code> activates the DataJoint <code>elements</code> and declares other required tables.</p> In\u00a0[2]: Copied! <pre>import datajoint as dj\nfrom workflow_deeplabcut.pipeline import lab, subject, session, train, model\n\n# Directing our pipeline to the appropriate config location\nfrom element_interface.utils import find_full_path\nfrom workflow_deeplabcut.paths import get_dlc_root_data_dir\nconfig_path = find_full_path(get_dlc_root_data_dir(), \n                             'from_top_tracking/config.yaml')\n</pre> import datajoint as dj from workflow_deeplabcut.pipeline import lab, subject, session, train, model  # Directing our pipeline to the appropriate config location from element_interface.utils import find_full_path from workflow_deeplabcut.paths import get_dlc_root_data_dir config_path = find_full_path(get_dlc_root_data_dir(),                               'from_top_tracking/config.yaml') <pre>Connecting cbroz@dss-db.datajoint.io:3306\n</pre> <p>We can insert entries into <code>dj.Manual</code> tables (green in diagrams) by providing values as a dictionary or a list of dictionaries.</p> In\u00a0[\u00a0]: Copied! <pre>session.Session.heading\n</pre> session.Session.heading <pre># \nsubject              : varchar(32)                  # \nsession_datetime     : datetime(3)                  # </pre> In\u00a0[4]: Copied! <pre>subject.Subject.insert1(dict(subject='subject6', \n                             sex='F', \n                             subject_birth_date='2020-01-01', \n                             subject_description='hneih_E105'))\nsession_keys = [dict(subject='subject6', session_datetime='2021-06-02 14:04:22'),\n                dict(subject='subject6', session_datetime='2021-06-03 14:43:10')]\nsession.Session.insert(session_keys)\n</pre> subject.Subject.insert1(dict(subject='subject6',                               sex='F',                               subject_birth_date='2020-01-01',                               subject_description='hneih_E105')) session_keys = [dict(subject='subject6', session_datetime='2021-06-02 14:04:22'),                 dict(subject='subject6', session_datetime='2021-06-03 14:43:10')] session.Session.insert(session_keys) <p>We can look at the contents of this table and restrict by a value.</p> In\u00a0[5]: Copied! <pre>session.Session() &amp; \"session_datetime &gt; '2021-06-01 12:00:00'\" &amp; \"subject='subject6'\"\n</pre> session.Session() &amp; \"session_datetime &gt; '2021-06-01 12:00:00'\" &amp; \"subject='subject6'\" Out[5]: <p>subject</p> <p>session_datetime</p> subject6 2021-06-02 14:04:22subject6 2021-06-03 14:43:10 <p>Total: 2</p> <p>The <code>VideoSet</code> table in the <code>train</code> schema retains records of files generated in the video labeling process (e.g., <code>h5</code>, <code>csv</code>, <code>png</code>). DeepLabCut will refer to the <code>mat</code> file located under the <code>training-datasets</code> directory.</p> <p>We recommend storing all paths as relative to the root in your config.</p> In\u00a0[17]: Copied! <pre>train.VideoSet.insert1({'video_set_id': 0})\nproject_folder = 'from_top_tracking/'\ntraining_files = ['labeled-data/train1/CollectedData_DJ.h5',\n                  'labeled-data/train1/CollectedData_DJ.csv',\n                  'labeled-data/train1/img00674.png',\n                  'videos/train1.mp4']\nfor idx, filename in enumerate(training_files):\n    train.VideoSet.File.insert1({'video_set_id': 0,\n                                 'file_id': idx,\n                                 'file_path': (project_folder + filename)})\n</pre> train.VideoSet.insert1({'video_set_id': 0}) project_folder = 'from_top_tracking/' training_files = ['labeled-data/train1/CollectedData_DJ.h5',                   'labeled-data/train1/CollectedData_DJ.csv',                   'labeled-data/train1/img00674.png',                   'videos/train1.mp4'] for idx, filename in enumerate(training_files):     train.VideoSet.File.insert1({'video_set_id': 0,                                  'file_id': idx,                                  'file_path': (project_folder + filename)}) In\u00a0[18]: Copied! <pre>train.VideoSet.File()\n</pre> train.VideoSet.File() Out[18]: Paths of training files (e.g., labeled pngs, CSV or video) <p>video_set_id</p> <p>file_id</p> <p>file_path</p> 0 0 from_top_tracking/labeled-data/train1/CollectedData_DJ.h50 1 from_top_tracking/labeled-data/train1/CollectedData_DJ.csv0 2 from_top_tracking/labeled-data/train1/img00674.png0 3 from_top_tracking/videos/train1.mp4 <p>Total: 4</p> <p>First, we'll add a <code>ModelTrainingParamSet</code>. This is a lookup table that we can reference when training a model.</p> In\u00a0[\u00a0]: Copied! <pre>train.TrainingParamSet.heading\n</pre> train.TrainingParamSet.heading <pre>paramset_idx         : smallint                     # \n---\nparamset_desc        : varchar(128)                 # \nparam_set_hash       : uuid                         # hash identifying this parameterset\nparams               : longblob                     # dictionary of all applicable parameters</pre> <p>The <code>params</code> longblob should be a dictionary that captures all items for DeepLabCut's <code>train_network</code> function. At minimum, this is the contents of the project's config file, as well as <code>suffle</code> and <code>trainingsetindex</code>, which are not included in the config.</p> In\u00a0[19]: Copied! <pre>from deeplabcut import train_network\nhelp(train_network) # for more information on optional parameters\n</pre> from deeplabcut import train_network help(train_network) # for more information on optional parameters <pre>Loading DLC 2.2.1.1...\nDLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\nHelp on function train_network in module deeplabcut.pose_estimation_tensorflow.training:\n\ntrain_network(config, shuffle=1, trainingsetindex=0, max_snapshots_to_keep=5, displayiters=None, saveiters=None, maxiters=None, allow_growth=True, gputouse=None, autotune=False, keepdeconvweights=True, modelprefix='')\n    Trains the network with the labels in the training dataset.\n    \n    Parameters\n    ----------\n    config : string\n        Full path of the config.yaml file as a string.\n    \n    shuffle: int, optional, default=1\n        Integer value specifying the shuffle index to select for training.\n    \n    trainingsetindex: int, optional, default=0\n        Integer specifying which TrainingsetFraction to use.\n        Note that TrainingFraction is a list in config.yaml.\n    \n    max_snapshots_to_keep: int or None\n        Sets how many snapshots are kept, i.e. states of the trained network. Every\n        saving interation many times a snapshot is stored, however only the last\n        ``max_snapshots_to_keep`` many are kept! If you change this to None, then all\n        are kept.\n        See: https://github.com/DeepLabCut/DeepLabCut/issues/8#issuecomment-387404835\n    \n    displayiters: optional, default=None\n        This variable is actually set in ``pose_config.yaml``. However, you can\n        overwrite it with this hack. Don't use this regularly, just if you are too lazy\n        to dig out the ``pose_config.yaml`` file for the corresponding project. If\n        ``None``, the value from there is used, otherwise it is overwritten!\n    \n    saveiters: optional, default=None\n        This variable is actually set in ``pose_config.yaml``. However, you can\n        overwrite it with this hack. Don't use this regularly, just if you are too lazy\n        to dig out the ``pose_config.yaml`` file for the corresponding project.\n        If ``None``, the value from there is used, otherwise it is overwritten!\n    \n    maxiters: optional, default=None\n        This variable is actually set in ``pose_config.yaml``. However, you can\n        overwrite it with this hack. Don't use this regularly, just if you are too lazy\n        to dig out the ``pose_config.yaml`` file for the corresponding project.\n        If ``None``, the value from there is used, otherwise it is overwritten!\n    \n    allow_growth: bool, optional, default=True.\n        For some smaller GPUs the memory issues happen. If ``True``, the memory\n        allocator does not pre-allocate the entire specified GPU memory region, instead\n        starting small and growing as needed.\n        See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2\n    \n    gputouse: optional, default=None\n        Natural number indicating the number of your GPU (see number in nvidia-smi).\n        If you do not have a GPU put None.\n        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries\n    \n    autotune: bool, optional, default=False\n        Property of TensorFlow, somehow faster if ``False``\n        (as Eldar found out, see https://github.com/tensorflow/tensorflow/issues/13317).\n    \n    keepdeconvweights: bool, optional, default=True\n        Also restores the weights of the deconvolution layers (and the backbone) when\n        training from a snapshot. Note that if you change the number of bodyparts, you\n        need to set this to false for re-training.\n    \n    modelprefix: str, optional, default=\"\"\n        Directory containing the deeplabcut models to use when evaluating the network.\n        By default, the models are assumed to exist in the project folder.\n    \n    Returns\n    -------\n    None\n    \n    Examples\n    --------\n    To train the network for first shuffle of the training dataset\n    \n    &gt;&gt;&gt; deeplabcut.train_network('/analysis/project/reaching-task/config.yaml')\n    \n    To train the network for second shuffle of the training dataset\n    \n    &gt;&gt;&gt; deeplabcut.train_network(\n            '/analysis/project/reaching-task/config.yaml',\n            shuffle=2,\n            keepdeconvweights=True,\n        )\n\n</pre> <p>Here, we give these items, load the config contents, and overwrite some defaults, including <code>maxiters</code>, to restrict our training iterations to 5.</p> In\u00a0[20]: Copied! <pre>import yaml\n\nparamset_idx = 0; paramset_desc='from_top_tracking'\n\nwith open(config_path, 'rb') as y:\n    config_params = yaml.safe_load(y)\ntraining_params = {'shuffle': '1',\n                   'trainingsetindex': '0',\n                   'maxiters': '5',\n                   'scorer_legacy': 'False',\n                   'maxiters': '5', \n                   'multianimalproject':'False'}\nconfig_params.update(training_params)\ntrain.TrainingParamSet.insert_new_params(paramset_idx=paramset_idx,\n                                         paramset_desc=paramset_desc,\n                                         params=config_params)\n</pre> import yaml  paramset_idx = 0; paramset_desc='from_top_tracking'  with open(config_path, 'rb') as y:     config_params = yaml.safe_load(y) training_params = {'shuffle': '1',                    'trainingsetindex': '0',                    'maxiters': '5',                    'scorer_legacy': 'False',                    'maxiters': '5',                     'multianimalproject':'False'} config_params.update(training_params) train.TrainingParamSet.insert_new_params(paramset_idx=paramset_idx,                                          paramset_desc=paramset_desc,                                          params=config_params) <p>Now, we add a <code>TrainingTask</code>. As a computed table, <code>ModelTraining</code> will reference this to start training when calling <code>populate()</code></p> In\u00a0[\u00a0]: Copied! <pre>train.TrainingTask.heading\n</pre> train.TrainingTask.heading <pre>video_set_id         : int                          # \nparamset_idx         : smallint                     # \ntraining_id          : int                          # \n---\nmodel_prefix=\"\"      : varchar(32)                  # \nproject_path=\"\"      : varchar(255)                 # DLC's project_path in config relative to root</pre> In\u00a0[22]: Copied! <pre>key={'video_set_id': 0,\n     'paramset_idx':0,\n     'training_id': 1,\n     'project_path':'from_top_tracking/'\n     }\ntrain.TrainingTask.insert1(key, skip_duplicates=True)\ntrain.TrainingTask()\n</pre> key={'video_set_id': 0,      'paramset_idx':0,      'training_id': 1,      'project_path':'from_top_tracking/'      } train.TrainingTask.insert1(key, skip_duplicates=True) train.TrainingTask() Out[22]: Specification for a DLC model training instance <p>video_set_id</p> <p>paramset_idx</p> <p>training_id</p> <p>model_prefix</p> <p>project_path</p> DLC's project_path in config relative to root 0 0 1 from_top_tracking/ <p>Total: 1</p> In\u00a0[\u00a0]: Copied! <pre>train.ModelTraining.populate()\n</pre> train.ModelTraining.populate() <p>(Output cleared for brevity)</p> <pre><code>The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n</code></pre> In\u00a0[24]: Copied! <pre>train.ModelTraining()\n</pre> train.ModelTraining() Out[24]: <p>video_set_id</p> <p>paramset_idx</p> <p>training_id</p> <p>latest_snapshot</p> latest exact snapshot index (i.e., never -1) <p>config_template</p> stored full config file 0 0 1 5 =BLOB= <p>Total: 1</p> <p>To resume training from a checkpoint, we would need to edit the relevant config file (see also <code>update_pose_cfg</code> in <code>workflow_deeplabcut.load_demo_data</code>). Emperical work suggests 200k iterations for any true use-case.</p> <p>For better quality predictions in this demo, we'll revert the checkpoint file and use a pretrained model.</p> In\u00a0[25]: Copied! <pre>from workflow_deeplabcut.load_demo_data import revert_checkpoint_file\nrevert_checkpoint_file()\n</pre> from workflow_deeplabcut.load_demo_data import revert_checkpoint_file revert_checkpoint_file() <p>The <code>model</code> schema uses a lookup table for managing Body Parts tracked across models.</p> In\u00a0[\u00a0]: Copied! <pre>model.BodyPart.heading\n</pre> model.BodyPart.heading <pre># \nbody_part            : varchar(32)                  # \n---\nbody_part_description=\"\" : varchar(1000)                # </pre> <p>Helper functions allow us to first, identify all the new body parts from a given config, and, second, insert them with user-friendly descriptions.</p> In\u00a0[26]: Copied! <pre>model.BodyPart.extract_new_body_parts(config_path)\n</pre> model.BodyPart.extract_new_body_parts(config_path) <pre>Existing body parts: ['bodycenter' 'head' 'tailbase']\nNew body parts: []\n</pre> Out[26]: <pre>array([], dtype='&lt;U10')</pre> In\u00a0[31]: Copied! <pre>bp_desc=['Body Center', 'Head', 'Base of Tail']\nmodel.BodyPart.insert_from_config(config_path,bp_desc)\n</pre> bp_desc=['Body Center', 'Head', 'Base of Tail'] model.BodyPart.insert_from_config(config_path,bp_desc) <pre>Existing body parts: []\nNew body parts: ['bodycenter' 'head' 'tailbase']\nNew descriptions: ['Body Center', 'Head', 'Base of Tail']\n</pre> <p>We can insert into <code>Model</code> table for automatic evaluation</p> In\u00a0[33]: Copied! <pre>model.Model.insert_new_model(model_name='FromTop-latest',dlc_config=config_path,\n                             shuffle=1,trainingsetindex=0,\n                             model_description='FromTop - latest snapshot',\n                             paramset_idx=0,\n                             params={\"snapshotindex\":-1})\n</pre> model.Model.insert_new_model(model_name='FromTop-latest',dlc_config=config_path,                              shuffle=1,trainingsetindex=0,                              model_description='FromTop - latest snapshot',                              paramset_idx=0,                              params={\"snapshotindex\":-1}) <pre>--- DLC Model specification to be inserted ---\n\tmodel_name: FromTop-latest\n\tmodel_description: FromTop - latest snapshot\n\tscorer: DLCmobnet100fromtoptrackingFeb23shuffle1\n\ttask: from_top_tracking\n\tdate: Feb23\n\titeration: 0\n\tsnapshotindex: -1\n\tshuffle: 1\n\ttrainingsetindex: 0\n\tproject_path: from_top_tracking\n\tparamset_idx: 0\n\t-- Template/Contents of config.yaml --\n\t\tTask: from_top_tracking\n\t\tscorer: DJ\n\t\tdate: Feb23\n\t\tvideo_sets: {'/tmp/test_data/from_top_tracking/videos/train1.mp4': {'crop': '0, 500, 0, 500'}}\n\t\tbodyparts: ['head', 'bodycenter', 'tailbase']\n\t\tstart: 0\n\t\tstop: 1\n\t\tnumframes2pick: 20\n\t\tpcutoff: 0.6\n\t\tdotsize: 3\n\t\talphavalue: 0.7\n\t\tcolormap: viridis\n\t\tTrainingFraction: [0.95]\n\t\titeration: 0\n\t\tdefault_net_type: resnet_50\n\t\tsnapshotindex: -1\n\t\tbatch_size: 8\n\t\tcropping: False\n\t\tx1: 0\n\t\tx2: 640\n\t\ty1: 277\n\t\ty2: 624\n\t\tcorner2move2: [50, 50]\n\t\tmove2corner: True\n\t\tcroppedtraining: None\n\t\tdefault_augmenter: default\n\t\tidentity: None\n\t\tmaxiters: 5\n\t\tmodelprefix: \n\t\tmultianimalproject: False\n\t\tscorer_legacy: False\n\t\tshuffle: 1\n\t\tskeleton: [['bodypart1', 'bodypart2'], ['objectA', 'bodypart3']]\n\t\tskeleton_color: black\n\t\ttrain_fraction: 0.95\n\t\ttrainingsetindex: 0\n\t\tproject_path: /tmp/test_data/from_top_tracking\n</pre> In\u00a0[34]: Copied! <pre>model.Model()\n</pre> model.Model() Out[34]: <p>model_name</p> User-friendly model name <p>task</p> Task in the config yaml <p>date</p> Date in the config yaml <p>iteration</p> Iteration/version of this model <p>snapshotindex</p> which snapshot for prediction (if -1, latest) <p>shuffle</p> Shuffle (1) or not (0) <p>trainingsetindex</p> Index of training fraction list in config.yaml <p>scorer</p> Scorer/network name - DLC's GetScorerName() <p>config_template</p> Dictionary of the config for analyze_videos() <p>project_path</p> DLC's project_path in config relative to root <p>model_prefix</p> <p>model_description</p> <p>paramset_idx</p> FromTop-latest from_top_tracking Feb23 0 -1 1 0 DLCmobnet100fromtoptrackingFeb23shuffle1 =BLOB= from_top_tracking FromTop - latest snapshot 0 <p>Total: 1</p> <p><code>ModelEvaluation</code> will reference the <code>Model</code> using the <code>populate</code> method and insert the  output from DeepLabCut's <code>evaluate_network</code> function</p> In\u00a0[\u00a0]: Copied! <pre>model.ModelEvaluation.heading\n</pre> model.ModelEvaluation.heading <pre>model_name           : varchar(64)                  # user-friendly model name\n---\ntrain_iterations     : int                          # Training iterations\ntrain_error=null     : float                        # Train error (px)\ntest_error=null      : float                        # Test error (px)\np_cutoff=null        : float                        # p-cutoff used\ntrain_error_p=null   : float                        # Train error with p-cutoff\ntest_error_p=null    : float                        # Test error with p-cutoff</pre> In\u00a0[35]: Copied! <pre>model.ModelEvaluation.populate()\n</pre> model.ModelEvaluation.populate() <pre>Config:\n{'all_joints': [[0], [1], [2]],\n 'all_joints_names': ['head', 'bodycenter', 'tailbase'],\n 'batch_size': 1,\n 'crop_pad': 0,\n 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_from_top_trackingFeb23/from_top_tracking_DJ95shuffle1.mat',\n 'dataset_type': 'imgaug',\n 'deterministic': False,\n 'fg_fraction': 0.25,\n 'global_scale': 0.8,\n 'init_weights': '/Volumes/GoogleDrive/My '\n                 'Drive/ref/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n 'intermediate_supervision': False,\n 'intermediate_supervision_layer': 12,\n 'location_refinement': True,\n 'locref_huber_loss': True,\n 'locref_loss_weight': 1.0,\n 'locref_stdev': 7.2801,\n 'log_dir': 'log',\n 'mean_pixel': [123.68, 116.779, 103.939],\n 'mirror': False,\n 'net_type': 'mobilenet_v2_1.0',\n 'num_joints': 3,\n 'optimizer': 'sgd',\n 'pairwise_huber_loss': True,\n 'pairwise_predict': False,\n 'partaffinityfield_predict': False,\n 'regularize': False,\n 'scoremap_dir': 'test',\n 'shuffle': True,\n 'snapshot_prefix': '/tmp/test_data/from_top_tracking/dlc-models/iteration-0/from_top_trackingFeb23-trainset95shuffle1/test/snapshot',\n 'stride': 8.0,\n 'weigh_negatives': False,\n 'weigh_only_present_joints': False,\n 'weigh_part_predictions': False,\n 'weight_decay': 0.0001}\n/Users/cb/miniconda3/envs/ele/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n  warnings.warn('`layer.apply` is deprecated and '\n</pre> <pre>Running  DLC_mobnet_100_from_top_trackingFeb23shuffle1_103000  with # of training iterations: 103000\nRunning evaluation ...\n</pre> <pre>20it [00:06,  3.29it/s]\n</pre> <pre>Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-103000\nResults for 103000  training iterations: 95 1 train error: 9.28 pixels. Test error: 9.84  pixels.\nWith pcutoff of 0.6  train error: 9.28 pixels. Test error: 9.84 pixels\nThereby, the errors are given by the average distances between the labels by DLC and the scorer.\nThe network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\nPlease check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\nUse the function 'analyze_video' to make predictions on new videos.\nOtherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n</pre> In\u00a0[36]: Copied! <pre>model.ModelEvaluation()\n</pre> model.ModelEvaluation() Out[36]: <p>model_name</p> User-friendly model name <p>train_iterations</p> Training iterations <p>train_error</p> Train error (px) <p>test_error</p> Test error (px) <p>p_cutoff</p> p-cutoff used <p>train_error_p</p> Train error with p-cutoff <p>test_error_p</p> Test error with p-cutoff FromTop-latest 103000 9.28 9.84 0.6 9.28 9.84 <p>Total: 1</p> <p>To use our model, we'll first need to insert a session recoring into <code>VideoRecording</code></p> In\u00a0[39]: Copied! <pre>model.VideoRecording()\n</pre> model.VideoRecording() Out[39]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> <p>device</p> <p>Total: 0</p> In\u00a0[40]: Copied! <pre>key = {'subject': 'subject6',\n       'session_datetime': '2021-06-02 14:04:22',\n       'recording_id': '1', 'device': 'Camera1'}\nmodel.VideoRecording.insert1(key)\n\n_ = key.pop('device') # get rid of secondary key from master table\nkey.update({'file_id': 1, \n            'file_path': 'from_top_tracking/videos/test-2s.mp4'})\nmodel.VideoRecording.File.insert1(key)\n</pre> key = {'subject': 'subject6',        'session_datetime': '2021-06-02 14:04:22',        'recording_id': '1', 'device': 'Camera1'} model.VideoRecording.insert1(key)  _ = key.pop('device') # get rid of secondary key from master table key.update({'file_id': 1,              'file_path': 'from_top_tracking/videos/test-2s.mp4'}) model.VideoRecording.File.insert1(key) In\u00a0[41]: Copied! <pre>model.VideoRecording.File()\n</pre> model.VideoRecording.File() Out[41]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> <p>file_id</p> <p>file_path</p> filepath of video, relative to root data directory subject6 2021-06-02 14:04:22 1 1 from_top_tracking/videos/test-2s.mp4 <p>Total: 1</p> <p><code>RecordingInfo</code> automatically populates with file information</p> In\u00a0[42]: Copied! <pre>model.RecordingInfo.populate()\nmodel.RecordingInfo()\n</pre> model.RecordingInfo.populate() model.RecordingInfo() Out[42]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> <p>px_height</p> height in pixels <p>px_width</p> width in pixels <p>nframes</p> number of frames <p>fps</p> (Hz) frames per second <p>recording_datetime</p> Datetime for the start of the recording <p>recording_duration</p> video duration (s) from nframes / fps subject6 2021-06-02 14:04:22 1 500 500 123 60 None 2.05 <p>Total: 1</p> <p>Next, we specify if the <code>PoseEstimation</code> table should load results from an existing file or trigger the estimation command. Here, we can also specify parameters for DeepLabCut's <code>analyze_videos</code> as a dictionary.</p> In\u00a0[43]: Copied! <pre>key = (model.VideoRecording &amp; {'recording_id': '1'}).fetch1('KEY')\nkey.update({'model_name': 'FromTop-latest', 'task_mode': 'trigger'})\nkey\n</pre> key = (model.VideoRecording &amp; {'recording_id': '1'}).fetch1('KEY') key.update({'model_name': 'FromTop-latest', 'task_mode': 'trigger'}) key Out[43]: <pre>{'subject': 'subject6',\n 'session_datetime': datetime.datetime(2021, 6, 2, 14, 4, 22),\n 'recording_id': 1,\n 'model_name': 'FromTop-latest',\n 'task_mode': 'trigger'}</pre> In\u00a0[44]: Copied! <pre>model.PoseEstimationTask.insert_estimation_task(key,params={'save_as_csv':True})\nmodel.PoseEstimation.populate()\n</pre> model.PoseEstimationTask.insert_estimation_task(key,params={'save_as_csv':True}) model.PoseEstimation.populate() <pre>Config:\n{'all_joints': [[0], [1], [2]],\n 'all_joints_names': ['head', 'bodycenter', 'tailbase'],\n 'batch_size': 1,\n 'crop_pad': 0,\n 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_from_top_trackingFeb23/from_top_tracking_DJ95shuffle1.mat',\n 'dataset_type': 'imgaug',\n 'deterministic': False,\n 'fg_fraction': 0.25,\n 'global_scale': 0.8,\n 'init_weights': '/Volumes/GoogleDrive/My '\n                 'Drive/ref/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n 'intermediate_supervision': False,\n 'intermediate_supervision_layer': 12,\n 'location_refinement': True,\n 'locref_huber_loss': True,\n 'locref_loss_weight': 1.0,\n 'locref_stdev': 7.2801,\n 'log_dir': 'log',\n 'mean_pixel': [123.68, 116.779, 103.939],\n 'mirror': False,\n 'net_type': 'mobilenet_v2_1.0',\n 'num_joints': 3,\n 'optimizer': 'sgd',\n 'pairwise_huber_loss': True,\n 'pairwise_predict': False,\n 'partaffinityfield_predict': False,\n 'regularize': False,\n 'scoremap_dir': 'test',\n 'shuffle': True,\n 'snapshot_prefix': '/tmp/test_data/from_top_tracking/dlc-models/iteration-0/from_top_trackingFeb23-trainset95shuffle1/test/snapshot',\n 'stride': 8.0,\n 'weigh_negatives': False,\n 'weigh_only_present_joints': False,\n 'weigh_part_predictions': False,\n 'weight_decay': 0.0001}\n/Users/cb/miniconda3/envs/ele/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n  warnings.warn('`layer.apply` is deprecated and '\n</pre> <pre>Using snapshot-103000 for model /tmp/test_data/from_top_tracking/dlc-models/iteration-0/from_top_trackingFeb23-trainset95shuffle1\nStarting to analyze %  /tmp/test_data/from_top_tracking/videos/test-2s.mp4\nLoading  /tmp/test_data/from_top_tracking/videos/test-2s.mp4\nDuration of video [s]:  2.05 , recorded with  60.0 fps!\nOverall # of frames:  123  found with (before cropping) frame dimensions:  500 500\nStarting to extract posture\n</pre> <pre> 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 120/123 [00:37&lt;00:00,  3.22it/s]\n</pre> <pre>Saving results in /tmp/test_data/from_top_tracking/videos/device_Camera1_recording_1_model_FromTop-latest...\nSaving csv poses!\nThe videos are analyzed. Now your research can truly start! \n You can create labeled videos with 'create_labeled_video'\nIf the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n</pre> <p>By default, DataJoint will store results in a subdirectory</p> <pre><code>  &lt;processed_dir&gt; / videos / device_&lt;name&gt;_recording_&lt;#&gt;_model_&lt;name&gt;</code></pre> <p>where <code>processed_dir</code> is optionally specified in the datajoint config. If unspecified, this will be the project directory. The device and model names are specified elsewhere in the schema.</p> <p>We can get this estimation directly as a pandas dataframe.</p> In\u00a0[45]: Copied! <pre>model.PoseEstimation.get_trajectory(key)\n</pre> model.PoseEstimation.get_trajectory(key) Out[45]: scorer FromTop-latest bodyparts bodycenter head tailbase coords x y z likelihood x y z likelihood x y z likelihood 0 246.782684 298.728088 0.0 0.999998 241.036957 316.332489 0.0 0.999850 256.203064 278.553314 0.0 0.999998 1 246.217529 299.358063 0.0 0.999997 239.048737 319.177002 0.0 0.999905 255.819626 280.200745 0.0 0.999996 2 244.459579 301.309235 0.0 0.999999 240.238800 320.525696 0.0 0.999899 255.705093 280.939056 0.0 0.999995 3 242.014755 302.865204 0.0 0.999999 238.536774 322.324463 0.0 0.999941 254.424484 282.015778 0.0 0.999990 4 240.900177 303.459167 0.0 0.999998 237.967987 324.072327 0.0 0.999941 252.180603 280.899200 0.0 0.999977 ... ... ... ... ... ... ... ... ... ... ... ... ... 118 248.682251 364.709869 0.0 0.999965 270.854980 371.893127 0.0 0.999961 234.899185 356.035583 0.0 0.999996 119 250.326385 366.870361 0.0 0.999972 271.488495 373.099884 0.0 0.999991 235.644073 356.815125 0.0 0.999989 120 251.634140 367.709198 0.0 0.999972 272.043884 373.402893 0.0 0.999995 236.953812 358.651459 0.0 0.999977 121 255.393692 364.111145 0.0 0.999979 273.417572 373.906799 0.0 0.999997 238.825363 361.561798 0.0 0.999885 122 257.736847 365.264008 0.0 0.999996 276.008667 373.901245 0.0 0.999992 239.148163 364.029297 0.0 0.999962 <p>123 rows \u00d7 12 columns</p> <p>In the next notebook, we'll look at additional tools in the workflow for automating these steps.</p>"}, {"location": "tutorials/03-Process/#datajoint-u24-workflow-deeplabcut", "title": "DataJoint U24 - Workflow DeepLabCut\u00b6", "text": ""}, {"location": "tutorials/03-Process/#interactively-run-the-workflow", "title": "Interactively run the workflow\u00b6", "text": ""}, {"location": "tutorials/03-Process/#manually-inserting-entries", "title": "Manually Inserting Entries\u00b6", "text": ""}, {"location": "tutorials/03-Process/#upstream-tables", "title": "Upstream tables\u00b6", "text": ""}, {"location": "tutorials/03-Process/#deeplabcut-tables", "title": "DeepLabcut Tables\u00b6", "text": ""}, {"location": "tutorials/03-Process/#training-a-network", "title": "Training a Network\u00b6", "text": ""}, {"location": "tutorials/03-Process/#tracking-jointsbody-parts", "title": "Tracking Joints/Body Parts\u00b6", "text": ""}, {"location": "tutorials/03-Process/#declaringevaluating-a-model", "title": "Declaring/Evaluating a Model\u00b6", "text": ""}, {"location": "tutorials/03-Process/#pose-estimation", "title": "Pose Estimation\u00b6", "text": ""}, {"location": "tutorials/04-Automate_Optional/", "title": "Automate", "text": "In\u00a0[1]: Copied! <pre>import os; from pathlib import Path\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\nassert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"\n                                                              + \"workflow directory\")\nfrom workflow_deeplabcut.pipeline import lab, subject, session, train, model\nfrom workflow_deeplabcut import process\n</pre> import os; from pathlib import Path # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd())=='notebooks': os.chdir('..') assert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"                                                               + \"workflow directory\") from workflow_deeplabcut.pipeline import lab, subject, session, train, model from workflow_deeplabcut import process <pre>Connecting cbroz@dss-db.datajoint.io:3306\n</pre> <p>We'll be using the <code>process.py</code> script to automatically loop through all <code>make</code> functions, as a shortcut for calling each individually.</p> <p>If you previously completed the 03-Process notebook, you may want to delete the contents ingested there, to avoid duplication errors.</p> In\u00a0[3]: Copied! <pre>safemode=True # Set to false to turn off confirmation prompts\n(session.Session &amp; 'subject=\"subject6\"').delete(safemode=safemode)\ntrain.TrainingParamSet.delete(safemode=safemode)\ntrain.VideoSet.delete(safemode=safemode)\n</pre> safemode=True # Set to false to turn off confirmation prompts (session.Session &amp; 'subject=\"subject6\"').delete(safemode=safemode) train.TrainingParamSet.delete(safemode=safemode) train.VideoSet.delete(safemode=safemode) <pre>Deleting 4 rows from `u24_dlc_session`.`session_directory`\nDeleting 4 rows from `u24_dlc_session`.`session_note`\nDeleting 4 rows from `u24_dlc_session`.`session`\nDeleting 3 rows from `u24_dlc_train`.`#training_param_set`\nDeleting 0 rows from `u24_dlc_train`.`video_set`\n</pre> Out[3]: <pre>0</pre> In\u00a0[2]: Copied! <pre>from workflow_deeplabcut.ingest import ingest_subjects, ingest_sessions, ingest_dlc_items\ningest_subjects()\ningest_sessions()\ningest_dlc_items()\n</pre> from workflow_deeplabcut.ingest import ingest_subjects, ingest_sessions, ingest_dlc_items ingest_subjects() ingest_sessions() ingest_dlc_items() <pre>\n---- Inserting 0 entry(s) into subject ----\n\n---- Inserting 4 entry(s) into session ----\n\n---- Inserting 4 entry(s) into session_directory ----\n\n---- Inserting 4 entry(s) into session_note ----\n\n---- Inserting 3 entry(s) into #model_training_param_set ----\n\n---- Inserting 3 entry(s) into video_set ----\n\n---- Inserting 10 entry(s) into video_set__file ----\n\n---- Inserting 1 entry(s) into video_recording ----\n\n---- Inserting 1 entry(s) into video_recording__file ----\n</pre> In\u00a0[3]: Copied! <pre>import datajoint as dj; dj.config.load('dj_local_conf.json')\nfrom element_interface.utils import find_full_path\ndata_dir = find_full_path(dj.config['custom']['dlc_root_data_dir'], # root from config\n                          'from_top_tracking')                      # DLC project dir\nconfig_path = (data_dir / 'config.yaml')\n</pre> import datajoint as dj; dj.config.load('dj_local_conf.json') from element_interface.utils import find_full_path data_dir = find_full_path(dj.config['custom']['dlc_root_data_dir'], # root from config                           'from_top_tracking')                      # DLC project dir config_path = (data_dir / 'config.yaml') <ol> <li>Next, we pair training files with training parameters, and launch training via <code>process</code>.<ul> <li>Some tables may try to populate without upstream keys.</li> <li>Others, like <code>RecordingInfo</code> already have keys loaded.</li> <li>Note: DLC's model processes (e.g., Training, Evaluation) log a lot of information to the console, to quiet this, pass <code>verbose=False</code> to <code>process</code></li> </ul> </li> </ol> In\u00a0[\u00a0]: Copied! <pre>key={'paramset_idx':0,'training_id':0,'video_set_id':0, \n     'project_path':'from_top_tracking/'}\ntrain.TrainingTask.insert1(key, skip_duplicates=True)\nprocess.run(verbose=True, display_progress=True)\nmodel.RecordingInfo()\n</pre> key={'paramset_idx':0,'training_id':0,'video_set_id':0,       'project_path':'from_top_tracking/'} train.TrainingTask.insert1(key, skip_duplicates=True) process.run(verbose=True, display_progress=True) model.RecordingInfo() <p>For the purposes of this demo, we'll want to use an older model, so the folling function will reload the original checkpoint file.</p> In\u00a0[5]: Copied! <pre>from workflow_deeplabcut.load_demo_data import revert_checkpoint_file\nrevert_checkpoint_file()\n</pre> from workflow_deeplabcut.load_demo_data import revert_checkpoint_file revert_checkpoint_file() <ol> <li>Now to add such a model upstream key<ul> <li>Include a user-friendly <code>model_name</code></li> <li>Include the relative path for the project's <code>config.yaml</code></li> <li>Add <code>shuffle</code> and <code>trainingsetindex</code></li> <li><code>insert_new_model</code> will prompt before inserting, but this can be skipped with <code>prompt=False</code></li> </ul> </li> </ol> In\u00a0[7]: Copied! <pre>model.Model.insert_new_model(model_name='FromTop-latest', \n                             dlc_config=config_path,\n                             shuffle=1,\n                             trainingsetindex=0,\n                             paramset_idx=1, \n                             prompt=True, # True is the default behavior\n                             model_description='FromTop - latest snapshot',\n                             params={\"snapshotindex\":-1})\nprocess.run()\n</pre> model.Model.insert_new_model(model_name='FromTop-latest',                               dlc_config=config_path,                              shuffle=1,                              trainingsetindex=0,                              paramset_idx=1,                               prompt=True, # True is the default behavior                              model_description='FromTop - latest snapshot',                              params={\"snapshotindex\":-1}) process.run() <pre>--- DLC Model specification to be inserted ---\n\tmodel_name: FromTop-latest\n\tmodel_description: FromTop - latest snapshot\n\tscorer: DLCmobnet100fromtoptrackingFeb23shuffle1\n\ttask: from_top_tracking\n\tdate: Feb23\n\titeration: 0\n\tsnapshotindex: -1\n\tshuffle: 1\n\ttrainingsetindex: 0\n\tproject_path: from_top_tracking\n\tparamset_idx: 1\n\t-- Template/Contents of config.yaml --\n\t\tTask: from_top_tracking\n\t\tscorer: DJ\n\t\tdate: Feb23\n\t\tvideo_sets: {'/tmp/test_data/from_top_tracking/videos/train1.mp4': {'crop': '0, 500, 0, 500'}}\n\t\tbodyparts: ['head', 'bodycenter', 'tailbase']\n\t\tstart: 0\n\t\tstop: 1\n\t\tnumframes2pick: 20\n\t\tpcutoff: 0.6\n\t\tdotsize: 3\n\t\talphavalue: 0.7\n\t\tcolormap: viridis\n\t\tTrainingFraction: [0.95]\n\t\titeration: 0\n\t\tdefault_net_type: resnet_50\n\t\tsnapshotindex: -1\n\t\tbatch_size: 8\n\t\tcropping: False\n\t\tx1: 0\n\t\tx2: 640\n\t\ty1: 277\n\t\ty2: 624\n\t\tcorner2move2: [50, 50]\n\t\tmove2corner: True\n\t\tcroppedtraining: None\n\t\tdefault_augmenter: default\n\t\tidentity: None\n\t\tmaxiters: 5\n\t\tmodelprefix: \n\t\tmultianimalproject: False\n\t\tscorer_legacy: False\n\t\tshuffle: 1\n\t\tskeleton: [['bodypart1', 'bodypart2'], ['objectA', 'bodypart3']]\n\t\tskeleton_color: black\n\t\ttrain_fraction: 0.95\n\t\ttrainingsetindex: 0\n\t\tproject_path: /tmp/test_data/from_top_tracking\n\n---- Populating __model_training ----\n</pre> <pre>ModelTraining: 0it [00:00, ?it/s]</pre> <pre>\n---- Populating _recording_info ----\n</pre> <pre>\nRecordingInfo: 0it [00:00, ?it/s]\n</pre> <pre>\n---- Populating __model_evaluation ----\n</pre> <pre>ModelEvaluation:   0%|          | 0/1 [00:00&lt;?, ?it/s]Config:\n{'all_joints': [[0], [1], [2]],\n 'all_joints_names': ['head', 'bodycenter', 'tailbase'],\n 'batch_size': 1,\n 'crop_pad': 0,\n 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_from_top_trackingFeb23/from_top_tracking_DJ95shuffle1.mat',\n 'dataset_type': 'imgaug',\n 'deterministic': False,\n 'fg_fraction': 0.25,\n 'global_scale': 0.8,\n 'init_weights': '/Volumes/GoogleDrive/My '\n                 'Drive/ref/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n 'intermediate_supervision': False,\n 'intermediate_supervision_layer': 12,\n 'location_refinement': True,\n 'locref_huber_loss': True,\n 'locref_loss_weight': 1.0,\n 'locref_stdev': 7.2801,\n 'log_dir': 'log',\n 'mean_pixel': [123.68, 116.779, 103.939],\n 'mirror': False,\n 'net_type': 'mobilenet_v2_1.0',\n 'num_joints': 3,\n 'optimizer': 'sgd',\n 'pairwise_huber_loss': True,\n 'pairwise_predict': False,\n 'partaffinityfield_predict': False,\n 'regularize': False,\n 'scoremap_dir': 'test',\n 'shuffle': True,\n 'snapshot_prefix': '/tmp/test_data/from_top_tracking/dlc-models/iteration-0/from_top_trackingFeb23-trainset95shuffle1/test/snapshot',\n 'stride': 8.0,\n 'weigh_negatives': False,\n 'weigh_only_present_joints': False,\n 'weigh_part_predictions': False,\n 'weight_decay': 0.0001}\nModelEvaluation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.95it/s]\n</pre> <pre>Running  DLC_mobnet_100_from_top_trackingFeb23shuffle1_103000  with # of training iterations: 103000\nThis net has already been evaluated!\n\n---- Populating __pose_estimation ----\n</pre> <pre>PoseEstimation: 0it [00:00, ?it/s]\n</pre> <ol> <li>Add a pose estimation task, and launch via <code>process</code>.<ul> <li>Get all primary key information for a given recording</li> <li>Add the model and <code>task_mode</code> (i.e., load vs. trigger) to be applied</li> <li>Add any additional analysis parameters for <code>deeplabcut.analyze_videos</code></li> </ul> </li> </ol> In\u00a0[8]: Copied! <pre>key=(model.VideoRecording &amp; 'recording_id=1').fetch1('KEY')\nkey.update({'model_name': 'FromTop-latest', 'task_mode': 'trigger'})\nanalyze_params={'save_as_csv':True} # add any others from deeplabcut.analyze_videos\nmodel.PoseEstimationTask.insert_estimation_task(key,params=analyze_params)\nprocess.run()\n</pre> key=(model.VideoRecording &amp; 'recording_id=1').fetch1('KEY') key.update({'model_name': 'FromTop-latest', 'task_mode': 'trigger'}) analyze_params={'save_as_csv':True} # add any others from deeplabcut.analyze_videos model.PoseEstimationTask.insert_estimation_task(key,params=analyze_params) process.run() <pre>\n---- Populating __model_training ----\n</pre> <pre>ModelTraining: 0it [00:00, ?it/s]\n</pre> <pre>\n---- Populating _recording_info ----\n</pre> <pre>RecordingInfo: 0it [00:00, ?it/s]\n</pre> <pre>\n---- Populating __model_evaluation ----\n</pre> <pre>ModelEvaluation: 0it [00:00, ?it/s]\n</pre> <pre>\n---- Populating __pose_estimation ----\n</pre> <pre>PoseEstimation:   0%|          | 0/1 [00:00&lt;?, ?it/s]Config:\n{'all_joints': [[0], [1], [2]],\n 'all_joints_names': ['head', 'bodycenter', 'tailbase'],\n 'batch_size': 1,\n 'crop_pad': 0,\n 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_from_top_trackingFeb23/from_top_tracking_DJ95shuffle1.mat',\n 'dataset_type': 'imgaug',\n 'deterministic': False,\n 'fg_fraction': 0.25,\n 'global_scale': 0.8,\n 'init_weights': '/Volumes/GoogleDrive/My '\n                 'Drive/ref/DeepLabCut/deeplabcut/pose_estimation_tensorflow/models/pretrained/mobilenet_v2_1.0_224.ckpt',\n 'intermediate_supervision': False,\n 'intermediate_supervision_layer': 12,\n 'location_refinement': True,\n 'locref_huber_loss': True,\n 'locref_loss_weight': 1.0,\n 'locref_stdev': 7.2801,\n 'log_dir': 'log',\n 'mean_pixel': [123.68, 116.779, 103.939],\n 'mirror': False,\n 'net_type': 'mobilenet_v2_1.0',\n 'num_joints': 3,\n 'optimizer': 'sgd',\n 'pairwise_huber_loss': True,\n 'pairwise_predict': False,\n 'partaffinityfield_predict': False,\n 'regularize': False,\n 'scoremap_dir': 'test',\n 'shuffle': True,\n 'snapshot_prefix': '/tmp/test_data/from_top_tracking/dlc-models/iteration-0/from_top_trackingFeb23-trainset95shuffle1/test/snapshot',\n 'stride': 8.0,\n 'weigh_negatives': False,\n 'weigh_only_present_joints': False,\n 'weigh_part_predictions': False,\n 'weight_decay': 0.0001}\n/Users/cb/miniconda3/envs/ele/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n  warnings.warn('`layer.apply` is deprecated and '\n</pre> <pre>Using snapshot-103000 for model /tmp/test_data/from_top_tracking/dlc-models/iteration-0/from_top_trackingFeb23-trainset95shuffle1\nStarting to analyze %  /tmp/test_data/from_top_tracking/videos/test-2s.mp4\nThe videos are analyzed. Now your research can truly start! \n You can create labeled videos with 'create_labeled_video'\nIf the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n</pre> <pre>PoseEstimation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:05&lt;00:00,  5.98s/it]\n</pre> <ol> <li>Retrieve estimated position data.</li> </ol> In\u00a0[9]: Copied! <pre>model.PoseEstimation.get_trajectory(key)\n</pre> model.PoseEstimation.get_trajectory(key) Out[9]: scorer FromTop-latest bodyparts bodycenter head tailbase coords x y z likelihood x y z likelihood x y z likelihood 0 246.782684 298.728088 0.0 0.999998 241.036957 316.332489 0.0 0.999850 256.203064 278.553314 0.0 0.999998 1 246.217529 299.358063 0.0 0.999997 239.048737 319.177002 0.0 0.999905 255.819626 280.200745 0.0 0.999996 2 244.459579 301.309235 0.0 0.999999 240.238800 320.525696 0.0 0.999899 255.705093 280.939056 0.0 0.999995 3 242.014755 302.865204 0.0 0.999999 238.536774 322.324463 0.0 0.999941 254.424484 282.015778 0.0 0.999990 4 240.900177 303.459167 0.0 0.999998 237.967987 324.072327 0.0 0.999941 252.180603 280.899200 0.0 0.999977 ... ... ... ... ... ... ... ... ... ... ... ... ... 118 248.682251 364.709869 0.0 0.999965 270.854980 371.893127 0.0 0.999961 234.899185 356.035583 0.0 0.999996 119 250.326385 366.870361 0.0 0.999972 271.488495 373.099884 0.0 0.999991 235.644073 356.815125 0.0 0.999989 120 251.634140 367.709198 0.0 0.999972 272.043884 373.402893 0.0 0.999995 236.953812 358.651459 0.0 0.999977 121 255.393692 364.111145 0.0 0.999979 273.417572 373.906799 0.0 0.999997 238.825363 361.561798 0.0 0.999885 122 257.736847 365.264008 0.0 0.999996 276.008667 373.901245 0.0 0.999992 239.148163 364.029297 0.0 0.999962 <p>123 rows \u00d7 12 columns</p>"}, {"location": "tutorials/04-Automate_Optional/#datajoint-u24-workflow-deeplabcut", "title": "DataJoint U24 - Workflow DeepLabCut\u00b6", "text": ""}, {"location": "tutorials/04-Automate_Optional/#workflow-automation", "title": "Workflow Automation\u00b6", "text": "<p>In the previous notebook 03-Process, we ran through the workflow in detailed steps, manually adding each. The current notebook provides a more automated approach.</p> <p>The commands here run a workflow using example data from the 00-DownloadData notebook, but note where placeholders could be changed for a different dataset.</p>"}, {"location": "tutorials/04-Automate_Optional/#ingestion-of-subjects-sessions-videos-and-training-parameters", "title": "Ingestion of subjects, sessions, videos and training parameters\u00b6", "text": "<p>Refer to the <code>user_data</code> folder in the workflow.</p> <ol> <li>Fill subject and session information in files <code>subjects.csv</code> and <code>sessions.csv</code></li> <li>Fill in recording and parameter information in <code>recordings.csv</code> and <code>config_params.csv</code><ul> <li>Add both training and estimation videos to the recording list</li> <li>Additional columns in <code>config_params.csv</code> will be treated as model training parameters</li> </ul> </li> <li>Run automatic scripts prepared in <code>workflow_deeplabcut.ingest</code> for ingestion:<ul> <li><code>ingest_subjects</code> for <code>subject.Subject</code></li> <li><code>ingest_sessions</code> - for session tables <code>Session</code>, <code>SessionDirectory</code>, and <code>SessionNote</code></li> <li><code>ingest_dlc_items</code> - for ...<ul> <li><code>train.ModelTrainingParamSet</code></li> <li><code>train.VideoSet</code> and the corresponding <code>File</code> part table</li> <li><code>model.VideoRecording</code> and the corresponding <code>File</code> part table</li> </ul> </li> </ul> </li> </ol>"}, {"location": "tutorials/04-Automate_Optional/#setting-project-variables", "title": "Setting project variables\u00b6", "text": "<ol> <li>Set your root directory in your DataJoint config file, under <code>custom</code> as <code>dlc_root_data_dir</code>.</li> </ol>"}, {"location": "tutorials/04-Automate_Optional/#summary-and-next-step", "title": "Summary and next step\u00b6", "text": "<ul> <li><p>This notebook runs through the workflow in an automatic manner.</p> </li> <li><p>The next notebook 05-Visualization demonstrates how to plot this data and label videos on disk.</p> </li> </ul>"}, {"location": "tutorials/05-Visualization_Optional/", "title": "Visualization", "text": "<p>The notebook requires DeepLabCut pose estimation already processed via DataJoint.</p> <ul> <li>If you don't have data, refer to 00-DataDownload and 01-Configure.</li> <li>For an overview of the schema, refer to 02-WorkflowStructure.</li> <li>For step-by-step or autmated ingestion, refer to 03-Process or 03-Automate.</li> </ul> <p>Let's change the directory to load the local config, <code>dj_local_conf.json</code> and import the relevant schema.</p> In\u00a0[1]: Copied! <pre>import os # change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\nassert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"\n                                                              + \"workflow directory\")\n\nimport datajoint as dj # Import relevant schema\nfrom workflow_deeplabcut.pipeline import model\n\n# Directing our pipeline to the appropriate config location\nfrom element_interface.utils import find_full_path\nfrom workflow_deeplabcut.paths import get_dlc_root_data_dir\nconfig_path = find_full_path(get_dlc_root_data_dir(), \n                             'from_top_tracking/config.yaml')\n\n# Grabbing the relevant key\nimport pandas as pd\nkey = (model.PoseEstimation &amp; \"recording_id=1\").fetch('KEY')\n</pre> import os # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd())=='notebooks': os.chdir('..') assert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"                                                               + \"workflow directory\")  import datajoint as dj # Import relevant schema from workflow_deeplabcut.pipeline import model  # Directing our pipeline to the appropriate config location from element_interface.utils import find_full_path from workflow_deeplabcut.paths import get_dlc_root_data_dir config_path = find_full_path(get_dlc_root_data_dir(),                               'from_top_tracking/config.yaml')  # Grabbing the relevant key import pandas as pd key = (model.PoseEstimation &amp; \"recording_id=1\").fetch('KEY') <pre>Connecting cbroz@dss-db.datajoint.io:3306\n</pre> <p>In the previous notebook, we saw how to fetch data as a pandas dataframe.</p> In\u00a0[192]: Copied! <pre>df=model.PoseEstimation.get_trajectory(key)\ndf_xy = df.iloc[:,df.columns.get_level_values(2).isin([\"x\",\"y\"])]['FromTop-latest']\ndf_xy.mean()\n</pre> df=model.PoseEstimation.get_trajectory(key) df_xy = df.iloc[:,df.columns.get_level_values(2).isin([\"x\",\"y\"])]['FromTop-latest'] df_xy.mean() Out[192]: <pre>bodyparts   coords\nbodycenter  x         231.709213\n            y         351.878936\nhead        x         234.893540\n            y         367.393746\ntailbase    x         235.567368\n            y         333.615991\ndtype: float64</pre> <p>We plot these coordinates over time.</p> In\u00a0[103]: Copied! <pre>df_xy.plot().legend(loc='right')\n</pre> df_xy.plot().legend(loc='right') Out[103]: <pre>&lt;matplotlib.legend.Legend at 0x7fbe8ab4b6d0&gt;</pre> <p>Next, we'll make a copy of the data for the next plot.</p> In\u00a0[\u00a0]: Copied! <pre>df_flat = df_xy.copy()\ndf_flat.columns = df_flat.columns.map('_'.join)\n</pre> df_flat = df_xy.copy() df_flat.columns = df_flat.columns.map('_'.join) <p>Here, we can overlay the traces of each point over time.</p> In\u00a0[196]: Copied! <pre>import matplotlib.pyplot as plt \nfig,ax=plt.subplots()\ndf_flat.plot(x='bodycenter_x',y='bodycenter_y',ax=ax)\ndf_flat.plot(x='head_x',y='head_y', ax=ax)\ndf_flat.plot(x='tailbase_x',y='tailbase_y', ax=ax)\n</pre> import matplotlib.pyplot as plt  fig,ax=plt.subplots() df_flat.plot(x='bodycenter_x',y='bodycenter_y',ax=ax) df_flat.plot(x='head_x',y='head_y', ax=ax) df_flat.plot(x='tailbase_x',y='tailbase_y', ax=ax) Out[196]: <pre>&lt;AxesSubplot:xlabel='tailbase_x'&gt;</pre> <p>Our visual check shows that these trajectories are more-or-less aligned.</p> <p>This Element adds to the DeepLabCut tree structure by sorting results files into output directories. Let's see where they're stored using <code>infer_output_dir</code>.</p> In\u00a0[199]: Copied! <pre>destfolder = model.PoseEstimationTask.infer_output_dir(key)\ndestfolder\n</pre> destfolder = model.PoseEstimationTask.infer_output_dir(key) destfolder Out[199]: <pre>PosixPath('/tmp/test_data/from_top_tracking/videos/device_Camera1_recording_1_model_FromTop-latest')</pre> <p>When labeling videos, we need to provide this as an additional argument.</p> <p>Note that DataJoint handles paths as <code>pathlib</code> objects, while DeepLabCut requires strings.</p> In\u00a0[200]: Copied! <pre>from deeplabcut.utils.make_labeled_video import create_labeled_video\n\nvideo_path = find_full_path( # Fetch the full video path\n    get_dlc_root_data_dir(), ((model.VideoRecording.File &amp; key).fetch1(\"file_path\"))\n)\n\nconfig_paths = sorted( # Of configs in the project path, defer to the datajoint-saved\n    list(\n        find_full_path(\n            get_dlc_root_data_dir(), ((model.Model &amp; key).fetch1(\"project_path\"))\n        ).glob(\"*.y*ml\")\n    )\n)\n\ncreate_labeled_video( # Pass strings to label the video\n    config=str(config_paths[-1]),\n    videos=str(video_path),\n    destfolder=str(destfolder),\n)\n</pre> from deeplabcut.utils.make_labeled_video import create_labeled_video  video_path = find_full_path( # Fetch the full video path     get_dlc_root_data_dir(), ((model.VideoRecording.File &amp; key).fetch1(\"file_path\")) )  config_paths = sorted( # Of configs in the project path, defer to the datajoint-saved     list(         find_full_path(             get_dlc_root_data_dir(), ((model.Model &amp; key).fetch1(\"project_path\"))         ).glob(\"*.y*ml\")     ) )  create_labeled_video( # Pass strings to label the video     config=str(config_paths[-1]),     videos=str(video_path),     destfolder=str(destfolder), )  <pre>Loading DLC 2.2.1.1...\nDLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\nLoading DLC 2.2.1.1...\nDLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\nStarting to process video: /tmp/test_data/from_top_tracking/videos/test-2s.mp4\nLoading /tmp/test_data/from_top_tracking/videos/test-2s.mp4 and data.\nDuration of video [s]: 2.05, recorded with 60.0 fps!\nOverall # of frames: 123 with cropped frame dimensions: 500 500\nGenerating frames and creating video.\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 123/123 [00:00&lt;00:00, 236.95it/s]\n</pre> <p>The video should now be labeled at this path</p> In\u00a0[206]: Copied! <pre>from IPython.display import FileLink\nFileLink(path=video_path)\n</pre> from IPython.display import FileLink FileLink(path=video_path) Out[206]: /tmp/test_data/from_top_tracking/videos/test-2s.mp4 <p>In the next notebook, 06-Drop, we'll demonstrate dropping schemas in this Element.</p>"}, {"location": "tutorials/05-Visualization_Optional/#datajoint-u24-workflow-deeplabcut", "title": "DataJoint U24 - Workflow DeepLabCut\u00b6", "text": ""}, {"location": "tutorials/05-Visualization_Optional/#setup", "title": "Setup\u00b6", "text": ""}, {"location": "tutorials/05-Visualization_Optional/#fetching-data", "title": "Fetching data\u00b6", "text": ""}, {"location": "tutorials/05-Visualization_Optional/#plotting", "title": "Plotting\u00b6", "text": ""}, {"location": "tutorials/05-Visualization_Optional/#video-labeling", "title": "Video Labeling\u00b6", "text": ""}, {"location": "tutorials/06-Drop_Optional/", "title": "Drop Schemas", "text": "<p>Change into the parent directory to find the <code>dj_local_conf.json</code> file.</p> In\u00a0[1]: Copied! <pre>import os; from pathlib import Path\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\nassert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"\n                                                              + \"workflow directory\")\n</pre> import os; from pathlib import Path # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd())=='notebooks': os.chdir('..') assert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"                                                               + \"workflow directory\") In\u00a0[\u00a0]: Copied! <pre>from workflow_deeplabcut.pipeline import *\n</pre> from workflow_deeplabcut.pipeline import * In\u00a0[\u00a0]: Copied! <pre># model.schema.drop()\n# train.schema.drop()\n# session.schema.drop()\n# subject.schema.drop()\n# lab.schema.drop()\n</pre> # model.schema.drop() # train.schema.drop() # session.schema.drop() # subject.schema.drop() # lab.schema.drop()"}, {"location": "tutorials/06-Drop_Optional/#datajoint-u24-workflow-deeplabcut", "title": "DataJoint U24 - Workflow DeepLabCut\u00b6", "text": ""}, {"location": "tutorials/06-Drop_Optional/#drop-schemas", "title": "Drop schemas\u00b6", "text": "<ul> <li>Schemas are not typically dropped in a production workflow with real data in it.</li> <li>At the developmental phase, it might be required for the table redesign.</li> <li>When dropping all schemas is needed, drop items starting with the most downstream.</li> </ul>"}, {"location": "tutorials/09-AlternateDataset/", "title": "Alternate Dataset", "text": "<p>This notebook provides a general introduction to DataJoint use via Element DeepLabcut. It follows the same structure as other notebooks in this directory, but uses data from the DeepLabCut team.</p> <p>We recommend the other notebooks as they provide access to a pretrained model and allow for a more in-depth exploration of the features of the Element.</p> <p>The directory will be organized as follows within your chosen root directory.</p> <pre><code> /your-root/openfield-Pranav-2018-10-30/\n   - config.yaml\n   - labeled-data\n      - m4s1\n          - CollectedData_Pranav.csv\n          - CollectedData_Pranav.h5\n          - img0000.png\n          - img0001.png\n          - img0002.png\n          - img{...}.png\n          - img0114.png\n          - img0115.png\n   - videos\n       - m3v1mp4.mp4\n</code></pre> <p>For those unfamiliar with DLC...</p> <ul> <li><code>config.yaml</code> contains all the key parameters of the project, including<ul> <li>file locations (currently empty)</li> <li>body parts</li> <li>cropping information</li> </ul> </li> <li><code>labeled-data</code> includes the frames coordinates for each body part in the training video</li> <li><code>videos</code> includes the full training video for this example</li> </ul> <p>Part of the demo setup involves an additional command (as shown here) to revise the project path within config file as well as generate the <code>training-datasets</code> directory.</p> In\u00a0[\u00a0]: Copied! <pre>your_root='/fill/in/your/root/with\\ escaped\\ spaces'\nfrom deeplabcut.create_project.demo_data import load_demo_data\nload_demo_data(your_root+'/openfield-Pranav-2018-10-30/config.yaml')\n</pre> your_root='/fill/in/your/root/with\\ escaped\\ spaces' from deeplabcut.create_project.demo_data import load_demo_data load_demo_data(your_root+'/openfield-Pranav-2018-10-30/config.yaml') <p>Later, we'll use the first few seconds of the training video as a 'separate session' to demonstrate pose estimation within the Element. <code>ffmpeg</code> is a dependency of DeepLabCut that can splice the training video for a demonstration purposes. The command below saves the first 2 seconds of the training video as a copy.</p> <ul> <li><code>-n</code> do not overwrite</li> <li><code>-hide_banner -loglevel error</code> less verbose output</li> <li><code>-ss 0 -t 2</code> start at second 0, add 2 seconds</li> <li><code>-i {vid_path}</code> input this video</li> <li><code>-{v/a}codec copy</code> copy the video and audio codecs of the input</li> <li><code>{vid_path}-copy.mp4</code> output file</li> </ul> In\u00a0[\u00a0]: Copied! <pre>vid_path = your_root + '/openfield-Pranav-2018-10-30/videos/m3v1mp4'\ncmd = (f'ffmpeg -n -hide_banner -loglevel error -ss 0 -t 2 -i {vid_path}.mp4 '\n       + f'-vcodec copy -acodec copy {vid_path}-copy.mp4')\nimport os; os.system(cmd)\n</pre> vid_path = your_root + '/openfield-Pranav-2018-10-30/videos/m3v1mp4' cmd = (f'ffmpeg -n -hide_banner -loglevel error -ss 0 -t 2 -i {vid_path}.mp4 '        + f'-vcodec copy -acodec copy {vid_path}-copy.mp4') import os; os.system(cmd) <pre>0</pre> <ul> <li><p>To run <code>workflow-deeplabcut</code>, we need to set up the DataJoint configuration file, called <code>dj_local_conf.json</code>, unique to each machine.</p> </li> <li><p>The config only needs to be set up once, skip to the next section.</p> </li> <li><p>By convention, we set a local config in the workflow directory. You may be interested in setting a global config.</p> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import os\n# change to the upper level folder to detect dj_local_conf.json\nif os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\nassert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"\n                                                              + \"workflow directory\")\n</pre> import os # change to the upper level folder to detect dj_local_conf.json if os.path.basename(os.getcwd())=='notebooks': os.chdir('..') assert os.path.basename(os.getcwd())=='workflow-deeplabcut', (\"Please move to the \"                                                               + \"workflow directory\") <p>Now let's set up the host, user and password in the <code>dj.config</code> following instructions here.</p> In\u00a0[\u00a0]: Copied! <pre>import datajoint as dj\nimport getpass\ndj.config['database.host'] = '{YOUR_HOST}'\ndj.config['database.user'] = '{YOUR_USERNAME}'\ndj.config['database.password'] = getpass.getpass() # enter the password securely\n</pre> import datajoint as dj import getpass dj.config['database.host'] = '{YOUR_HOST}' dj.config['database.user'] = '{YOUR_USERNAME}' dj.config['database.password'] = getpass.getpass() # enter the password securely <pre> \u00b7\u00b7\u00b7\u00b7\n</pre> <p>You should be able to connect to the database at this stage.</p> In\u00a0[\u00a0]: Copied! <pre>dj.conn()\n</pre> dj.conn() <p>Prefix: Giving a prefix to your schema could help manage privelages on a server.</p> <ul> <li>If we set prefix <code>neuro_</code>, every schema created with the current workflow will start with <code>neuro_</code>, e.g. <code>neuro_lab</code>, <code>neuro_subject</code>, <code>neuro_imaging</code> etc.</li> <li>Teams who work on the same schemas should use the same prefix, set as follows:</li> </ul> In\u00a0[\u00a0]: Copied! <pre>dj.config['custom'] = {'database.prefix': 'neuro_'}\n</pre> dj.config['custom'] = {'database.prefix': 'neuro_'} <p>Root dir: <code>custom</code> keeps track of your root directory for this project. With multiple roots the Element will figure out which to use based on the files it expects.</p> <ul> <li>Please set one root to the parent directory of DLC's <code>openfield-Pranav-2018-10-30</code> example.</li> <li>In other cases, this should be the parent of your DLC project path.</li> </ul> <p>We can then check that the path connects with a tool from element-interface.</p> In\u00a0[\u00a0]: Copied! <pre>dj.config['custom'] = {'dlc_root_data_dir' : ['your-root1', 'your-root2']}\n\nfrom element_interface.utils import find_full_path\ndata_dir = find_full_path(dj.config['custom']['dlc_root_data_dir'],\n                          'openfield-Pranav-2018-10-30')\nassert data_dir.exists(), \"Please check the that you have the folder openfield-Pranav\"\n</pre> dj.config['custom'] = {'dlc_root_data_dir' : ['your-root1', 'your-root2']}  from element_interface.utils import find_full_path data_dir = find_full_path(dj.config['custom']['dlc_root_data_dir'],                           'openfield-Pranav-2018-10-30') assert data_dir.exists(), \"Please check the that you have the folder openfield-Pranav\" <p>With the proper configurations, we could save this as a file, either as a local json file, or a global file. DataJoint will default to a local file, then check for a global if none is found.</p> In\u00a0[\u00a0]: Copied! <pre>dj.config.save_local() # saved as dj_local_conf.json in the root workflow dir\n# dj.config.save_global() # saved as .datajoint_config.json in your home dir\n</pre> dj.config.save_local() # saved as dj_local_conf.json in the root workflow dir # dj.config.save_global() # saved as .datajoint_config.json in your home dir <p>Schemas are conceptually related sets of tables. By importing schemas from <code>workflow_deeplabcut.pipeline</code>, we'll declare the tables on the server with the prefix we set. If these tables are already declared, we'll gain access. For more information about lab, animal and session Elements, see session workflow.</p> <ul> <li><code>dj.list_schemas()</code> lists all schemas a user has access to in the current database</li> <li><code>&lt;schema&gt;.schema.list_tables()</code> will provide names for each table in the format used under the hood.</li> </ul> In\u00a0[1]: Copied! <pre>import datajoint as dj\nfrom workflow_deeplabcut.pipeline import lab, subject, session, train, model\n\ndj.list_schemas()\n\ntrain.schema.list_tables()\n</pre> import datajoint as dj from workflow_deeplabcut.pipeline import lab, subject, session, train, model  dj.list_schemas()  train.schema.list_tables() <pre>Connecting cbroz@dss-db.datajoint.io:3306\n</pre> Out[1]: <pre>['#training_param_set',\n 'video_set',\n 'video_set__file',\n 'training_task',\n '__model_training']</pre> <p><code>dj.Diagram()</code> plots tables and dependencies in a schema. To see additional upstream or downstream connections, add <code>- N</code> or <code>+ N</code>.</p> <ul> <li><code>train</code>: Optional schema to manage model training within DataJoint</li> <li><code>model</code>: Schema to manage pose estimation</li> </ul> In\u00a0[3]: Copied! <pre>dj.Diagram(train) #- 1\n</pre> dj.Diagram(train) #- 1 Out[3]: In\u00a0[3]: Copied! <pre>dj.Diagram(model)\n</pre> dj.Diagram(model) Out[3]: <ul> <li><code>&lt;table&gt;()</code> show table contents</li> <li><code>heading</code> shows attribute definitions</li> <li><code>describe()</code> show table defintiion with foreign key references</li> </ul> In\u00a0[3]: Copied! <pre>model.VideoRecording.File()\n</pre> model.VideoRecording.File() Out[3]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> <p>file_id</p> <p>file_path</p> filepath of video, relative to root data directory <p>Total: 0</p> In\u00a0[8]: Copied! <pre>model.Model.heading\n</pre> model.Model.heading Out[8]: <pre># \nmodel_name           : varchar(64)                  # user-friendly model name\n---\ntask                 : varchar(32)                  # task in the config yaml\ndate                 : varchar(16)                  # date in the config yaml\niteration            : int                          # iteration/version of this model\nsnapshotindex        : int                          # which snapshot for prediction (if -1, latest)\nshuffle              : int                          # which shuffle of the training dataset\ntrainingsetindex     : int                          # which training set fraction to generate model\nscorer               : varchar(64)                  # scorer/network name - DLC's GetScorerName()\nconfig_template      : longblob                     # dictionary of the config for analyze_videos()\nproject_path         : varchar(255)                 # DLC's project_path in config relative to root\nmodel_prefix=\"\"      : varchar(32)                  # \nmodel_description=\"\" : varchar(1000)                # \nparamset_idx=null    : smallint                     # </pre> In\u00a0[7]: Copied! <pre>train.TrainingTask.describe()\n</pre> train.TrainingTask.describe() <pre># Specification for a DLC model training instance\n-&gt; train.VideoSet\n-&gt; train.TrainingParamSet\ntraining_id          : int                          \n---\nmodel_prefix=\"\"      : varchar(32)                  \nproject_path=\"\"      : varchar(255)                 # DLC's project_path in config relative to root\n\n</pre> Out[7]: <pre>'# Specification for a DLC model training instance\\n-&gt; train.VideoSet\\n-&gt; train.TrainingParamSet\\ntraining_id          : int                          \\n---\\nmodel_prefix=\"\"      : varchar(32)                  \\nproject_path=\"\"      : varchar(255)                 # DLC\\'s project_path in config relative to root\\n'</pre> <p><code>Pipeline.py</code> activates the DataJoint <code>elements</code> and declares other required tables.</p> In\u00a0[2]: Copied! <pre>import datajoint as dj\nfrom workflow_deeplabcut.pipeline import lab, subject, session, train, model\n\n# Directing our pipeline to the appropriate config location\nfrom element_interface.utils import find_full_path\nfrom workflow_deeplabcut.paths import get_dlc_root_data_dir\nconfig_path = find_full_path(get_dlc_root_data_dir(), \n                             'openfield-Pranav-2018-10-30/config.yaml')\n</pre> import datajoint as dj from workflow_deeplabcut.pipeline import lab, subject, session, train, model  # Directing our pipeline to the appropriate config location from element_interface.utils import find_full_path from workflow_deeplabcut.paths import get_dlc_root_data_dir config_path = find_full_path(get_dlc_root_data_dir(),                               'openfield-Pranav-2018-10-30/config.yaml') <pre>Connecting cbroz@dss-db.datajoint.io:3306\n</pre> <p>We can insert entries into <code>dj.Manual</code> tables (green in diagrams) by directly providing values as a dictionary.</p> In\u00a0[7]: Copied! <pre>session.Session.heading\n</pre> session.Session.heading Out[7]: <pre># \nsubject              : varchar(32)                  # \nsession_datetime     : datetime(3)                  # </pre> In\u00a0[4]: Copied! <pre>subject.Subject.insert1(dict(subject='subject6', \n                             sex='F', \n                             subject_birth_date='2020-01-01', \n                             subject_description='hneih_E105'))\nsession_keys = [dict(subject='subject6', session_datetime='2021-06-02 14:04:22'),\n                dict(subject='subject6', session_datetime='2021-06-03 14:43:10')]\nsession.Session.insert(session_keys)\n</pre> subject.Subject.insert1(dict(subject='subject6',                               sex='F',                               subject_birth_date='2020-01-01',                               subject_description='hneih_E105')) session_keys = [dict(subject='subject6', session_datetime='2021-06-02 14:04:22'),                 dict(subject='subject6', session_datetime='2021-06-03 14:43:10')] session.Session.insert(session_keys) <p>We can look at the contents of this table and restrict by a value.</p> In\u00a0[6]: Copied! <pre>session.Session() &amp; \"session_datetime &gt; '2021-06-01 12:00:00'\" &amp; \"subject='subject6'\"\n</pre> session.Session() &amp; \"session_datetime &gt; '2021-06-01 12:00:00'\" &amp; \"subject='subject6'\" Out[6]: <p>subject</p> <p>session_datetime</p> subject6 2021-06-02 14:04:22subject6 2021-06-03 14:43:10 <p>Total: 2</p> <p>The <code>VideoSet</code> table in the <code>train</code> schema retains records of files generated in the video labeling process (e.g., <code>h5</code>, <code>csv</code>, <code>png</code>). DeepLabCut will refer to the <code>mat</code> file located under the <code>training-datasets</code> directory.</p> In\u00a0[\u00a0]: Copied! <pre>train.VideoSet.insert1({'video_set_id': 1})\nlabeled_dir = 'openfield-Pranav-2018-10-30/labeled-data/m4s1/'\ntraining_files = ['CollectedData_Pranav.h5',\n                  'CollectedData_Pranav.csv',\n                  'img0000.png']\nfor idx, filename in training_files:\n    train.VideoSet.File.insert1({'video_set_id': 1,\n                                 'file_id': idx,   \n                                 'file_path': (labeled_dir + file)})\ntrain.VideoSet.File.insert1({'video_set_id':1, 'file_id': 4, 'file_path': \n                            'openfield-Pranav-2018-10-30/videos/m3v1mp4.mp4'})\n</pre> train.VideoSet.insert1({'video_set_id': 1}) labeled_dir = 'openfield-Pranav-2018-10-30/labeled-data/m4s1/' training_files = ['CollectedData_Pranav.h5',                   'CollectedData_Pranav.csv',                   'img0000.png'] for idx, filename in training_files:     train.VideoSet.File.insert1({'video_set_id': 1,                                  'file_id': idx,                                     'file_path': (labeled_dir + file)}) train.VideoSet.File.insert1({'video_set_id':1, 'file_id': 4, 'file_path':                              'openfield-Pranav-2018-10-30/videos/m3v1mp4.mp4'}) In\u00a0[6]: Copied! <pre>train.VideoSet.File()\n</pre> train.VideoSet.File() Out[6]: Paths of training files (e.g., labeled pngs, CSV or video) <p>video_set_id</p> <p>file_path</p> 1 openfield-Pranav-2018-10-30/labeled-data/m4s1/CollectedData_Pranav.csv1 openfield-Pranav-2018-10-30/labeled-data/m4s1/CollectedData_Pranav.h51 openfield-Pranav-2018-10-30/labeled-data/m4s1/img0000.png1 openfield-Pranav-2018-10-30/videos/m3v1mp4.mp4 <p>Total: 4</p> <p>First, we'll add a <code>ModelTrainingParamSet</code>. This is a lookup table that we can reference when training a model.</p> In\u00a0[10]: Copied! <pre>train.TrainingParamSet.heading\n</pre> train.TrainingParamSet.heading Out[10]: <pre>paramset_idx         : smallint                     # \n---\nparamset_desc        : varchar(128)                 # \nparam_set_hash       : uuid                         # hash identifying this parameterset\nparams               : longblob                     # dictionary of all applicable parameters</pre> <p>The <code>params</code> longblob should be a dictionary that captures all items for DeepLabCut's <code>train_network</code> function. At minimum, this is the contents of the project's config file, as well as <code>suffle</code> and <code>trainingsetindex</code>, which are not included in the config.</p> In\u00a0[\u00a0]: Copied! <pre>from deeplabcut import train_network\nhelp(train_network) # for more information on optional parameters\n</pre> from deeplabcut import train_network help(train_network) # for more information on optional parameters <p>Here, we give these items, load the config contents, and overwrite some defaults, including <code>maxiters</code>, to restrict our training iterations to 5.</p> In\u00a0[7]: Copied! <pre>import yaml\n\nparamset_idx = 1; paramset_desc='OpenField'\n\nwith open(config_path, 'rb') as y:\n    config_params = yaml.safe_load(y)\ntraining_params = {'shuffle': '1',\n                   'trainingsetindex': '0',\n                   'maxiters': '5',\n                   'scorer_legacy': 'False',\n                   'maxiters': '5', \n                   'multianimalproject':'False'}\nconfig_params.update(training_params)\ntrain.TrainingParamSet.insert_new_params(paramset_idx=paramset_idx,\n                                         paramset_desc=paramset_desc,\n                                         params=config_params)\n</pre> import yaml  paramset_idx = 1; paramset_desc='OpenField'  with open(config_path, 'rb') as y:     config_params = yaml.safe_load(y) training_params = {'shuffle': '1',                    'trainingsetindex': '0',                    'maxiters': '5',                    'scorer_legacy': 'False',                    'maxiters': '5',                     'multianimalproject':'False'} config_params.update(training_params) train.TrainingParamSet.insert_new_params(paramset_idx=paramset_idx,                                          paramset_desc=paramset_desc,                                          params=config_params) <p>Now, we add a <code>TrainingTask</code>. As a computed table, <code>ModelTraining</code> will reference this to start training when calling <code>populate()</code></p> In\u00a0[19]: Copied! <pre>train.TrainingTask.heading\n</pre> train.TrainingTask.heading Out[19]: <pre>video_set_id         : int                          # \nparamset_idx         : smallint                     # \ntraining_id          : int                          # \n---\nmodel_prefix=\"\"      : varchar(32)                  # \nproject_path=\"\"      : varchar(255)                 # DLC's project_path in config relative to root</pre> In\u00a0[13]: Copied! <pre>key={'video_set_id': 1, 'paramset_idx':1,'training_id':1,\n     'project_path':'openfield-Pranav-2018-10-30/'}\ntrain.TrainingTask.insert1(key, skip_duplicates=True)\ntrain.TrainingTask()\n</pre> key={'video_set_id': 1, 'paramset_idx':1,'training_id':1,      'project_path':'openfield-Pranav-2018-10-30/'} train.TrainingTask.insert1(key, skip_duplicates=True) train.TrainingTask() Out[13]: Specification for a DLC model training instance <p>video_set_id</p> <p>paramset_idx</p> <p>training_id</p> <p>model_prefix</p> <p>project_path</p> DLC's project_path in config relative to root 1 1 1 openfield-Pranav-2018-10-30/ <p>Total: 1</p> In\u00a0[8]: Copied! <pre>train.ModelTraining.populate()\n</pre> train.ModelTraining.populate() In\u00a0[8]: Copied! <pre>train.ModelTraining()\n</pre> train.ModelTraining() Out[8]: <p>video_set_id</p> <p>paramset_idx</p> <p>training_id</p> <p>latest_snapshot</p> latest exact snapshot index (i.e., never -1) <p>config_template</p> stored full config file 1 1 1 5 =BLOB= <p>Total: 1</p> <p>To resume training from a checkpoint, we would need to edit the relevant config file. Emperical work from the Mathis team suggests 200k iterations for any true use-case.</p> <p>The <code>model</code> schema uses a lookup table for managing Body Parts tracked across models.</p> In\u00a0[24]: Copied! <pre>model.BodyPart.heading\n</pre> model.BodyPart.heading Out[24]: <pre># \nbody_part            : varchar(32)                  # \n---\nbody_part_description=\"\" : varchar(1000)                # </pre> <p>Helper functions allow us to first, identify all the new body parts from a given config, and, second, insert them with user-friendly descriptions.</p> In\u00a0[16]: Copied! <pre>model.BodyPart.extract_new_body_parts(config_path)\n</pre> model.BodyPart.extract_new_body_parts(config_path) <pre>Existing body parts: ['leftear' 'rightear' 'snout' 'tailbase']\nNew body parts: []\n</pre> Out[16]: <pre>array([], dtype='&lt;U8')</pre> In\u00a0[9]: Copied! <pre>bp_desc=['Left Ear', 'Right Ear', 'Snout Position', 'Base of Tail']\nmodel.BodyPart.insert_from_config(config_path,bp_desc)\n</pre> bp_desc=['Left Ear', 'Right Ear', 'Snout Position', 'Base of Tail'] model.BodyPart.insert_from_config(config_path,bp_desc) <pre>Existing body parts: []\nNew body parts: ['leftear' 'rightear' 'snout' 'tailbase']\nNew descriptions: ['Left Ear', 'Right Ear', 'Snout Position', 'Base of Tail']\nInsert 4 new body part(s)? [yes, no]:  yes\n</pre> <p>We can insert into <code>Model</code> table for automatic evaluation</p> In\u00a0[\u00a0]: Copied! <pre>model.Model.insert_new_model(model_name='OpenField-5',dlc_config=config_path,\n                             shuffle=1,trainingsetindex=0,\n                             model_description='Open field model trained 5 iterations',\n                             paramset_idx=1)\n</pre> model.Model.insert_new_model(model_name='OpenField-5',dlc_config=config_path,                              shuffle=1,trainingsetindex=0,                              model_description='Open field model trained 5 iterations',                              paramset_idx=1) In\u00a0[11]: Copied! <pre>model.Model()\n</pre> model.Model() Out[11]: <p>model_name</p> user-friendly model name <p>task</p> task in the config yaml <p>date</p> date in the config yaml <p>iteration</p> iteration/version of this model <p>snapshotindex</p> which snapshot for prediction (if -1, latest) <p>shuffle</p> which shuffle of the training dataset <p>trainingsetindex</p> which training set fraction to generate model <p>scorer</p> scorer/network name - DLC's GetScorerName() <p>config_template</p> dictionary of the config for analyze_videos() <p>project_path</p> DLC's project_path in config relative to root <p>model_prefix</p> <p>model_description</p> <p>paramset_idx</p> OpenField-5 openfield Oct30 0 -1 1 0 DLCresnet50openfieldOct30shuffle1 =BLOB= openfield-Pranav-2018-10-30 Open field model trained 5 iterations 1 <p>Total: 1</p> <p><code>ModelEvaluation</code> will reference the <code>Model</code> using the <code>populate</code> method and insert the  output from DeepLabCut's <code>evaluate_network</code> function</p> In\u00a0[47]: Copied! <pre>model.ModelEvaluation.heading\n</pre> model.ModelEvaluation.heading Out[47]: <pre>model_name           : varchar(64)                  # user-friendly model name\n---\ntrain_iterations     : int                          # Training iterations\ntrain_error=null     : float                        # Train error (px)\ntest_error=null      : float                        # Test error (px)\np_cutoff=null        : float                        # p-cutoff used\ntrain_error_p=null   : float                        # Train error with p-cutoff\ntest_error_p=null    : float                        # Test error with p-cutoff</pre> In\u00a0[13]: Copied! <pre>model.ModelEvaluation.populate()\n</pre> model.ModelEvaluation.populate() <pre>Running  DLC_resnet50_openfieldOct30shuffle1_5  with # of training iterations: 5\n</pre> <pre>/Users/cb/miniconda3/envs/venv-dlc/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n  warnings.warn('`layer.apply` is deprecated and '\n</pre> <pre>Running evaluation ...\n</pre> <pre>116it [01:17,  1.50it/s]\n</pre> <pre>Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-5\nResults for 5  training iterations: 95 1 train error: 245.06 pixels. Test error: 247.52  pixels.\nWith pcutoff of 0.4  train error: 239.24 pixels. Test error: 238.07 pixels\nThereby, the errors are given by the average distances between the labels by DLC and the scorer.\nThe network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\nPlease check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\nUse the function 'analyze_video' to make predictions on new videos.\nOtherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n</pre> In\u00a0[14]: Copied! <pre>model.ModelEvaluation()\n</pre> model.ModelEvaluation() Out[14]: <p>model_name</p> user-friendly model name <p>train_iterations</p> Training iterations <p>train_error</p> Train error (px) <p>test_error</p> Test error (px) <p>p_cutoff</p> p-cutoff used <p>train_error_p</p> Train error with p-cutoff <p>test_error_p</p> Test error with p-cutoff OpenField-5 5 245.06 247.52 0.4 239.24 238.07 <p>Total: 1</p> <p>To use our model, we'll first need to insert a session recoring into <code>VideoRecording</code></p> In\u00a0[\u00a0]: Copied! <pre>key = {'subject': 'subject6',\n       'session_datetime': '2021-06-02 14:04:22',\n       'recording_id': '1', 'device': 'Camera1'}\nmodel.VideoRecording.insert1(key)\n\n_ = key.pop('device') # get rid of secondary key from master table\nkey.update({'file_id': 1, \n            'file_path': 'openfield-Pranav-2018-10-30/videos/m3v1mp4-copy.mp4'})\nmodel.VideoRecording.File.insert1(key)\n</pre> key = {'subject': 'subject6',        'session_datetime': '2021-06-02 14:04:22',        'recording_id': '1', 'device': 'Camera1'} model.VideoRecording.insert1(key)  _ = key.pop('device') # get rid of secondary key from master table key.update({'file_id': 1,              'file_path': 'openfield-Pranav-2018-10-30/videos/m3v1mp4-copy.mp4'}) model.VideoRecording.File.insert1(key) In\u00a0[13]: Copied! <pre>model.VideoRecording.File()\n</pre> model.VideoRecording.File() Out[13]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> <p>file_id</p> <p>file_path</p> filepath of video, relative to root data directory subject6 2021-06-02 14:04:22 1 1 openfield-Pranav-2018-10-30/videos/m3v1mp4-copy.mp4 <p>Total: 1</p> <p><code>RecordingInfo</code> automatically populates with file information</p> In\u00a0[14]: Copied! <pre>model.RecordingInfo.populate()\nmodel.RecordingInfo()\n</pre> model.RecordingInfo.populate() model.RecordingInfo() Out[14]: <p>subject</p> <p>session_datetime</p> <p>recording_id</p> <p>px_height</p> height in pixels <p>px_width</p> width in pixels <p>nframes</p> number of frames <p>fps</p> (Hz) frames per second <p>recording_datetime</p> Datetime for the start of the recording <p>recording_duration</p> video duration in seconds subject6 2021-06-02 14:04:22 1 480 640 63 30 None 2.1 <p>Total: 1</p> <p>Next, we specify if the <code>PoseEstimation</code> table should load results from an existing file or trigger the estimation command. Here, we can also specify parameters for DeepLabCut's <code>analyze_videos</code> as a dictionary.</p> In\u00a0[4]: Copied! <pre>key = (model.VideoRecording &amp; {'recording_id': '1'}).fetch1('KEY')\nkey.update({'model_name': 'OpenField-5', 'task_mode': 'trigger'})\nkey\n</pre> key = (model.VideoRecording &amp; {'recording_id': '1'}).fetch1('KEY') key.update({'model_name': 'OpenField-5', 'task_mode': 'trigger'}) key Out[4]: <pre>{'subject': 'subject6',\n 'session_datetime': datetime.datetime(2021, 6, 2, 14, 4, 22),\n 'camera_id': 1,\n 'recording_id': 1,\n 'model_name': 'OpenField-5',\n 'task_mode': 'trigger'}</pre> In\u00a0[5]: Copied! <pre>model.PoseEstimationTask.insert_estimation_task(key,params={'save_as_csv':True})\nmodel.PoseEstimation.populate()\n</pre> model.PoseEstimationTask.insert_estimation_task(key,params={'save_as_csv':True}) model.PoseEstimation.populate() <p>By default, DataJoint will store results in a subdirectory</p> <pre><code>  &lt;processed_dir&gt; / videos / device_&lt;name&gt;_recording_&lt;#&gt;_model_&lt;name&gt;</code></pre> <p>where <code>processed_dir</code> is optionally specified in the datajoint config. If unspecified, this will be the project directory. The device and model names are specified elsewhere in the schema.</p> <p>We can get this estimation directly as a pandas dataframe.</p> In\u00a0[9]: Copied! <pre>model.PoseEstimation.get_trajectory(key)\n</pre> model.PoseEstimation.get_trajectory(key) Out[9]: scorer OpenField-5 bodyparts leftear rightear snout tailbase coords x y z likelihood x y z likelihood x y z likelihood x y z likelihood 0 0.790677 7.965729 0.0 0.397091 115.835762 164.004028 0.0 0.518405 58.818291 4.837649 0.0 0.514612 4.134376 463.009460 0.0 0.717231 1 2.807120 10.973466 0.0 0.435590 10.124892 470.653931 0.0 0.514644 15.192053 472.954376 0.0 0.509128 4.339864 462.988220 0.0 0.711722 2 9.415764 16.290619 0.0 0.400282 10.313096 470.749420 0.0 0.513927 15.203813 473.046204 0.0 0.509683 4.241215 463.060944 0.0 0.709923 3 8.467562 15.072682 0.0 0.407272 10.299086 470.716309 0.0 0.515085 14.914599 472.946564 0.0 0.507931 4.296385 463.385590 0.0 0.704007 4 1.952696 10.845516 0.0 0.388948 10.309416 470.719910 0.0 0.511848 14.834159 472.920166 0.0 0.504538 4.267960 463.363556 0.0 0.702786 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 58 5.497818 12.181496 0.0 0.503961 10.725180 470.430847 0.0 0.505526 15.931270 474.692963 0.0 0.507564 9.060750 481.278442 0.0 0.704268 59 4.192788 10.005349 0.0 0.455334 10.476208 470.846588 0.0 0.499014 3.508626 26.821339 0.0 0.537064 3.786860 462.760376 0.0 0.689251 60 2.216149 10.115728 0.0 0.420141 10.644203 471.036102 0.0 0.487316 3.166887 26.835373 0.0 0.548109 8.188313 481.524902 0.0 0.707340 61 5.196610 10.838953 0.0 0.484508 178.007233 72.935913 0.0 0.576688 4.478888 26.513628 0.0 0.531905 4.350879 462.553345 0.0 0.703052 62 2.678554 10.277241 0.0 0.426758 10.260103 471.321564 0.0 0.502590 15.026831 472.492065 0.0 0.528700 8.123420 481.642578 0.0 0.707681 <p>63 rows \u00d7 16 columns</p> <p>.</p> <p>Below is a more automatic approach to run through the pipeline using some utility functions in the workflow using the <code>process.py</code> script to automatically trigger all computed tables.</p> <p>Because we just inserted all the data, we'll delete using the command below to start over.</p> In\u00a0[\u00a0]: Copied! <pre>from workflow_deeplabcut.process import run\nsafemode=None # Set to false to turn off confirmation prompts\n(session.Session &amp; 'subject=\"subject6\"').delete(safemode=safemode)\ntrain.TrainingParamSet.delete(safemode=safemode)\ntrain.VideoSet.delete(safemode=safemode)\n</pre> from workflow_deeplabcut.process import run safemode=None # Set to false to turn off confirmation prompts (session.Session &amp; 'subject=\"subject6\"').delete(safemode=safemode) train.TrainingParamSet.delete(safemode=safemode) train.VideoSet.delete(safemode=safemode) In\u00a0[2]: Copied! <pre>from workflow_deeplabcut.ingest import ingest_subjects, ingest_sessions, ingest_dlc_items\ningest_subjects(); ingest_sessions(); ingest_dlc_items()\n</pre> from workflow_deeplabcut.ingest import ingest_subjects, ingest_sessions, ingest_dlc_items ingest_subjects(); ingest_sessions(); ingest_dlc_items() <pre>\n---- Inserting 0 entry(s) into subject ----\n\n---- Inserting 2 entry(s) into session ----\n\n---- Inserting 2 entry(s) into session_directory ----\n\n---- Inserting 2 entry(s) into session_note ----\n\n---- Inserting 3 entry(s) into #model_training_param_set ----\n\n---- Inserting 2 entry(s) into video_set ----\n\n---- Inserting 8 entry(s) into video_set__file ----\n\n---- Inserting 2 entry(s) into video_recording ----\n\n---- Inserting 2 entry(s) into video_recording__file ----\n</pre> In\u00a0[4]: Copied! <pre>import datajoint as dj; dj.config.load('dj_local_conf.json')\nfrom element_interface.utils import find_full_path\ndata_dir = find_full_path(dj.config['custom']['dlc_root_data_dir'], # root from config\n                          'openfield-Pranav-2018-10-30')            # DLC project dir\nconfig_path = (data_dir / 'config.yaml')\n</pre> import datajoint as dj; dj.config.load('dj_local_conf.json') from element_interface.utils import find_full_path data_dir = find_full_path(dj.config['custom']['dlc_root_data_dir'], # root from config                           'openfield-Pranav-2018-10-30')            # DLC project dir config_path = (data_dir / 'config.yaml') In\u00a0[\u00a0]: Copied! <pre>key={'paramset_idx':1,'training_id':1,'video_set_id':1, \n     'project_path':'openfield-Pranav-2018-10-30/'}\ntrain.TrainingTask.insert1(key, skip_duplicates=True)\nrun(verbose=True)\nmodel.RecordingInfo()\n</pre> key={'paramset_idx':1,'training_id':1,'video_set_id':1,       'project_path':'openfield-Pranav-2018-10-30/'} train.TrainingTask.insert1(key, skip_duplicates=True) run(verbose=True) model.RecordingInfo() <p>Now, add to <code>Model</code>, including</p> <ul> <li>Include a user-friendly <code>model_name</code></li> <li>Include the relative path for the project's <code>config.yaml</code></li> <li>Add <code>shuffle</code> and <code>trainingsetindex</code></li> <li><code>insert_new_model</code> will prompt before inserting, but this can be skipped with <code>prompt=False</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre>model.Model.insert_new_model(model_name='OpenField-5', \n                             dlc_config=config_path,\n                             shuffle=1,\n                             trainingsetindex=0,\n                             paramset_idx=1, \n                             prompt=True, # True is the default behavior\n                             model_description='Open field model trained 5 iterations')\nrun()\n</pre> model.Model.insert_new_model(model_name='OpenField-5',                               dlc_config=config_path,                              shuffle=1,                              trainingsetindex=0,                              paramset_idx=1,                               prompt=True, # True is the default behavior                              model_description='Open field model trained 5 iterations') run() <p>Add a pose estimation task, using</p> <ul> <li>All primary key information for a given recording</li> <li>Add the model and <code>task_mode</code> (i.e., load vs. trigger) to be applied</li> <li>Add any additional analysis parameters for <code>deeplabcut.analyze_videos</code></li> </ul> In\u00a0[\u00a0]: Copied! <pre>key=(model.VideoRecording &amp; 'recording_id=2').fetch1('KEY')\nkey.update({'model_name': 'OpenField-5', 'task_mode': 'trigger'})\nanalyze_params={'save_as_csv':True} # add any others from deeplabcut.analyze_videos\nmodel.PoseEstimationTask.insert_estimation_task(key,params=analyze_params)\nrun()\n</pre> key=(model.VideoRecording &amp; 'recording_id=2').fetch1('KEY') key.update({'model_name': 'OpenField-5', 'task_mode': 'trigger'}) analyze_params={'save_as_csv':True} # add any others from deeplabcut.analyze_videos model.PoseEstimationTask.insert_estimation_task(key,params=analyze_params) run() <p>Retrieve estimated position data:</p> In\u00a0[7]: Copied! <pre>model.PoseEstimation.get_trajectory(key)\n</pre> model.PoseEstimation.get_trajectory(key) Out[7]: scorer OpenField-5 bodyparts leftear rightear snout tailbase coords x y z likelihood x y z likelihood x y z likelihood x y z likelihood 0 125.213768 0.464425 0.0 0.142836 1.902155 184.619431 0.0 0.123875 -7.285146 61.402088 0.0 0.267532 2.360505 30.929823 0.0 0.132607 1 125.009758 1.058969 0.0 0.136179 1.532405 183.668121 0.0 0.130291 -7.269304 61.589397 0.0 0.269269 9.910207 168.702576 0.0 0.140683 2 123.785698 1.801253 0.0 0.150994 1.467412 183.721542 0.0 0.129725 -6.988381 61.624317 0.0 0.266620 2.753981 30.949059 0.0 0.136884 3 122.621880 2.729937 0.0 0.150831 1.424251 184.009323 0.0 0.133028 -7.054953 61.331848 0.0 0.286876 2.399938 30.467714 0.0 0.146240 4 123.729645 2.901060 0.0 0.163442 1.417472 183.914078 0.0 0.129994 -6.633567 60.880890 0.0 0.283661 2.544708 30.362843 0.0 0.139938 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 58 240.315948 -1.135241 0.0 0.141477 2.564324 153.450378 0.0 0.108718 -6.014613 59.291553 0.0 0.264213 2.494397 30.713549 0.0 0.127640 59 240.919571 -1.104096 0.0 0.122847 6.900490 -0.243096 0.0 0.104687 -6.632689 59.683407 0.0 0.236766 3.034356 30.454117 0.0 0.127521 60 255.197067 -0.876162 0.0 0.141331 3.224912 170.105179 0.0 0.102174 -6.597838 59.643513 0.0 0.236705 2.666216 30.185883 0.0 0.123881 61 255.042603 0.554700 0.0 0.152119 6.523534 -0.563077 0.0 0.102816 -6.134833 59.962490 0.0 0.249565 2.555799 30.326237 0.0 0.130592 62 255.079330 -0.326163 0.0 0.168699 3.389258 170.141495 0.0 0.103460 -6.661276 59.593884 0.0 0.236305 2.978589 30.103178 0.0 0.124924 <p>63 rows \u00d7 16 columns</p> <ul> <li>Schemas are not typically dropped in a production workflow with real data in it.</li> <li>At the developmental phase, it might be required for the table redesign.</li> <li>When dropping all schemas is needed, drop items starting with the most downstream.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from workflow_deeplabcut.pipeline import *\n# model.schema.drop()\n# train.schema.drop()\n# session.schema.drop()\n# subject.schema.drop()\n# lab.schema.drop()\n</pre> from workflow_deeplabcut.pipeline import * # model.schema.drop() # train.schema.drop() # session.schema.drop() # subject.schema.drop() # lab.schema.drop()"}, {"location": "tutorials/09-AlternateDataset/#workflow-deeplabcut-alternate-data", "title": "Workflow DeepLabCut - Alternate Data\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#introduction", "title": "Introduction\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#example-data", "title": "Example data\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#download", "title": "Download\u00b6", "text": "<p>If you've already cloned the main DLC repository, you already have this folder under <code>examples/openfield-Pranav-2018-10-30</code>. This link via DownGit will start the single-directory download automatically as a zip. Unpack this zip and place it in a directory we'll refer to as your root.</p>"}, {"location": "tutorials/09-AlternateDataset/#structure", "title": "Structure\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#new-video", "title": "New video\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#configuring-datajoint", "title": "Configuring DataJoint\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#datajoint-local-config", "title": "DataJoint Local Config\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#configure-database-credentials", "title": "Configure database credentials\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#workflow-specific-items", "title": "Workflow-specific items\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#saving-the-config", "title": "Saving the config\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#workflow-structure", "title": "Workflow Structure\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#schemas-diagrams-and-tables", "title": "Schemas, Diagrams and Tables\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#table-types", "title": "Table Types\u00b6", "text": "<ul> <li>Manual table: green box, manually inserted table, expect new entries daily, e.g. Subject, ProbeInsertion.</li> <li>Lookup table: gray box, pre inserted table, commonly used for general facts or parameters. e.g. Strain, ClusteringMethod, ClusteringParamSet.</li> <li>Imported table: blue oval, auto-processing table, the processing depends on the importing of external files. e.g. process of Clustering requires output files from kilosort2.</li> <li>Computed table: red circle, auto-processing table, the processing does not depend on files external to the database, commonly used for</li> <li>Part table: plain text, as an appendix to the master table, all the part entries of a given master entry represent a intact set of the master entry. e.g. Unit of a CuratedClustering.</li> </ul>"}, {"location": "tutorials/09-AlternateDataset/#table-links", "title": "Table Links\u00b6", "text": "<ul> <li>One-to-one primary: thick solid line, share the exact same primary key, meaning the child table inherits all the primary key fields from the parent table as its own primary key.</li> <li>One-to-many primary: thin solid line, inherit the primary key from the parent table, but have additional field(s) as part of the primary key as well</li> <li>Secondary dependency: dashed line, the child table inherits the primary key fields from parent table as its own secondary attribute.</li> </ul>"}, {"location": "tutorials/09-AlternateDataset/#common-table-functions", "title": "Common Table Functions\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#running-the-workflow", "title": "Running the Workflow\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#manually-inserting-entries", "title": "Manually Inserting Entries\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#upstream-tables", "title": "Upstream tables\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#deeplabcut-tables", "title": "DeepLabcut Tables\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#training-a-network", "title": "Training a Network\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#tracking-jointsbody-parts", "title": "Tracking Joints/Body Parts\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#declaringevaluating-a-model", "title": "Declaring/Evaluating a Model\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#pose-estimation", "title": "Pose Estimation\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#workflow-automation", "title": "Workflow Automation\u00b6", "text": ""}, {"location": "tutorials/09-AlternateDataset/#automated-ingestion", "title": "Automated Ingestion\u00b6", "text": "<p>Refer to the <code>user_data</code> folder in the workflow for CSVs to fill in various tables.</p> <ol> <li>Upstream tables:<ul> <li><code>subject.Subject</code> via <code>subjects.csv</code></li> <li><code>session.Session</code> via <code>sessions.csv</code></li> </ul> </li> <li><code>train</code> schema:<ul> <li><code>train.TrainingParamSet</code> via <code>config_params.csv</code></li> <li><code>train.VideoSet</code> via <code>train_videosets.csv</code></li> </ul> </li> <li><code>model</code> schema:<ul> <li><code>model.VideoRecording</code> via <code>model_videos.csv</code></li> <li><code>model.Model</code> via <code>model_model.csv</code></li> </ul> </li> </ol> <p>Run automatic ingestion via functions in <code>workflow_deeplabcut.ingest</code></p>"}, {"location": "tutorials/09-AlternateDataset/#setting-project-variables", "title": "Setting project variables\u00b6", "text": "<p>Set your root directory in your DataJoint config file, under <code>custom</code> as <code>dlc_root_data_dir</code>.</p>"}, {"location": "tutorials/09-AlternateDataset/#launching-trainig", "title": "Launching trainig\u00b6", "text": "<p>Pair training files with training parameters, and launch training via <code>process</code>.</p> <p>Note: DLC's model processes (e.g., Training, Evaluation) log a lot of information to the console, to quiet this, pass <code>verbose=False</code> to <code>process</code></p>"}, {"location": "tutorials/09-AlternateDataset/#dropping-schemas", "title": "Dropping schemas\u00b6", "text": ""}]}