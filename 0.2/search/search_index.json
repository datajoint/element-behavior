{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "", "text": "Element DeepLabCut for Pose Estimation \u00b6 DataJoint Element for markerless pose estimation with DeepLabCut . DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline. Element DeepLabCut runs DeepLabCut which uses image recognition machine learning models to generate animal position estimates from consumer grade video equipment. The Element is composed of two schemas for storing data and running analysis: train - Manages model training model - Manages models and launches pose estimation Visit the Concepts page for more information on pose estimation and Element DeepLabCut. To get started with building your data pipeline visit the Tutorials page .", "title": "Element DeepLabCut"}, {"location": "#element-deeplabcut-for-pose-estimation", "text": "DataJoint Element for markerless pose estimation with DeepLabCut . DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline. Element DeepLabCut runs DeepLabCut which uses image recognition machine learning models to generate animal position estimates from consumer grade video equipment. The Element is composed of two schemas for storing data and running analysis: train - Manages model training model - Manages models and launches pose estimation Visit the Concepts page for more information on pose estimation and Element DeepLabCut. To get started with building your data pipeline visit the Tutorials page .", "title": "Element DeepLabCut for Pose Estimation"}, {"location": "changelog/", "text": "Changelog \u00b6 Observes Semantic Versioning standard and Keep a Changelog convention. [0.2.1] - 2022-10-23 \u00b6 Update - Docstrings for mkdocs deployment 0.2.0 - 2022-10-10 \u00b6 Update - Remove direct dependency ( element-interface ) for PyPI release. Update - Docstring PEP257 compliance #24 Update - Explicit handling of KeyboardInterrupt #26 Update - Streamline insert_new_params #27 Update - Relocate module imports to the top of the files Update - Missing f for formatted string in read_yaml Change - Rename datajoint-saved config to dj_dlc_config.yaml Add - Call reusable CICD Add - NWB export Add - mkdocs deployment with workflow API docs 0.1.1 - 2022-06-10 \u00b6 Fixed - Replace lazy imports Fixed - Project path in the model.Model 0.1.0 - 2022-05-10 \u00b6 Add - Adopted black formatting into code base Add - Table for RecordingInfo Add - File ID for tracking updatable secondary key filepaths Add - make functions for Computed/Imported tables 0.0.0a - 2021-11-15 \u00b6 Add - Drafts from a collection of precursor pipelines, including DataJoint_Demo_DeepLabCut graciously provided by the Mathis Lab. Add - Support for 2d single-animal models", "title": "Changelog"}, {"location": "changelog/#changelog", "text": "Observes Semantic Versioning standard and Keep a Changelog convention.", "title": "Changelog"}, {"location": "changelog/#021-2022-10-23", "text": "Update - Docstrings for mkdocs deployment", "title": "[0.2.1] - 2022-10-23"}, {"location": "changelog/#020-2022-10-10", "text": "Update - Remove direct dependency ( element-interface ) for PyPI release. Update - Docstring PEP257 compliance #24 Update - Explicit handling of KeyboardInterrupt #26 Update - Streamline insert_new_params #27 Update - Relocate module imports to the top of the files Update - Missing f for formatted string in read_yaml Change - Rename datajoint-saved config to dj_dlc_config.yaml Add - Call reusable CICD Add - NWB export Add - mkdocs deployment with workflow API docs", "title": "0.2.0 - 2022-10-10"}, {"location": "changelog/#011-2022-06-10", "text": "Fixed - Replace lazy imports Fixed - Project path in the model.Model", "title": "0.1.1 - 2022-06-10"}, {"location": "changelog/#010-2022-05-10", "text": "Add - Adopted black formatting into code base Add - Table for RecordingInfo Add - File ID for tracking updatable secondary key filepaths Add - make functions for Computed/Imported tables", "title": "0.1.0 - 2022-05-10"}, {"location": "changelog/#000a-2021-11-15", "text": "Add - Drafts from a collection of precursor pipelines, including DataJoint_Demo_DeepLabCut graciously provided by the Mathis Lab. Add - Support for 2d single-animal models", "title": "0.0.0a - 2021-11-15"}, {"location": "citation/", "text": "Citation \u00b6 If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID). Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358 DataJoint Elements ( RRID:SCR_021894 ) - Element DeepLabCut (version 0.2.1)", "title": "Citation"}, {"location": "citation/#citation", "text": "If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID). Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358 DataJoint Elements ( RRID:SCR_021894 ) - Element DeepLabCut (version 0.2.1)", "title": "Citation"}, {"location": "concepts/", "text": "Concepts \u00b6 Pose Estimation in Neurophysiology \u00b6 Studying the inner workings of the brain requires understanding the relationship between neural activity and environmental stimuli, natural behavior, or inferred cognitive states. Pose estimation is a computer vision method to track the position, and thereby behavior, of the subject over the course of an experiment, which can then be paired with neuronal recordings to answer scientific questions about the brain. Previous pose estimation methods required reflective markers placed on a subject, as well as multiple expensive high-frame-rate infrared cameras to triangulate position within a limited field. Recent advancements in machine learning have facilitated dramatic advancements in capturing pose data with a video camera alone. In particular, DeepLabCut (DLC) facilitates the use of pre-trained machine learning models for 2-D and 3-D non-invasive markerless pose estimation. DeepLabCut offers the ability to continue training an exisiting object detection model to further specialize in videos in the training data set. In other words, researchers can take a well-known generalizable machine learning model and apply it to their experimental setup, making it relatively easy to produce pose estimation inferences for subsequent experimental sessions. While some alternative tools are either species-specific (e.g., DeepFly3D ) or uniquely 2D (e.g., DeepPoseKit ), DLC highlights a diversity of use-cases via a Model Zoo . Even compared to tools with similar functionality (e.g., SLEAP and dannce ), DLC has more users, as measured by either GitHub forks or more citations (1600 vs. 900). DLC's trajectory toward an industry standard is attributable to continued funding , extensive documentation and both creator- and peer-support. Other comparable tools include mmpose , idtracker.ai , TREBA , B-KinD , VAME , and MARS . Key Partnerships \u00b6 Mackenzie Mathis (Swiss Federal Institute of Technology Lausanne) is both a lead developer of DLC and a key advisor on DataJoint open source development as a member of the Scientific Steering Committee . DataJoint is also partnered with a number of groups who use DLC as part of broader workflows. In these collaborations, members of the DataJoint team have interviewed the scientists to understand their needs in experimental setup, pipeline design, and interfaces. These teams include: Moser Group (Norwegian University of Science and Technology) - see pipeline design Mesoscale Activity Project (Janelia Research Campus/Baylor College of Medicine/New York University) Hui-Chen Lu Lab (Indiana University) Tobias Rose Lab (University of Bonn) James Cotton Lab (Northwestern University) Element Features \u00b6 Development of the Element began with an open source repository shared by the Mathis team. We further identified common needs across our respective partnerships to offer the following features for single-camera 2D models: Manage training data and configuration parameters Launch model training Evaluate models automatically and directly compare models Manage model metadata Launch inference video analysis Capture pose estimation output for each session Element Architecture \u00b6 Each node in the following diagram represents the analysis code in the workflow and the corresponding tables in the database. Within the workflow, Element DeepLabCut connects to upstream Elements including Lab, Animal, and Session. For more detailed documentation on each table, see the API docs for the respective schemas. lab schema ( API docs ) \u00b6 Table Description Device Camera metadata subject schema ( API docs ) \u00b6 Although not required, most choose to connect the Session table to a Subject table. Table Description Subject Basic information of the research subject session schema ( API docs ) \u00b6 Table Description Session Unique experimental session identifier train schema ( API docs ) \u00b6 Optional tables related to model training. Table Description VideoSet Set of files corresponding to a training dataset. TrainingParamSet A collection of model training parameters, represented by an index. TrainingTask A set of tasks specifying model training methods. ModelTraining A record of training iterations launched by TrainingTask . model schema ( API ) \u00b6 Tables related to DeepLabCut models and pose estimation. The model schema can be used without the train schema. Table Description VideoRecording Video(s) from one recording session, for pose estimation. BodyPart Unique body parts (a.k.a. joints) and descriptions thereof. Model A central table for storing unique models. ModelEvaluation Evaluation results for each model. PoseEstimationTask A series of pose estimation tasks to be completed. Pairings of video recordings with models to be use for pose estimation. PoseEstimation Results of pose estimation using a given model. Data Export and Publishing \u00b6 Element DeepLabCut includes an export function that saves the outputs as a Neurodata Without Borders (NWB) file. By running a single command, the data from an experimental session is saved to a NWB file. For more details on the export function, see the Tutorials page . Once NWB files are generated they can be readily shared with collaborators and published on DANDI Archive . The DataJoint Elements ecosystem includes a function to upload the NWB files to DANDI (see Element Interface ). 1 dlc_session_to_nwb ( pose_key , use_element_session , session_kwargs ) Roadmap \u00b6 Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will add the following features to this Element: Support for multi-animal or multi-camera models Tools to label training data", "title": "Concepts"}, {"location": "concepts/#concepts", "text": "", "title": "Concepts"}, {"location": "concepts/#pose-estimation-in-neurophysiology", "text": "Studying the inner workings of the brain requires understanding the relationship between neural activity and environmental stimuli, natural behavior, or inferred cognitive states. Pose estimation is a computer vision method to track the position, and thereby behavior, of the subject over the course of an experiment, which can then be paired with neuronal recordings to answer scientific questions about the brain. Previous pose estimation methods required reflective markers placed on a subject, as well as multiple expensive high-frame-rate infrared cameras to triangulate position within a limited field. Recent advancements in machine learning have facilitated dramatic advancements in capturing pose data with a video camera alone. In particular, DeepLabCut (DLC) facilitates the use of pre-trained machine learning models for 2-D and 3-D non-invasive markerless pose estimation. DeepLabCut offers the ability to continue training an exisiting object detection model to further specialize in videos in the training data set. In other words, researchers can take a well-known generalizable machine learning model and apply it to their experimental setup, making it relatively easy to produce pose estimation inferences for subsequent experimental sessions. While some alternative tools are either species-specific (e.g., DeepFly3D ) or uniquely 2D (e.g., DeepPoseKit ), DLC highlights a diversity of use-cases via a Model Zoo . Even compared to tools with similar functionality (e.g., SLEAP and dannce ), DLC has more users, as measured by either GitHub forks or more citations (1600 vs. 900). DLC's trajectory toward an industry standard is attributable to continued funding , extensive documentation and both creator- and peer-support. Other comparable tools include mmpose , idtracker.ai , TREBA , B-KinD , VAME , and MARS .", "title": "Pose Estimation in Neurophysiology"}, {"location": "concepts/#key-partnerships", "text": "Mackenzie Mathis (Swiss Federal Institute of Technology Lausanne) is both a lead developer of DLC and a key advisor on DataJoint open source development as a member of the Scientific Steering Committee . DataJoint is also partnered with a number of groups who use DLC as part of broader workflows. In these collaborations, members of the DataJoint team have interviewed the scientists to understand their needs in experimental setup, pipeline design, and interfaces. These teams include: Moser Group (Norwegian University of Science and Technology) - see pipeline design Mesoscale Activity Project (Janelia Research Campus/Baylor College of Medicine/New York University) Hui-Chen Lu Lab (Indiana University) Tobias Rose Lab (University of Bonn) James Cotton Lab (Northwestern University)", "title": "Key Partnerships"}, {"location": "concepts/#element-features", "text": "Development of the Element began with an open source repository shared by the Mathis team. We further identified common needs across our respective partnerships to offer the following features for single-camera 2D models: Manage training data and configuration parameters Launch model training Evaluate models automatically and directly compare models Manage model metadata Launch inference video analysis Capture pose estimation output for each session", "title": "Element Features"}, {"location": "concepts/#element-architecture", "text": "Each node in the following diagram represents the analysis code in the workflow and the corresponding tables in the database. Within the workflow, Element DeepLabCut connects to upstream Elements including Lab, Animal, and Session. For more detailed documentation on each table, see the API docs for the respective schemas.", "title": "Element Architecture"}, {"location": "concepts/#lab-schema-api-docs", "text": "Table Description Device Camera metadata", "title": "lab schema (API docs)"}, {"location": "concepts/#subject-schema-api-docs", "text": "Although not required, most choose to connect the Session table to a Subject table. Table Description Subject Basic information of the research subject", "title": "subject schema (API docs)"}, {"location": "concepts/#session-schema-api-docs", "text": "Table Description Session Unique experimental session identifier", "title": "session schema (API docs)"}, {"location": "concepts/#train-schema-api-docs", "text": "Optional tables related to model training. Table Description VideoSet Set of files corresponding to a training dataset. TrainingParamSet A collection of model training parameters, represented by an index. TrainingTask A set of tasks specifying model training methods. ModelTraining A record of training iterations launched by TrainingTask .", "title": "train schema (API docs)"}, {"location": "concepts/#model-schema-api", "text": "Tables related to DeepLabCut models and pose estimation. The model schema can be used without the train schema. Table Description VideoRecording Video(s) from one recording session, for pose estimation. BodyPart Unique body parts (a.k.a. joints) and descriptions thereof. Model A central table for storing unique models. ModelEvaluation Evaluation results for each model. PoseEstimationTask A series of pose estimation tasks to be completed. Pairings of video recordings with models to be use for pose estimation. PoseEstimation Results of pose estimation using a given model.", "title": "model schema (API)"}, {"location": "concepts/#data-export-and-publishing", "text": "Element DeepLabCut includes an export function that saves the outputs as a Neurodata Without Borders (NWB) file. By running a single command, the data from an experimental session is saved to a NWB file. For more details on the export function, see the Tutorials page . Once NWB files are generated they can be readily shared with collaborators and published on DANDI Archive . The DataJoint Elements ecosystem includes a function to upload the NWB files to DANDI (see Element Interface ). 1 dlc_session_to_nwb ( pose_key , use_element_session , session_kwargs )", "title": "Data Export and Publishing"}, {"location": "concepts/#roadmap", "text": "Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will add the following features to this Element: Support for multi-animal or multi-camera models Tools to label training data", "title": "Roadmap"}, {"location": "tutorials/", "text": "Tutorials \u00b6 Coming soon!", "title": "Tutorials"}, {"location": "tutorials/#tutorials", "text": "Coming soon!", "title": "Tutorials"}, {"location": "api/element_deeplabcut/model/", "text": "Code adapted from the Mathis Lab MIT License Copyright (c) 2022 Mackenzie Mathis DataJoint Schema for DeepLabCut 2.x, Supports 2D and 3D DLC via triangulation. activate ( model_schema_name , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activate this schema. Parameters: Name Type Description Default model_schema_name str schema name on the database server required create_schema bool when True (default), create schema in the database if it does not yet exist. True create_tables str when True (default), create schema tables in the database if they do not yet exist. True linking_module str a module (or name) containing the required dependencies. None Dependencies: Upstream tables Session: A parent table to VideoRecording, identifying a recording session. Equipment: A parent table to VideoRecording, identifying a recording device. Functions get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder. Source code in element_deeplabcut/model.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def activate ( model_schema_name : str , * , create_schema : bool = True , create_tables : bool = True , linking_module : bool = None , ): \"\"\"Activate this schema. Args: model_schema_name (str): schema name on the database server create_schema (bool): when True (default), create schema in the database if it does not yet exist. create_tables (str): when True (default), create schema tables in the database if they do not yet exist. linking_module (str): a module (or name) containing the required dependencies. Dependencies: Upstream tables: Session: A parent table to VideoRecording, identifying a recording session. Equipment: A parent table to VideoRecording, identifying a recording device. Functions: get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" assert hasattr ( linking_module , \"get_dlc_root_data_dir\" ), \"The linking module must specify a lookup funtion for a root data directory\" global _linking_module _linking_module = linking_module # activate schema . activate ( model_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) get_dlc_root_data_dir () \u00b6 Pulls relevant func from parent namespace to specify root data dir(s). It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strongs for possible root data directories. Source code in element_deeplabcut/model.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_dlc_root_data_dir () -> list : \"\"\"Pulls relevant func from parent namespace to specify root data dir(s). It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strongs for possible root data directories. \"\"\" root_directories = _linking_module . get_dlc_root_data_dir () if isinstance ( root_directories , ( str , Path )): root_directories = [ root_directories ] if ( hasattr ( _linking_module , \"get_dlc_processed_data_dir\" ) and get_dlc_processed_data_dir () not in root_directories ): root_directories . append ( _linking_module . get_dlc_processed_data_dir ()) return root_directories get_dlc_processed_data_dir () \u00b6 Pulls relevant func from parent namespace. Defaults to DLC's project /videos/. Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default. Source code in element_deeplabcut/model.py 100 101 102 103 104 105 106 107 108 109 110 def get_dlc_processed_data_dir () -> Optional [ str ]: \"\"\"Pulls relevant func from parent namespace. Defaults to DLC's project /videos/. Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default. \"\"\" if hasattr ( _linking_module , \"get_dlc_processed_data_dir\" ): return _linking_module . get_dlc_processed_data_dir () else : return None VideoRecording \u00b6 Bases: dj . Manual Set of video recordings for DLC inferences. Attributes: Name Type Description Session foreign key Session primary key. Equipment foreign key Equipment primary key, used for default output directory path information. recording_id int Unique recording ID. recording_start_time datetime Recording start time. Source code in element_deeplabcut/model.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 @schema class VideoRecording ( dj . Manual ): \"\"\"Set of video recordings for DLC inferences. Attributes: Session (foreign key): Session primary key. Equipment (foreign key): Equipment primary key, used for default output directory path information. recording_id (int): Unique recording ID. recording_start_time (datetime): Recording start time.\"\"\" definition = \"\"\" -> Session recording_id: int --- -> Device \"\"\" class File ( dj . Part ): \"\"\"File IDs and paths associated with a given recording_id Attributes: VideoRecording (foreign key): Video recording primary key. file_path ( varchar(255) ): file path of video, relative to root data dir. \"\"\" definition = \"\"\" -> master file_id: int --- file_path: varchar(255) # filepath of video, relative to root data directory \"\"\" File \u00b6 Bases: dj . Part File IDs and paths associated with a given recording_id Attributes: Name Type Description VideoRecording foreign key Video recording primary key. file_path varchar(255) file path of video, relative to root data dir. Source code in element_deeplabcut/model.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class File ( dj . Part ): \"\"\"File IDs and paths associated with a given recording_id Attributes: VideoRecording (foreign key): Video recording primary key. file_path ( varchar(255) ): file path of video, relative to root data dir. \"\"\" definition = \"\"\" -> master file_id: int --- file_path: varchar(255) # filepath of video, relative to root data directory \"\"\" RecordingInfo \u00b6 Bases: dj . Imported Automated table with video file metadata. Attributes: Name Type Description VideoRecording foreign key Video recording key. px_height smallint Height in pixels. px_width smallint Width in pixels. nframes smallint Number of frames. fps int Optional. Frames per second, Hz. recording_datetime datetime Optional. Datetime for the start of recording. recording_duration float video duration (s) from nframes / fps. Source code in element_deeplabcut/model.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 @schema class RecordingInfo ( dj . Imported ): \"\"\"Automated table with video file metadata. Attributes: VideoRecording (foreign key): Video recording key. px_height (smallint): Height in pixels. px_width (smallint): Width in pixels. nframes (smallint): Number of frames. fps (int): Optional. Frames per second, Hz. recording_datetime (datetime): Optional. Datetime for the start of recording. recording_duration (float): video duration (s) from nframes / fps.\"\"\" definition = \"\"\" -> VideoRecording --- px_height : smallint # height in pixels px_width : smallint # width in pixels nframes : smallint # number of frames fps = NULL : int # (Hz) frames per second recording_datetime = NULL : datetime # Datetime for the start of the recording recording_duration : float # video duration (s) from nframes / fps \"\"\" @property def key_source ( self ): \"\"\"Defines order of keys for make function when called via `populate()`\"\"\" return VideoRecording & VideoRecording . File def make ( self , key ): \"\"\"Populates table with video metadata using CV2.\"\"\" file_paths = ( VideoRecording . File & key ) . fetch ( \"file_path\" ) nframes = 0 px_height , px_width , fps = None , None , None for file_path in file_paths : file_path = ( find_full_path ( get_dlc_root_data_dir (), file_path )) . as_posix () cap = cv2 . VideoCapture ( file_path ) info = ( int ( cap . get ( cv2 . CAP_PROP_FRAME_HEIGHT )), int ( cap . get ( cv2 . CAP_PROP_FRAME_WIDTH )), int ( cap . get ( cv2 . CAP_PROP_FPS )), ) if px_height is not None : assert ( px_height , px_width , fps ) == info px_height , px_width , fps = info nframes += int ( cap . get ( cv2 . CAP_PROP_FRAME_COUNT )) cap . release () self . insert1 ( { ** key , \"px_height\" : px_height , \"px_width\" : px_width , \"nframes\" : nframes , \"fps\" : fps , \"recording_duration\" : nframes / fps , } ) key_source () property \u00b6 Defines order of keys for make function when called via populate() Source code in element_deeplabcut/model.py 174 175 176 177 @property def key_source ( self ): \"\"\"Defines order of keys for make function when called via `populate()`\"\"\" return VideoRecording & VideoRecording . File make ( key ) \u00b6 Populates table with video metadata using CV2. Source code in element_deeplabcut/model.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def make ( self , key ): \"\"\"Populates table with video metadata using CV2.\"\"\" file_paths = ( VideoRecording . File & key ) . fetch ( \"file_path\" ) nframes = 0 px_height , px_width , fps = None , None , None for file_path in file_paths : file_path = ( find_full_path ( get_dlc_root_data_dir (), file_path )) . as_posix () cap = cv2 . VideoCapture ( file_path ) info = ( int ( cap . get ( cv2 . CAP_PROP_FRAME_HEIGHT )), int ( cap . get ( cv2 . CAP_PROP_FRAME_WIDTH )), int ( cap . get ( cv2 . CAP_PROP_FPS )), ) if px_height is not None : assert ( px_height , px_width , fps ) == info px_height , px_width , fps = info nframes += int ( cap . get ( cv2 . CAP_PROP_FRAME_COUNT )) cap . release () self . insert1 ( { ** key , \"px_height\" : px_height , \"px_width\" : px_width , \"nframes\" : nframes , \"fps\" : fps , \"recording_duration\" : nframes / fps , } ) BodyPart \u00b6 Bases: dj . Lookup Body parts tracked by DeepLabCut models Attributes: Name Type Description Model foreign key Model name. BodyPart foreign key Body part short name. Source code in element_deeplabcut/model.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 @schema class BodyPart ( dj . Lookup ): \"\"\"Body parts tracked by DeepLabCut models Attributes: Model (foreign key): Model name. BodyPart (foreign key): Body part short name.\"\"\" definition = \"\"\" body_part : varchar(32) --- body_part_description='' : varchar(1000) \"\"\" @classmethod def extract_new_body_parts ( cls , dlc_config : dict , verbose : bool = True ): \"\"\"Returns list of body parts present in dlc config, but not BodyPart table. Args: dlc_config (str or dict): path to a config.y*ml, or dict of such contents. verbose (bool): Default True. Print both existing and new items to console. \"\"\" if not isinstance ( dlc_config , dict ): dlc_config_fp = find_full_path ( get_dlc_root_data_dir (), Path ( dlc_config )) assert dlc_config_fp . exists () and dlc_config_fp . suffix in ( \".yml\" , \".yaml\" , ), f \"dlc_config is neither dict nor filepath \\n Check: { dlc_config_fp } \" if dlc_config_fp . suffix in ( \".yml\" , \".yaml\" ): with open ( dlc_config_fp , \"rb\" ) as f : dlc_config = yaml . safe_load ( f ) # -- Check and insert new BodyPart -- assert \"bodyparts\" in dlc_config , f \"Found no bodyparts section in { dlc_config } \" tracked_body_parts = cls . fetch ( \"body_part\" ) new_body_parts = np . setdiff1d ( dlc_config [ \"bodyparts\" ], tracked_body_parts ) if verbose : # Added to silence duplicate prompt during `insert_new_model` print ( f \"Existing body parts: { tracked_body_parts } \" ) print ( f \"New body parts: { new_body_parts } \" ) return new_body_parts @classmethod def insert_from_config ( cls , dlc_config : dict , descriptions : list = None , prompt = True ): \"\"\"Insert all body parts from a config file. Args: dlc_config (str or dict): path to a config.y*ml, or dict of such contents. descriptions (list): Optional. List of strings describing new body parts. prompt (bool): Optional, default True. Promp for confirmation before insert. \"\"\" # handle dlc_config being a yaml file new_body_parts = cls . extract_new_body_parts ( dlc_config , verbose = False ) if new_body_parts is not None : # Required bc np.array is ambiguous as bool if descriptions : assert len ( descriptions ) == len ( new_body_parts ), ( \"Descriptions list does not match \" + \" the number of new_body_parts\" ) print ( f \"New descriptions: { descriptions } \" ) if descriptions is None : descriptions = [ \"\" for x in range ( len ( new_body_parts ))] if ( prompt and dj . utils . user_choice ( f \"Insert { len ( new_body_parts ) } new body \" + \"part(s)?\" ) != \"yes\" ): print ( \"Canceled insert.\" ) return cls . insert ( [ { \"body_part\" : b , \"body_part_description\" : d } for b , d in zip ( new_body_parts , descriptions ) ] ) extract_new_body_parts ( dlc_config , verbose = True ) classmethod \u00b6 Returns list of body parts present in dlc config, but not BodyPart table. Parameters: Name Type Description Default dlc_config str or dict path to a config.y*ml, or dict of such contents. required verbose bool Default True. Print both existing and new items to console. True Source code in element_deeplabcut/model.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 @classmethod def extract_new_body_parts ( cls , dlc_config : dict , verbose : bool = True ): \"\"\"Returns list of body parts present in dlc config, but not BodyPart table. Args: dlc_config (str or dict): path to a config.y*ml, or dict of such contents. verbose (bool): Default True. Print both existing and new items to console. \"\"\" if not isinstance ( dlc_config , dict ): dlc_config_fp = find_full_path ( get_dlc_root_data_dir (), Path ( dlc_config )) assert dlc_config_fp . exists () and dlc_config_fp . suffix in ( \".yml\" , \".yaml\" , ), f \"dlc_config is neither dict nor filepath \\n Check: { dlc_config_fp } \" if dlc_config_fp . suffix in ( \".yml\" , \".yaml\" ): with open ( dlc_config_fp , \"rb\" ) as f : dlc_config = yaml . safe_load ( f ) # -- Check and insert new BodyPart -- assert \"bodyparts\" in dlc_config , f \"Found no bodyparts section in { dlc_config } \" tracked_body_parts = cls . fetch ( \"body_part\" ) new_body_parts = np . setdiff1d ( dlc_config [ \"bodyparts\" ], tracked_body_parts ) if verbose : # Added to silence duplicate prompt during `insert_new_model` print ( f \"Existing body parts: { tracked_body_parts } \" ) print ( f \"New body parts: { new_body_parts } \" ) return new_body_parts insert_from_config ( dlc_config , descriptions = None , prompt = True ) classmethod \u00b6 Insert all body parts from a config file. Parameters: Name Type Description Default dlc_config str or dict path to a config.y*ml, or dict of such contents. required descriptions list Optional. List of strings describing new body parts. None prompt bool Optional, default True. Promp for confirmation before insert. True Source code in element_deeplabcut/model.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 @classmethod def insert_from_config ( cls , dlc_config : dict , descriptions : list = None , prompt = True ): \"\"\"Insert all body parts from a config file. Args: dlc_config (str or dict): path to a config.y*ml, or dict of such contents. descriptions (list): Optional. List of strings describing new body parts. prompt (bool): Optional, default True. Promp for confirmation before insert. \"\"\" # handle dlc_config being a yaml file new_body_parts = cls . extract_new_body_parts ( dlc_config , verbose = False ) if new_body_parts is not None : # Required bc np.array is ambiguous as bool if descriptions : assert len ( descriptions ) == len ( new_body_parts ), ( \"Descriptions list does not match \" + \" the number of new_body_parts\" ) print ( f \"New descriptions: { descriptions } \" ) if descriptions is None : descriptions = [ \"\" for x in range ( len ( new_body_parts ))] if ( prompt and dj . utils . user_choice ( f \"Insert { len ( new_body_parts ) } new body \" + \"part(s)?\" ) != \"yes\" ): print ( \"Canceled insert.\" ) return cls . insert ( [ { \"body_part\" : b , \"body_part_description\" : d } for b , d in zip ( new_body_parts , descriptions ) ] ) Model \u00b6 Bases: dj . Manual DeepLabCut Models applied to generate pose estimations. Attributes: Name Type Description model_name varchar(64) User-friendly model name. task varchar(32) Task in the config yaml. date varchar(16) Date in the config yaml. iteration int Iteration/version of this model. snapshotindex int Which snapshot for prediction (if -1, latest). shuffle int Which shuffle of the training dataset. trainingsetindex int Which training set fraction to generate model. scorer varchar(64) Scorer/network name - DLC's GetScorerName(). config_template longblob Dictionary of the config for analyze_videos(). project_path varchar(255) DLC's project_path in config relative to root. model_prefix varchar(32) Optional. Prefix for model files. model_description varchar(1000) Optional. User-entered description. TrainingParamSet foreign key Optional. Training parameters primary key. Note Models are uniquely identified by the union of task, date, iteration, shuffle, snapshotindex, and trainingsetindex. Source code in element_deeplabcut/model.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 @schema class Model ( dj . Manual ): \"\"\"DeepLabCut Models applied to generate pose estimations. Attributes: model_name ( varchar(64) ): User-friendly model name. task ( varchar(32) ): Task in the config yaml. date ( varchar(16) ): Date in the config yaml. iteration (int): Iteration/version of this model. snapshotindex (int): Which snapshot for prediction (if -1, latest). shuffle (int): Which shuffle of the training dataset. trainingsetindex (int): Which training set fraction to generate model. scorer ( varchar(64) ): Scorer/network name - DLC's GetScorerName(). config_template (longblob): Dictionary of the config for analyze_videos(). project_path ( varchar(255) ): DLC's project_path in config relative to root. model_prefix ( varchar(32) ): Optional. Prefix for model files. model_description ( varchar(1000) ): Optional. User-entered description. TrainingParamSet (foreign key): Optional. Training parameters primary key. Note: Models are uniquely identified by the union of task, date, iteration, shuffle, snapshotindex, and trainingsetindex. \"\"\" definition = \"\"\" model_name : varchar(64) # User-friendly model name --- task : varchar(32) # Task in the config yaml date : varchar(16) # Date in the config yaml iteration : int # Iteration/version of this model snapshotindex : int # which snapshot for prediction (if -1, latest) shuffle : int # Shuffle (1) or not (0) trainingsetindex : int # Index of training fraction list in config.yaml unique index (task, date, iteration, shuffle, snapshotindex, trainingsetindex) scorer : varchar(64) # Scorer/network name - DLC's GetScorerName() config_template : longblob # Dictionary of the config for analyze_videos() project_path : varchar(255) # DLC's project_path in config relative to root model_prefix='' : varchar(32) model_description='' : varchar(300) -> [nullable] train.TrainingParamSet \"\"\" # project_path is the only item required downstream in the pose schema class BodyPart ( dj . Part ): \"\"\"Body parts associated with a given model Attributes: body_part ( varchar(32) ): Short name. Also called joint. body_part_description ( varchar(1000) ): Optional. Longer description.\"\"\" definition = \"\"\" -> master -> BodyPart \"\"\" @classmethod def insert_new_model ( cls , model_name : str , dlc_config , * , shuffle : int , trainingsetindex , project_path = None , model_description = \"\" , model_prefix = \"\" , paramset_idx : int = None , prompt = True , params = None , ): \"\"\"Insert new model into the dlc.Model table. Args: model_name (str): User-friendly name for this model. dlc_config (str or dict): path to a config.y*ml, or dict of such contents. shuffle (int): Shuffled or not as 1 or 0. trainingsetindex (int): Index of training fraction list in config.yaml. model_description (str): Optional. Description of this model. model_prefix (str): Optional. Filename prefix used across DLC project paramset_idx (int): Optional. Index from the TrainingParamSet table prompt (bool): Optional. Prompt the user with all info before inserting. params (dict): Optional. If dlc_config is path, dict of override items \"\"\" # handle dlc_config being a yaml file if not isinstance ( dlc_config , dict ): dlc_config_fp = find_full_path ( get_dlc_root_data_dir (), Path ( dlc_config )) assert dlc_config_fp . exists (), ( \"dlc_config is neither dict nor filepath\" + f \" \\n Check: { dlc_config_fp } \" ) if dlc_config_fp . suffix in ( \".yml\" , \".yaml\" ): with open ( dlc_config_fp , \"rb\" ) as f : dlc_config = yaml . safe_load ( f ) if isinstance ( params , dict ): dlc_config . update ( params ) # ---- Get and resolve project path ---- project_path = find_full_path ( get_dlc_root_data_dir (), dlc_config . get ( \"project_path\" , project_path ) ) dlc_config [ \"project_path\" ] = str ( project_path ) # update if different root_dir = find_root_directory ( get_dlc_root_data_dir (), project_path ) # ---- Verify config ---- needed_attributes = [ \"Task\" , \"date\" , \"iteration\" , \"snapshotindex\" , \"TrainingFraction\" , ] for attribute in needed_attributes : assert attribute in dlc_config , f \"Couldn't find { attribute } in config\" # ---- Get scorer name ---- # \"or 'f'\" below covers case where config returns None. str_to_bool handles else scorer_legacy = str_to_bool ( dlc_config . get ( \"scorer_legacy\" , \"f\" )) dlc_scorer = GetScorerName ( cfg = dlc_config , shuffle = shuffle , trainFraction = dlc_config [ \"TrainingFraction\" ][ int ( trainingsetindex )], modelprefix = model_prefix , )[ scorer_legacy ] if dlc_config [ \"snapshotindex\" ] == - 1 : dlc_scorer = \"\" . join ( dlc_scorer . split ( \"_\" )[: - 1 ]) # ---- Insert ---- model_dict = { \"model_name\" : model_name , \"model_description\" : model_description , \"scorer\" : dlc_scorer , \"task\" : dlc_config [ \"Task\" ], \"date\" : dlc_config [ \"date\" ], \"iteration\" : dlc_config [ \"iteration\" ], \"snapshotindex\" : dlc_config [ \"snapshotindex\" ], \"shuffle\" : shuffle , \"trainingsetindex\" : int ( trainingsetindex ), \"project_path\" : project_path . relative_to ( root_dir ) . as_posix (), \"paramset_idx\" : paramset_idx , \"config_template\" : dlc_config , } # -- prompt for confirmation -- if prompt : print ( \"--- DLC Model specification to be inserted ---\" ) for k , v in model_dict . items (): if k != \"config_template\" : print ( \" \\t {} : {} \" . format ( k , v )) else : print ( \" \\t -- Template/Contents of config.yaml --\" ) for k , v in model_dict [ \"config_template\" ] . items (): print ( \" \\t\\t {} : {} \" . format ( k , v )) if ( prompt and dj . utils . user_choice ( \"Proceed with new DLC model insert?\" ) != \"yes\" ): print ( \"Canceled insert.\" ) return # ---- Save DJ-managed config ---- _ = dlc_reader . save_yaml ( project_path , dlc_config ) # ____ Insert into table ---- with cls . connection . transaction : cls . insert1 ( model_dict ) # Returns array, so check size for unambiguous truth value if BodyPart . extract_new_body_parts ( dlc_config , verbose = False ) . size > 0 : BodyPart . insert_from_config ( dlc_config , prompt = prompt ) cls . BodyPart . insert (( model_name , bp ) for bp in dlc_config [ \"bodyparts\" ]) BodyPart \u00b6 Bases: dj . Part Body parts associated with a given model Attributes: Name Type Description body_part varchar(32) Short name. Also called joint. body_part_description varchar(1000) Optional. Longer description. Source code in element_deeplabcut/model.py 337 338 339 340 341 342 343 344 345 346 347 class BodyPart ( dj . Part ): \"\"\"Body parts associated with a given model Attributes: body_part ( varchar(32) ): Short name. Also called joint. body_part_description ( varchar(1000) ): Optional. Longer description.\"\"\" definition = \"\"\" -> master -> BodyPart \"\"\" insert_new_model ( model_name , dlc_config , * , shuffle , trainingsetindex , project_path = None , model_description = '' , model_prefix = '' , paramset_idx = None , prompt = True , params = None ) classmethod \u00b6 Insert new model into the dlc.Model table. Parameters: Name Type Description Default model_name str User-friendly name for this model. required dlc_config str or dict path to a config.y*ml, or dict of such contents. required shuffle int Shuffled or not as 1 or 0. required trainingsetindex int Index of training fraction list in config.yaml. required model_description str Optional. Description of this model. '' model_prefix str Optional. Filename prefix used across DLC project '' paramset_idx int Optional. Index from the TrainingParamSet table None prompt bool Optional. Prompt the user with all info before inserting. True params dict Optional. If dlc_config is path, dict of override items None Source code in element_deeplabcut/model.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 @classmethod def insert_new_model ( cls , model_name : str , dlc_config , * , shuffle : int , trainingsetindex , project_path = None , model_description = \"\" , model_prefix = \"\" , paramset_idx : int = None , prompt = True , params = None , ): \"\"\"Insert new model into the dlc.Model table. Args: model_name (str): User-friendly name for this model. dlc_config (str or dict): path to a config.y*ml, or dict of such contents. shuffle (int): Shuffled or not as 1 or 0. trainingsetindex (int): Index of training fraction list in config.yaml. model_description (str): Optional. Description of this model. model_prefix (str): Optional. Filename prefix used across DLC project paramset_idx (int): Optional. Index from the TrainingParamSet table prompt (bool): Optional. Prompt the user with all info before inserting. params (dict): Optional. If dlc_config is path, dict of override items \"\"\" # handle dlc_config being a yaml file if not isinstance ( dlc_config , dict ): dlc_config_fp = find_full_path ( get_dlc_root_data_dir (), Path ( dlc_config )) assert dlc_config_fp . exists (), ( \"dlc_config is neither dict nor filepath\" + f \" \\n Check: { dlc_config_fp } \" ) if dlc_config_fp . suffix in ( \".yml\" , \".yaml\" ): with open ( dlc_config_fp , \"rb\" ) as f : dlc_config = yaml . safe_load ( f ) if isinstance ( params , dict ): dlc_config . update ( params ) # ---- Get and resolve project path ---- project_path = find_full_path ( get_dlc_root_data_dir (), dlc_config . get ( \"project_path\" , project_path ) ) dlc_config [ \"project_path\" ] = str ( project_path ) # update if different root_dir = find_root_directory ( get_dlc_root_data_dir (), project_path ) # ---- Verify config ---- needed_attributes = [ \"Task\" , \"date\" , \"iteration\" , \"snapshotindex\" , \"TrainingFraction\" , ] for attribute in needed_attributes : assert attribute in dlc_config , f \"Couldn't find { attribute } in config\" # ---- Get scorer name ---- # \"or 'f'\" below covers case where config returns None. str_to_bool handles else scorer_legacy = str_to_bool ( dlc_config . get ( \"scorer_legacy\" , \"f\" )) dlc_scorer = GetScorerName ( cfg = dlc_config , shuffle = shuffle , trainFraction = dlc_config [ \"TrainingFraction\" ][ int ( trainingsetindex )], modelprefix = model_prefix , )[ scorer_legacy ] if dlc_config [ \"snapshotindex\" ] == - 1 : dlc_scorer = \"\" . join ( dlc_scorer . split ( \"_\" )[: - 1 ]) # ---- Insert ---- model_dict = { \"model_name\" : model_name , \"model_description\" : model_description , \"scorer\" : dlc_scorer , \"task\" : dlc_config [ \"Task\" ], \"date\" : dlc_config [ \"date\" ], \"iteration\" : dlc_config [ \"iteration\" ], \"snapshotindex\" : dlc_config [ \"snapshotindex\" ], \"shuffle\" : shuffle , \"trainingsetindex\" : int ( trainingsetindex ), \"project_path\" : project_path . relative_to ( root_dir ) . as_posix (), \"paramset_idx\" : paramset_idx , \"config_template\" : dlc_config , } # -- prompt for confirmation -- if prompt : print ( \"--- DLC Model specification to be inserted ---\" ) for k , v in model_dict . items (): if k != \"config_template\" : print ( \" \\t {} : {} \" . format ( k , v )) else : print ( \" \\t -- Template/Contents of config.yaml --\" ) for k , v in model_dict [ \"config_template\" ] . items (): print ( \" \\t\\t {} : {} \" . format ( k , v )) if ( prompt and dj . utils . user_choice ( \"Proceed with new DLC model insert?\" ) != \"yes\" ): print ( \"Canceled insert.\" ) return # ---- Save DJ-managed config ---- _ = dlc_reader . save_yaml ( project_path , dlc_config ) # ____ Insert into table ---- with cls . connection . transaction : cls . insert1 ( model_dict ) # Returns array, so check size for unambiguous truth value if BodyPart . extract_new_body_parts ( dlc_config , verbose = False ) . size > 0 : BodyPart . insert_from_config ( dlc_config , prompt = prompt ) cls . BodyPart . insert (( model_name , bp ) for bp in dlc_config [ \"bodyparts\" ]) ModelEvaluation \u00b6 Bases: dj . Computed Performance characteristics model calculated by deeplabcut.evaluate_network Attributes: Name Type Description Model foreign key Model name. train_iterations int Training iterations. train_error float Optional. Train error (px). test_error float Optional. Test error (px). p_cutoff float Optional. p-cutoff used. train_error_p float Optional. Train error with p-cutoff. test_error_p float Optional. Test error with p-cutoff. Source code in element_deeplabcut/model.py 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 @schema class ModelEvaluation ( dj . Computed ): \"\"\"Performance characteristics model calculated by `deeplabcut.evaluate_network` Attributes: Model (foreign key): Model name. train_iterations (int): Training iterations. train_error (float): Optional. Train error (px). test_error (float): Optional. Test error (px). p_cutoff (float): Optional. p-cutoff used. train_error_p (float): Optional. Train error with p-cutoff. test_error_p (float): Optional. Test error with p-cutoff.\"\"\" definition = \"\"\" -> Model --- train_iterations : int # Training iterations train_error=null : float # Train error (px) test_error=null : float # Test error (px) p_cutoff=null : float # p-cutoff used train_error_p=null : float # Train error with p-cutoff test_error_p=null : float # Test error with p-cutoff \"\"\" def make ( self , key ): \"\"\".populate() method will launch evaulation for each unique entry in Model.\"\"\" dlc_config , project_path , model_prefix , shuffle , trainingsetindex = ( Model & key ) . fetch1 ( \"config_template\" , \"project_path\" , \"model_prefix\" , \"shuffle\" , \"trainingsetindex\" , ) project_path = find_full_path ( get_dlc_root_data_dir (), project_path ) yml_path , _ = dlc_reader . read_yaml ( project_path ) evaluate_network ( yml_path , Shuffles = [ shuffle ], # this needs to be a list trainingsetindex = trainingsetindex , comparisonbodyparts = \"all\" , ) eval_folder = get_evaluation_folder ( trainFraction = dlc_config [ \"TrainingFraction\" ][ trainingsetindex ], shuffle = shuffle , cfg = dlc_config , modelprefix = model_prefix , ) eval_path = project_path / eval_folder assert eval_path . exists (), f \"Couldn't find evaluation folder: \\n { eval_path } \" eval_csvs = list ( eval_path . glob ( \"*csv\" )) max_modified_time = 0 for eval_csv in eval_csvs : modified_time = os . path . getmtime ( eval_csv ) if modified_time > max_modified_time : eval_csv_latest = eval_csv with open ( eval_csv_latest , newline = \"\" ) as f : results = list ( csv . DictReader ( f , delimiter = \",\" ))[ 0 ] # in testing, test_error_p returned empty string self . insert1 ( dict ( key , train_iterations = results [ \"Training iterations:\" ], train_error = results [ \" Train error(px)\" ], test_error = results [ \" Test error(px)\" ], p_cutoff = results [ \"p-cutoff used\" ], train_error_p = results [ \"Train error with p-cutoff\" ], test_error_p = results [ \"Test error with p-cutoff\" ], ) ) make ( key ) \u00b6 .populate() method will launch evaulation for each unique entry in Model. Source code in element_deeplabcut/model.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 def make ( self , key ): \"\"\".populate() method will launch evaulation for each unique entry in Model.\"\"\" dlc_config , project_path , model_prefix , shuffle , trainingsetindex = ( Model & key ) . fetch1 ( \"config_template\" , \"project_path\" , \"model_prefix\" , \"shuffle\" , \"trainingsetindex\" , ) project_path = find_full_path ( get_dlc_root_data_dir (), project_path ) yml_path , _ = dlc_reader . read_yaml ( project_path ) evaluate_network ( yml_path , Shuffles = [ shuffle ], # this needs to be a list trainingsetindex = trainingsetindex , comparisonbodyparts = \"all\" , ) eval_folder = get_evaluation_folder ( trainFraction = dlc_config [ \"TrainingFraction\" ][ trainingsetindex ], shuffle = shuffle , cfg = dlc_config , modelprefix = model_prefix , ) eval_path = project_path / eval_folder assert eval_path . exists (), f \"Couldn't find evaluation folder: \\n { eval_path } \" eval_csvs = list ( eval_path . glob ( \"*csv\" )) max_modified_time = 0 for eval_csv in eval_csvs : modified_time = os . path . getmtime ( eval_csv ) if modified_time > max_modified_time : eval_csv_latest = eval_csv with open ( eval_csv_latest , newline = \"\" ) as f : results = list ( csv . DictReader ( f , delimiter = \",\" ))[ 0 ] # in testing, test_error_p returned empty string self . insert1 ( dict ( key , train_iterations = results [ \"Training iterations:\" ], train_error = results [ \" Train error(px)\" ], test_error = results [ \" Test error(px)\" ], p_cutoff = results [ \"p-cutoff used\" ], train_error_p = results [ \"Train error with p-cutoff\" ], test_error_p = results [ \"Test error with p-cutoff\" ], ) ) PoseEstimationTask \u00b6 Bases: dj . Manual Staging table for pairing of video recording and model before inference. Attributes: Name Type Description VideoRecording foreign key Video recording key. Model foreign key Model name. task_mode load or trigger Optional. Default load. Or trigger computation. pose_estimation_output_dir varchar(255) Optional. Output dir relative to get_dlc_root_data_dir. pose_estimation_params longblob Optional. Params for DLC's analyze_videos params, if not default. Source code in element_deeplabcut/model.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 @schema class PoseEstimationTask ( dj . Manual ): \"\"\"Staging table for pairing of video recording and model before inference. Attributes: VideoRecording (foreign key): Video recording key. Model (foreign key): Model name. task_mode (load or trigger): Optional. Default load. Or trigger computation. pose_estimation_output_dir ( varchar(255) ): Optional. Output dir relative to get_dlc_root_data_dir. pose_estimation_params (longblob): Optional. Params for DLC's analyze_videos params, if not default.\"\"\" definition = \"\"\" -> VideoRecording # Session -> Recording + File part table -> Model # Must specify a DLC project_path --- task_mode='load' : enum('load', 'trigger') # load results or trigger computation pose_estimation_output_dir='': varchar(255) # output dir relative to the root dir pose_estimation_params=null : longblob # analyze_videos params, if not default \"\"\" @classmethod def infer_output_dir ( cls , key : dict , relative : bool = False , mkdir : bool = False ): \"\"\"Return the expected pose_estimation_output_dir. Spaces in model name are replaced with hyphens. Based on convention: / video_dir / Device_{}_Recording_{}_Model_{} Args: key: DataJoint key specifying a pairing of VideoRecording and Model. relative (bool): Report directory relative to get_dlc_processed_data_dir(). mkdir (bool): Default False. Make directory if it doesn't exist. \"\"\" video_filepath = find_full_path ( get_dlc_root_data_dir (), ( VideoRecording . File & key ) . fetch ( \"file_path\" , limit = 1 )[ 0 ], ) root_dir = find_root_directory ( get_dlc_root_data_dir (), video_filepath . parent ) recording_key = VideoRecording & key device = \"-\" . join ( str ( v ) for v in ( _linking_module . Device & recording_key ) . fetch1 ( \"KEY\" ) . values () ) if get_dlc_processed_data_dir (): processed_dir = Path ( get_dlc_processed_data_dir ()) else : # if processed not provided, default to where video is processed_dir = root_dir output_dir = ( processed_dir / video_filepath . parent . relative_to ( root_dir ) / ( f 'device_ { device } _recording_ { key [ \"recording_id\" ] } _model_' + key [ \"model_name\" ] . replace ( \" \" , \"-\" ) ) ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def insert_estimation_task ( cls , key : dict , task_mode : str = \"trigger\" , params : dict = None , relative : bool = True , mkdir : bool = True , skip_duplicates : bool = False , ): \"\"\"Insert PoseEstimationTask in inferred output dir. Based on the convention / video_dir / device_{}_recording_{}_model_{} Args: key: DataJoint key specifying a pairing of VideoRecording and Model. task_mode (bool): Default 'trigger' computation. Or 'load' existing results. params (dict): Optional. Parameters passed to DLC's analyze_videos: videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve relative (bool): Report directory relative to get_dlc_processed_data_dir(). mkdir (bool): Default False. Make directory if it doesn't exist. \"\"\" output_dir = cls . infer_output_dir ( key , relative = relative , mkdir = mkdir ) cls . insert1 ( { ** key , \"task_mode\" : task_mode , \"pose_estimation_params\" : params , \"pose_estimation_output_dir\" : output_dir , }, skip_duplicates = skip_duplicates , ) infer_output_dir ( key , relative = False , mkdir = False ) classmethod \u00b6 Return the expected pose_estimation_output_dir. Spaces in model name are replaced with hyphens. Based on convention: / video_dir / Device_{} Recording {} Model {} Parameters: Name Type Description Default key dict DataJoint key specifying a pairing of VideoRecording and Model. required relative bool Report directory relative to get_dlc_processed_data_dir(). False mkdir bool Default False. Make directory if it doesn't exist. False Source code in element_deeplabcut/model.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 @classmethod def infer_output_dir ( cls , key : dict , relative : bool = False , mkdir : bool = False ): \"\"\"Return the expected pose_estimation_output_dir. Spaces in model name are replaced with hyphens. Based on convention: / video_dir / Device_{}_Recording_{}_Model_{} Args: key: DataJoint key specifying a pairing of VideoRecording and Model. relative (bool): Report directory relative to get_dlc_processed_data_dir(). mkdir (bool): Default False. Make directory if it doesn't exist. \"\"\" video_filepath = find_full_path ( get_dlc_root_data_dir (), ( VideoRecording . File & key ) . fetch ( \"file_path\" , limit = 1 )[ 0 ], ) root_dir = find_root_directory ( get_dlc_root_data_dir (), video_filepath . parent ) recording_key = VideoRecording & key device = \"-\" . join ( str ( v ) for v in ( _linking_module . Device & recording_key ) . fetch1 ( \"KEY\" ) . values () ) if get_dlc_processed_data_dir (): processed_dir = Path ( get_dlc_processed_data_dir ()) else : # if processed not provided, default to where video is processed_dir = root_dir output_dir = ( processed_dir / video_filepath . parent . relative_to ( root_dir ) / ( f 'device_ { device } _recording_ { key [ \"recording_id\" ] } _model_' + key [ \"model_name\" ] . replace ( \" \" , \"-\" ) ) ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir insert_estimation_task ( key , task_mode = 'trigger' , params = None , relative = True , mkdir = True , skip_duplicates = False ) classmethod \u00b6 Insert PoseEstimationTask in inferred output dir. Based on the convention / video_dir / device_{} recording {} model {} Parameters: Name Type Description Default key dict DataJoint key specifying a pairing of VideoRecording and Model. required task_mode bool Default 'trigger' computation. Or 'load' existing results. 'trigger' params dict Optional. Parameters passed to DLC's analyze_videos: videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve None relative bool Report directory relative to get_dlc_processed_data_dir(). True mkdir bool Default False. Make directory if it doesn't exist. True Source code in element_deeplabcut/model.py 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 @classmethod def insert_estimation_task ( cls , key : dict , task_mode : str = \"trigger\" , params : dict = None , relative : bool = True , mkdir : bool = True , skip_duplicates : bool = False , ): \"\"\"Insert PoseEstimationTask in inferred output dir. Based on the convention / video_dir / device_{}_recording_{}_model_{} Args: key: DataJoint key specifying a pairing of VideoRecording and Model. task_mode (bool): Default 'trigger' computation. Or 'load' existing results. params (dict): Optional. Parameters passed to DLC's analyze_videos: videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve relative (bool): Report directory relative to get_dlc_processed_data_dir(). mkdir (bool): Default False. Make directory if it doesn't exist. \"\"\" output_dir = cls . infer_output_dir ( key , relative = relative , mkdir = mkdir ) cls . insert1 ( { ** key , \"task_mode\" : task_mode , \"pose_estimation_params\" : params , \"pose_estimation_output_dir\" : output_dir , }, skip_duplicates = skip_duplicates , ) PoseEstimation \u00b6 Bases: dj . Computed Results of pose estimation. Attributes: Name Type Description PoseEstimationTask foreign key Pose Estimation Task key. post_estimation_time datetime time of generation of this set of DLC results. Source code in element_deeplabcut/model.py 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 @schema class PoseEstimation ( dj . Computed ): \"\"\"Results of pose estimation. Attributes: PoseEstimationTask (foreign key): Pose Estimation Task key. post_estimation_time (datetime): time of generation of this set of DLC results. \"\"\" definition = \"\"\" -> PoseEstimationTask --- pose_estimation_time: datetime # time of generation of this set of DLC results \"\"\" class BodyPartPosition ( dj . Part ): \"\"\"Position of individual body parts by frame index Attributes: PoseEstimation (foreign key): Pose Estimation key. Model.BodyPart (foreign key): Body Part key. frame_index (longblob): Frame index in model. x_pos (longblob): X position. y_pos (longblob): Y position. z_pos (longblob): Optional. Z position. likelihood (longblob): Model confidence.\"\"\" definition = \"\"\" # uses DeepLabCut h5 output for body part position -> master -> Model.BodyPart --- frame_index : longblob # frame index in model x_pos : longblob y_pos : longblob z_pos=null : longblob likelihood : longblob \"\"\" def make ( self , key ): \"\"\".populate() method will launch training for each PoseEstimationTask\"\"\" # ID model and directories dlc_model = ( Model & key ) . fetch1 () task_mode , analyze_video_params , output_dir = ( PoseEstimationTask & key ) . fetch1 ( \"task_mode\" , \"pose_estimation_params\" , \"pose_estimation_output_dir\" ) analyze_video_params = analyze_video_params or {} output_dir = find_full_path ( get_dlc_root_data_dir (), output_dir ) video_filepaths = [ find_full_path ( get_dlc_root_data_dir (), fp ) . as_posix () for fp in ( VideoRecording . File & key ) . fetch ( \"file_path\" ) ] project_path = find_full_path ( get_dlc_root_data_dir (), dlc_model [ \"project_path\" ] ) # Triger PoseEstimation if task_mode == \"trigger\" : dlc_reader . do_pose_estimation ( video_filepaths , dlc_model , project_path , output_dir , ** analyze_video_params , ) dlc_result = dlc_reader . PoseEstimation ( output_dir ) creation_time = datetime . fromtimestamp ( dlc_result . creation_time ) . strftime ( \"%Y-%m- %d %H:%M:%S\" ) body_parts = [ { ** key , \"body_part\" : k , \"frame_index\" : np . arange ( dlc_result . nframes ), \"x_pos\" : v [ \"x\" ], \"y_pos\" : v [ \"y\" ], \"z_pos\" : v . get ( \"z\" ), \"likelihood\" : v [ \"likelihood\" ], } for k , v in dlc_result . data . items () ] self . insert1 ({ ** key , \"pose_estimation_time\" : creation_time }) self . BodyPartPosition . insert ( body_parts ) @classmethod def get_trajectory ( cls , key : dict , body_parts : list = \"all\" ) -> pd . DataFrame : \"\"\"Returns a pandas dataframe of coordinates of the specified body_part(s) Args: key (dict): A DataJoint query specifying one PoseEstimation entry. body_parts (list, optional): Body parts as a list. If \"all\", all joints Returns: df: multi index pandas dataframe with DLC scorer names, body_parts and x/y coordinates of each joint name for a camera_id, similar to output of DLC dataframe. If 2D, z is set of zeros \"\"\" model_name = key [ \"model_name\" ] if body_parts == \"all\" : body_parts = ( cls . BodyPartPosition & key ) . fetch ( \"body_part\" ) elif not isinstance ( body_parts , list ): body_parts = list ( body_parts ) df = None for body_part in body_parts : x_pos , y_pos , z_pos , likelihood = ( cls . BodyPartPosition & { \"body_part\" : body_part } ) . fetch1 ( \"x_pos\" , \"y_pos\" , \"z_pos\" , \"likelihood\" ) if not z_pos : z_pos = np . zeros_like ( x_pos ) a = np . vstack (( x_pos , y_pos , z_pos , likelihood )) a = a . T pdindex = pd . MultiIndex . from_product ( [[ model_name ], [ body_part ], [ \"x\" , \"y\" , \"z\" , \"likelihood\" ]], names = [ \"scorer\" , \"bodyparts\" , \"coords\" ], ) frame = pd . DataFrame ( a , columns = pdindex , index = range ( 0 , a . shape [ 0 ])) df = pd . concat ([ df , frame ], axis = 1 ) return df BodyPartPosition \u00b6 Bases: dj . Part Position of individual body parts by frame index Attributes: Name Type Description PoseEstimation foreign key Pose Estimation key. Model.BodyPart foreign key Body Part key. frame_index longblob Frame index in model. x_pos longblob X position. y_pos longblob Y position. z_pos longblob Optional. Z position. likelihood longblob Model confidence. Source code in element_deeplabcut/model.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 class BodyPartPosition ( dj . Part ): \"\"\"Position of individual body parts by frame index Attributes: PoseEstimation (foreign key): Pose Estimation key. Model.BodyPart (foreign key): Body Part key. frame_index (longblob): Frame index in model. x_pos (longblob): X position. y_pos (longblob): Y position. z_pos (longblob): Optional. Z position. likelihood (longblob): Model confidence.\"\"\" definition = \"\"\" # uses DeepLabCut h5 output for body part position -> master -> Model.BodyPart --- frame_index : longblob # frame index in model x_pos : longblob y_pos : longblob z_pos=null : longblob likelihood : longblob \"\"\" make ( key ) \u00b6 .populate() method will launch training for each PoseEstimationTask Source code in element_deeplabcut/model.py 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 def make ( self , key ): \"\"\".populate() method will launch training for each PoseEstimationTask\"\"\" # ID model and directories dlc_model = ( Model & key ) . fetch1 () task_mode , analyze_video_params , output_dir = ( PoseEstimationTask & key ) . fetch1 ( \"task_mode\" , \"pose_estimation_params\" , \"pose_estimation_output_dir\" ) analyze_video_params = analyze_video_params or {} output_dir = find_full_path ( get_dlc_root_data_dir (), output_dir ) video_filepaths = [ find_full_path ( get_dlc_root_data_dir (), fp ) . as_posix () for fp in ( VideoRecording . File & key ) . fetch ( \"file_path\" ) ] project_path = find_full_path ( get_dlc_root_data_dir (), dlc_model [ \"project_path\" ] ) # Triger PoseEstimation if task_mode == \"trigger\" : dlc_reader . do_pose_estimation ( video_filepaths , dlc_model , project_path , output_dir , ** analyze_video_params , ) dlc_result = dlc_reader . PoseEstimation ( output_dir ) creation_time = datetime . fromtimestamp ( dlc_result . creation_time ) . strftime ( \"%Y-%m- %d %H:%M:%S\" ) body_parts = [ { ** key , \"body_part\" : k , \"frame_index\" : np . arange ( dlc_result . nframes ), \"x_pos\" : v [ \"x\" ], \"y_pos\" : v [ \"y\" ], \"z_pos\" : v . get ( \"z\" ), \"likelihood\" : v [ \"likelihood\" ], } for k , v in dlc_result . data . items () ] self . insert1 ({ ** key , \"pose_estimation_time\" : creation_time }) self . BodyPartPosition . insert ( body_parts ) get_trajectory ( key , body_parts = 'all' ) classmethod \u00b6 Returns a pandas dataframe of coordinates of the specified body_part(s) Parameters: Name Type Description Default key dict A DataJoint query specifying one PoseEstimation entry. required body_parts list Body parts as a list. If \"all\", all joints 'all' Returns: Name Type Description df pd . DataFrame multi index pandas dataframe with DLC scorer names, body_parts and x/y coordinates of each joint name for a camera_id, similar to output of DLC dataframe. If 2D, z is set of zeros Source code in element_deeplabcut/model.py 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 @classmethod def get_trajectory ( cls , key : dict , body_parts : list = \"all\" ) -> pd . DataFrame : \"\"\"Returns a pandas dataframe of coordinates of the specified body_part(s) Args: key (dict): A DataJoint query specifying one PoseEstimation entry. body_parts (list, optional): Body parts as a list. If \"all\", all joints Returns: df: multi index pandas dataframe with DLC scorer names, body_parts and x/y coordinates of each joint name for a camera_id, similar to output of DLC dataframe. If 2D, z is set of zeros \"\"\" model_name = key [ \"model_name\" ] if body_parts == \"all\" : body_parts = ( cls . BodyPartPosition & key ) . fetch ( \"body_part\" ) elif not isinstance ( body_parts , list ): body_parts = list ( body_parts ) df = None for body_part in body_parts : x_pos , y_pos , z_pos , likelihood = ( cls . BodyPartPosition & { \"body_part\" : body_part } ) . fetch1 ( \"x_pos\" , \"y_pos\" , \"z_pos\" , \"likelihood\" ) if not z_pos : z_pos = np . zeros_like ( x_pos ) a = np . vstack (( x_pos , y_pos , z_pos , likelihood )) a = a . T pdindex = pd . MultiIndex . from_product ( [[ model_name ], [ body_part ], [ \"x\" , \"y\" , \"z\" , \"likelihood\" ]], names = [ \"scorer\" , \"bodyparts\" , \"coords\" ], ) frame = pd . DataFrame ( a , columns = pdindex , index = range ( 0 , a . shape [ 0 ])) df = pd . concat ([ df , frame ], axis = 1 ) return df str_to_bool ( value ) \u00b6 Return whether the provided string represents true. Otherwise false. Parameters: Name Type Description Default value any Any input required Returns: Name Type Description bool bool True if value in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\") Source code in element_deeplabcut/model.py 765 766 767 768 769 770 771 772 773 774 775 776 777 778 def str_to_bool ( value ) -> bool : \"\"\"Return whether the provided string represents true. Otherwise false. Args: value (any): Any input Returns: bool (bool): True if value in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\") \"\"\" # Due to distutils equivalent depreciation in 3.10 # Adopted from github.com/PostHog/posthog/blob/master/posthog/utils.py if not value : return False return str ( value ) . lower () in ( \"y\" , \"yes\" , \"t\" , \"true\" , \"on\" , \"1\" )", "title": "model.py"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.activate", "text": "Activate this schema. Parameters: Name Type Description Default model_schema_name str schema name on the database server required create_schema bool when True (default), create schema in the database if it does not yet exist. True create_tables str when True (default), create schema tables in the database if they do not yet exist. True linking_module str a module (or name) containing the required dependencies. None Dependencies: Upstream tables Session: A parent table to VideoRecording, identifying a recording session. Equipment: A parent table to VideoRecording, identifying a recording device. Functions get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder. Source code in element_deeplabcut/model.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def activate ( model_schema_name : str , * , create_schema : bool = True , create_tables : bool = True , linking_module : bool = None , ): \"\"\"Activate this schema. Args: model_schema_name (str): schema name on the database server create_schema (bool): when True (default), create schema in the database if it does not yet exist. create_tables (str): when True (default), create schema tables in the database if they do not yet exist. linking_module (str): a module (or name) containing the required dependencies. Dependencies: Upstream tables: Session: A parent table to VideoRecording, identifying a recording session. Equipment: A parent table to VideoRecording, identifying a recording device. Functions: get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" assert hasattr ( linking_module , \"get_dlc_root_data_dir\" ), \"The linking module must specify a lookup funtion for a root data directory\" global _linking_module _linking_module = linking_module # activate schema . activate ( model_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , )", "title": "activate()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.get_dlc_root_data_dir", "text": "Pulls relevant func from parent namespace to specify root data dir(s). It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strongs for possible root data directories. Source code in element_deeplabcut/model.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_dlc_root_data_dir () -> list : \"\"\"Pulls relevant func from parent namespace to specify root data dir(s). It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strongs for possible root data directories. \"\"\" root_directories = _linking_module . get_dlc_root_data_dir () if isinstance ( root_directories , ( str , Path )): root_directories = [ root_directories ] if ( hasattr ( _linking_module , \"get_dlc_processed_data_dir\" ) and get_dlc_processed_data_dir () not in root_directories ): root_directories . append ( _linking_module . get_dlc_processed_data_dir ()) return root_directories", "title": "get_dlc_root_data_dir()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.get_dlc_processed_data_dir", "text": "Pulls relevant func from parent namespace. Defaults to DLC's project /videos/. Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default. Source code in element_deeplabcut/model.py 100 101 102 103 104 105 106 107 108 109 110 def get_dlc_processed_data_dir () -> Optional [ str ]: \"\"\"Pulls relevant func from parent namespace. Defaults to DLC's project /videos/. Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default. \"\"\" if hasattr ( _linking_module , \"get_dlc_processed_data_dir\" ): return _linking_module . get_dlc_processed_data_dir () else : return None", "title": "get_dlc_processed_data_dir()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.VideoRecording", "text": "Bases: dj . Manual Set of video recordings for DLC inferences. Attributes: Name Type Description Session foreign key Session primary key. Equipment foreign key Equipment primary key, used for default output directory path information. recording_id int Unique recording ID. recording_start_time datetime Recording start time. Source code in element_deeplabcut/model.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 @schema class VideoRecording ( dj . Manual ): \"\"\"Set of video recordings for DLC inferences. Attributes: Session (foreign key): Session primary key. Equipment (foreign key): Equipment primary key, used for default output directory path information. recording_id (int): Unique recording ID. recording_start_time (datetime): Recording start time.\"\"\" definition = \"\"\" -> Session recording_id: int --- -> Device \"\"\" class File ( dj . Part ): \"\"\"File IDs and paths associated with a given recording_id Attributes: VideoRecording (foreign key): Video recording primary key. file_path ( varchar(255) ): file path of video, relative to root data dir. \"\"\" definition = \"\"\" -> master file_id: int --- file_path: varchar(255) # filepath of video, relative to root data directory \"\"\"", "title": "VideoRecording"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.VideoRecording.File", "text": "Bases: dj . Part File IDs and paths associated with a given recording_id Attributes: Name Type Description VideoRecording foreign key Video recording primary key. file_path varchar(255) file path of video, relative to root data dir. Source code in element_deeplabcut/model.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class File ( dj . Part ): \"\"\"File IDs and paths associated with a given recording_id Attributes: VideoRecording (foreign key): Video recording primary key. file_path ( varchar(255) ): file path of video, relative to root data dir. \"\"\" definition = \"\"\" -> master file_id: int --- file_path: varchar(255) # filepath of video, relative to root data directory \"\"\"", "title": "File"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.RecordingInfo", "text": "Bases: dj . Imported Automated table with video file metadata. Attributes: Name Type Description VideoRecording foreign key Video recording key. px_height smallint Height in pixels. px_width smallint Width in pixels. nframes smallint Number of frames. fps int Optional. Frames per second, Hz. recording_datetime datetime Optional. Datetime for the start of recording. recording_duration float video duration (s) from nframes / fps. Source code in element_deeplabcut/model.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 @schema class RecordingInfo ( dj . Imported ): \"\"\"Automated table with video file metadata. Attributes: VideoRecording (foreign key): Video recording key. px_height (smallint): Height in pixels. px_width (smallint): Width in pixels. nframes (smallint): Number of frames. fps (int): Optional. Frames per second, Hz. recording_datetime (datetime): Optional. Datetime for the start of recording. recording_duration (float): video duration (s) from nframes / fps.\"\"\" definition = \"\"\" -> VideoRecording --- px_height : smallint # height in pixels px_width : smallint # width in pixels nframes : smallint # number of frames fps = NULL : int # (Hz) frames per second recording_datetime = NULL : datetime # Datetime for the start of the recording recording_duration : float # video duration (s) from nframes / fps \"\"\" @property def key_source ( self ): \"\"\"Defines order of keys for make function when called via `populate()`\"\"\" return VideoRecording & VideoRecording . File def make ( self , key ): \"\"\"Populates table with video metadata using CV2.\"\"\" file_paths = ( VideoRecording . File & key ) . fetch ( \"file_path\" ) nframes = 0 px_height , px_width , fps = None , None , None for file_path in file_paths : file_path = ( find_full_path ( get_dlc_root_data_dir (), file_path )) . as_posix () cap = cv2 . VideoCapture ( file_path ) info = ( int ( cap . get ( cv2 . CAP_PROP_FRAME_HEIGHT )), int ( cap . get ( cv2 . CAP_PROP_FRAME_WIDTH )), int ( cap . get ( cv2 . CAP_PROP_FPS )), ) if px_height is not None : assert ( px_height , px_width , fps ) == info px_height , px_width , fps = info nframes += int ( cap . get ( cv2 . CAP_PROP_FRAME_COUNT )) cap . release () self . insert1 ( { ** key , \"px_height\" : px_height , \"px_width\" : px_width , \"nframes\" : nframes , \"fps\" : fps , \"recording_duration\" : nframes / fps , } )", "title": "RecordingInfo"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.RecordingInfo.key_source", "text": "Defines order of keys for make function when called via populate() Source code in element_deeplabcut/model.py 174 175 176 177 @property def key_source ( self ): \"\"\"Defines order of keys for make function when called via `populate()`\"\"\" return VideoRecording & VideoRecording . File", "title": "key_source()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.RecordingInfo.make", "text": "Populates table with video metadata using CV2. Source code in element_deeplabcut/model.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def make ( self , key ): \"\"\"Populates table with video metadata using CV2.\"\"\" file_paths = ( VideoRecording . File & key ) . fetch ( \"file_path\" ) nframes = 0 px_height , px_width , fps = None , None , None for file_path in file_paths : file_path = ( find_full_path ( get_dlc_root_data_dir (), file_path )) . as_posix () cap = cv2 . VideoCapture ( file_path ) info = ( int ( cap . get ( cv2 . CAP_PROP_FRAME_HEIGHT )), int ( cap . get ( cv2 . CAP_PROP_FRAME_WIDTH )), int ( cap . get ( cv2 . CAP_PROP_FPS )), ) if px_height is not None : assert ( px_height , px_width , fps ) == info px_height , px_width , fps = info nframes += int ( cap . get ( cv2 . CAP_PROP_FRAME_COUNT )) cap . release () self . insert1 ( { ** key , \"px_height\" : px_height , \"px_width\" : px_width , \"nframes\" : nframes , \"fps\" : fps , \"recording_duration\" : nframes / fps , } )", "title": "make()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.BodyPart", "text": "Bases: dj . Lookup Body parts tracked by DeepLabCut models Attributes: Name Type Description Model foreign key Model name. BodyPart foreign key Body part short name. Source code in element_deeplabcut/model.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 @schema class BodyPart ( dj . Lookup ): \"\"\"Body parts tracked by DeepLabCut models Attributes: Model (foreign key): Model name. BodyPart (foreign key): Body part short name.\"\"\" definition = \"\"\" body_part : varchar(32) --- body_part_description='' : varchar(1000) \"\"\" @classmethod def extract_new_body_parts ( cls , dlc_config : dict , verbose : bool = True ): \"\"\"Returns list of body parts present in dlc config, but not BodyPart table. Args: dlc_config (str or dict): path to a config.y*ml, or dict of such contents. verbose (bool): Default True. Print both existing and new items to console. \"\"\" if not isinstance ( dlc_config , dict ): dlc_config_fp = find_full_path ( get_dlc_root_data_dir (), Path ( dlc_config )) assert dlc_config_fp . exists () and dlc_config_fp . suffix in ( \".yml\" , \".yaml\" , ), f \"dlc_config is neither dict nor filepath \\n Check: { dlc_config_fp } \" if dlc_config_fp . suffix in ( \".yml\" , \".yaml\" ): with open ( dlc_config_fp , \"rb\" ) as f : dlc_config = yaml . safe_load ( f ) # -- Check and insert new BodyPart -- assert \"bodyparts\" in dlc_config , f \"Found no bodyparts section in { dlc_config } \" tracked_body_parts = cls . fetch ( \"body_part\" ) new_body_parts = np . setdiff1d ( dlc_config [ \"bodyparts\" ], tracked_body_parts ) if verbose : # Added to silence duplicate prompt during `insert_new_model` print ( f \"Existing body parts: { tracked_body_parts } \" ) print ( f \"New body parts: { new_body_parts } \" ) return new_body_parts @classmethod def insert_from_config ( cls , dlc_config : dict , descriptions : list = None , prompt = True ): \"\"\"Insert all body parts from a config file. Args: dlc_config (str or dict): path to a config.y*ml, or dict of such contents. descriptions (list): Optional. List of strings describing new body parts. prompt (bool): Optional, default True. Promp for confirmation before insert. \"\"\" # handle dlc_config being a yaml file new_body_parts = cls . extract_new_body_parts ( dlc_config , verbose = False ) if new_body_parts is not None : # Required bc np.array is ambiguous as bool if descriptions : assert len ( descriptions ) == len ( new_body_parts ), ( \"Descriptions list does not match \" + \" the number of new_body_parts\" ) print ( f \"New descriptions: { descriptions } \" ) if descriptions is None : descriptions = [ \"\" for x in range ( len ( new_body_parts ))] if ( prompt and dj . utils . user_choice ( f \"Insert { len ( new_body_parts ) } new body \" + \"part(s)?\" ) != \"yes\" ): print ( \"Canceled insert.\" ) return cls . insert ( [ { \"body_part\" : b , \"body_part_description\" : d } for b , d in zip ( new_body_parts , descriptions ) ] )", "title": "BodyPart"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.BodyPart.extract_new_body_parts", "text": "Returns list of body parts present in dlc config, but not BodyPart table. Parameters: Name Type Description Default dlc_config str or dict path to a config.y*ml, or dict of such contents. required verbose bool Default True. Print both existing and new items to console. True Source code in element_deeplabcut/model.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 @classmethod def extract_new_body_parts ( cls , dlc_config : dict , verbose : bool = True ): \"\"\"Returns list of body parts present in dlc config, but not BodyPart table. Args: dlc_config (str or dict): path to a config.y*ml, or dict of such contents. verbose (bool): Default True. Print both existing and new items to console. \"\"\" if not isinstance ( dlc_config , dict ): dlc_config_fp = find_full_path ( get_dlc_root_data_dir (), Path ( dlc_config )) assert dlc_config_fp . exists () and dlc_config_fp . suffix in ( \".yml\" , \".yaml\" , ), f \"dlc_config is neither dict nor filepath \\n Check: { dlc_config_fp } \" if dlc_config_fp . suffix in ( \".yml\" , \".yaml\" ): with open ( dlc_config_fp , \"rb\" ) as f : dlc_config = yaml . safe_load ( f ) # -- Check and insert new BodyPart -- assert \"bodyparts\" in dlc_config , f \"Found no bodyparts section in { dlc_config } \" tracked_body_parts = cls . fetch ( \"body_part\" ) new_body_parts = np . setdiff1d ( dlc_config [ \"bodyparts\" ], tracked_body_parts ) if verbose : # Added to silence duplicate prompt during `insert_new_model` print ( f \"Existing body parts: { tracked_body_parts } \" ) print ( f \"New body parts: { new_body_parts } \" ) return new_body_parts", "title": "extract_new_body_parts()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.BodyPart.insert_from_config", "text": "Insert all body parts from a config file. Parameters: Name Type Description Default dlc_config str or dict path to a config.y*ml, or dict of such contents. required descriptions list Optional. List of strings describing new body parts. None prompt bool Optional, default True. Promp for confirmation before insert. True Source code in element_deeplabcut/model.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 @classmethod def insert_from_config ( cls , dlc_config : dict , descriptions : list = None , prompt = True ): \"\"\"Insert all body parts from a config file. Args: dlc_config (str or dict): path to a config.y*ml, or dict of such contents. descriptions (list): Optional. List of strings describing new body parts. prompt (bool): Optional, default True. Promp for confirmation before insert. \"\"\" # handle dlc_config being a yaml file new_body_parts = cls . extract_new_body_parts ( dlc_config , verbose = False ) if new_body_parts is not None : # Required bc np.array is ambiguous as bool if descriptions : assert len ( descriptions ) == len ( new_body_parts ), ( \"Descriptions list does not match \" + \" the number of new_body_parts\" ) print ( f \"New descriptions: { descriptions } \" ) if descriptions is None : descriptions = [ \"\" for x in range ( len ( new_body_parts ))] if ( prompt and dj . utils . user_choice ( f \"Insert { len ( new_body_parts ) } new body \" + \"part(s)?\" ) != \"yes\" ): print ( \"Canceled insert.\" ) return cls . insert ( [ { \"body_part\" : b , \"body_part_description\" : d } for b , d in zip ( new_body_parts , descriptions ) ] )", "title": "insert_from_config()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.Model", "text": "Bases: dj . Manual DeepLabCut Models applied to generate pose estimations. Attributes: Name Type Description model_name varchar(64) User-friendly model name. task varchar(32) Task in the config yaml. date varchar(16) Date in the config yaml. iteration int Iteration/version of this model. snapshotindex int Which snapshot for prediction (if -1, latest). shuffle int Which shuffle of the training dataset. trainingsetindex int Which training set fraction to generate model. scorer varchar(64) Scorer/network name - DLC's GetScorerName(). config_template longblob Dictionary of the config for analyze_videos(). project_path varchar(255) DLC's project_path in config relative to root. model_prefix varchar(32) Optional. Prefix for model files. model_description varchar(1000) Optional. User-entered description. TrainingParamSet foreign key Optional. Training parameters primary key. Note Models are uniquely identified by the union of task, date, iteration, shuffle, snapshotindex, and trainingsetindex. Source code in element_deeplabcut/model.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 @schema class Model ( dj . Manual ): \"\"\"DeepLabCut Models applied to generate pose estimations. Attributes: model_name ( varchar(64) ): User-friendly model name. task ( varchar(32) ): Task in the config yaml. date ( varchar(16) ): Date in the config yaml. iteration (int): Iteration/version of this model. snapshotindex (int): Which snapshot for prediction (if -1, latest). shuffle (int): Which shuffle of the training dataset. trainingsetindex (int): Which training set fraction to generate model. scorer ( varchar(64) ): Scorer/network name - DLC's GetScorerName(). config_template (longblob): Dictionary of the config for analyze_videos(). project_path ( varchar(255) ): DLC's project_path in config relative to root. model_prefix ( varchar(32) ): Optional. Prefix for model files. model_description ( varchar(1000) ): Optional. User-entered description. TrainingParamSet (foreign key): Optional. Training parameters primary key. Note: Models are uniquely identified by the union of task, date, iteration, shuffle, snapshotindex, and trainingsetindex. \"\"\" definition = \"\"\" model_name : varchar(64) # User-friendly model name --- task : varchar(32) # Task in the config yaml date : varchar(16) # Date in the config yaml iteration : int # Iteration/version of this model snapshotindex : int # which snapshot for prediction (if -1, latest) shuffle : int # Shuffle (1) or not (0) trainingsetindex : int # Index of training fraction list in config.yaml unique index (task, date, iteration, shuffle, snapshotindex, trainingsetindex) scorer : varchar(64) # Scorer/network name - DLC's GetScorerName() config_template : longblob # Dictionary of the config for analyze_videos() project_path : varchar(255) # DLC's project_path in config relative to root model_prefix='' : varchar(32) model_description='' : varchar(300) -> [nullable] train.TrainingParamSet \"\"\" # project_path is the only item required downstream in the pose schema class BodyPart ( dj . Part ): \"\"\"Body parts associated with a given model Attributes: body_part ( varchar(32) ): Short name. Also called joint. body_part_description ( varchar(1000) ): Optional. Longer description.\"\"\" definition = \"\"\" -> master -> BodyPart \"\"\" @classmethod def insert_new_model ( cls , model_name : str , dlc_config , * , shuffle : int , trainingsetindex , project_path = None , model_description = \"\" , model_prefix = \"\" , paramset_idx : int = None , prompt = True , params = None , ): \"\"\"Insert new model into the dlc.Model table. Args: model_name (str): User-friendly name for this model. dlc_config (str or dict): path to a config.y*ml, or dict of such contents. shuffle (int): Shuffled or not as 1 or 0. trainingsetindex (int): Index of training fraction list in config.yaml. model_description (str): Optional. Description of this model. model_prefix (str): Optional. Filename prefix used across DLC project paramset_idx (int): Optional. Index from the TrainingParamSet table prompt (bool): Optional. Prompt the user with all info before inserting. params (dict): Optional. If dlc_config is path, dict of override items \"\"\" # handle dlc_config being a yaml file if not isinstance ( dlc_config , dict ): dlc_config_fp = find_full_path ( get_dlc_root_data_dir (), Path ( dlc_config )) assert dlc_config_fp . exists (), ( \"dlc_config is neither dict nor filepath\" + f \" \\n Check: { dlc_config_fp } \" ) if dlc_config_fp . suffix in ( \".yml\" , \".yaml\" ): with open ( dlc_config_fp , \"rb\" ) as f : dlc_config = yaml . safe_load ( f ) if isinstance ( params , dict ): dlc_config . update ( params ) # ---- Get and resolve project path ---- project_path = find_full_path ( get_dlc_root_data_dir (), dlc_config . get ( \"project_path\" , project_path ) ) dlc_config [ \"project_path\" ] = str ( project_path ) # update if different root_dir = find_root_directory ( get_dlc_root_data_dir (), project_path ) # ---- Verify config ---- needed_attributes = [ \"Task\" , \"date\" , \"iteration\" , \"snapshotindex\" , \"TrainingFraction\" , ] for attribute in needed_attributes : assert attribute in dlc_config , f \"Couldn't find { attribute } in config\" # ---- Get scorer name ---- # \"or 'f'\" below covers case where config returns None. str_to_bool handles else scorer_legacy = str_to_bool ( dlc_config . get ( \"scorer_legacy\" , \"f\" )) dlc_scorer = GetScorerName ( cfg = dlc_config , shuffle = shuffle , trainFraction = dlc_config [ \"TrainingFraction\" ][ int ( trainingsetindex )], modelprefix = model_prefix , )[ scorer_legacy ] if dlc_config [ \"snapshotindex\" ] == - 1 : dlc_scorer = \"\" . join ( dlc_scorer . split ( \"_\" )[: - 1 ]) # ---- Insert ---- model_dict = { \"model_name\" : model_name , \"model_description\" : model_description , \"scorer\" : dlc_scorer , \"task\" : dlc_config [ \"Task\" ], \"date\" : dlc_config [ \"date\" ], \"iteration\" : dlc_config [ \"iteration\" ], \"snapshotindex\" : dlc_config [ \"snapshotindex\" ], \"shuffle\" : shuffle , \"trainingsetindex\" : int ( trainingsetindex ), \"project_path\" : project_path . relative_to ( root_dir ) . as_posix (), \"paramset_idx\" : paramset_idx , \"config_template\" : dlc_config , } # -- prompt for confirmation -- if prompt : print ( \"--- DLC Model specification to be inserted ---\" ) for k , v in model_dict . items (): if k != \"config_template\" : print ( \" \\t {} : {} \" . format ( k , v )) else : print ( \" \\t -- Template/Contents of config.yaml --\" ) for k , v in model_dict [ \"config_template\" ] . items (): print ( \" \\t\\t {} : {} \" . format ( k , v )) if ( prompt and dj . utils . user_choice ( \"Proceed with new DLC model insert?\" ) != \"yes\" ): print ( \"Canceled insert.\" ) return # ---- Save DJ-managed config ---- _ = dlc_reader . save_yaml ( project_path , dlc_config ) # ____ Insert into table ---- with cls . connection . transaction : cls . insert1 ( model_dict ) # Returns array, so check size for unambiguous truth value if BodyPart . extract_new_body_parts ( dlc_config , verbose = False ) . size > 0 : BodyPart . insert_from_config ( dlc_config , prompt = prompt ) cls . BodyPart . insert (( model_name , bp ) for bp in dlc_config [ \"bodyparts\" ])", "title": "Model"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.Model.BodyPart", "text": "Bases: dj . Part Body parts associated with a given model Attributes: Name Type Description body_part varchar(32) Short name. Also called joint. body_part_description varchar(1000) Optional. Longer description. Source code in element_deeplabcut/model.py 337 338 339 340 341 342 343 344 345 346 347 class BodyPart ( dj . Part ): \"\"\"Body parts associated with a given model Attributes: body_part ( varchar(32) ): Short name. Also called joint. body_part_description ( varchar(1000) ): Optional. Longer description.\"\"\" definition = \"\"\" -> master -> BodyPart \"\"\"", "title": "BodyPart"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.Model.insert_new_model", "text": "Insert new model into the dlc.Model table. Parameters: Name Type Description Default model_name str User-friendly name for this model. required dlc_config str or dict path to a config.y*ml, or dict of such contents. required shuffle int Shuffled or not as 1 or 0. required trainingsetindex int Index of training fraction list in config.yaml. required model_description str Optional. Description of this model. '' model_prefix str Optional. Filename prefix used across DLC project '' paramset_idx int Optional. Index from the TrainingParamSet table None prompt bool Optional. Prompt the user with all info before inserting. True params dict Optional. If dlc_config is path, dict of override items None Source code in element_deeplabcut/model.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 @classmethod def insert_new_model ( cls , model_name : str , dlc_config , * , shuffle : int , trainingsetindex , project_path = None , model_description = \"\" , model_prefix = \"\" , paramset_idx : int = None , prompt = True , params = None , ): \"\"\"Insert new model into the dlc.Model table. Args: model_name (str): User-friendly name for this model. dlc_config (str or dict): path to a config.y*ml, or dict of such contents. shuffle (int): Shuffled or not as 1 or 0. trainingsetindex (int): Index of training fraction list in config.yaml. model_description (str): Optional. Description of this model. model_prefix (str): Optional. Filename prefix used across DLC project paramset_idx (int): Optional. Index from the TrainingParamSet table prompt (bool): Optional. Prompt the user with all info before inserting. params (dict): Optional. If dlc_config is path, dict of override items \"\"\" # handle dlc_config being a yaml file if not isinstance ( dlc_config , dict ): dlc_config_fp = find_full_path ( get_dlc_root_data_dir (), Path ( dlc_config )) assert dlc_config_fp . exists (), ( \"dlc_config is neither dict nor filepath\" + f \" \\n Check: { dlc_config_fp } \" ) if dlc_config_fp . suffix in ( \".yml\" , \".yaml\" ): with open ( dlc_config_fp , \"rb\" ) as f : dlc_config = yaml . safe_load ( f ) if isinstance ( params , dict ): dlc_config . update ( params ) # ---- Get and resolve project path ---- project_path = find_full_path ( get_dlc_root_data_dir (), dlc_config . get ( \"project_path\" , project_path ) ) dlc_config [ \"project_path\" ] = str ( project_path ) # update if different root_dir = find_root_directory ( get_dlc_root_data_dir (), project_path ) # ---- Verify config ---- needed_attributes = [ \"Task\" , \"date\" , \"iteration\" , \"snapshotindex\" , \"TrainingFraction\" , ] for attribute in needed_attributes : assert attribute in dlc_config , f \"Couldn't find { attribute } in config\" # ---- Get scorer name ---- # \"or 'f'\" below covers case where config returns None. str_to_bool handles else scorer_legacy = str_to_bool ( dlc_config . get ( \"scorer_legacy\" , \"f\" )) dlc_scorer = GetScorerName ( cfg = dlc_config , shuffle = shuffle , trainFraction = dlc_config [ \"TrainingFraction\" ][ int ( trainingsetindex )], modelprefix = model_prefix , )[ scorer_legacy ] if dlc_config [ \"snapshotindex\" ] == - 1 : dlc_scorer = \"\" . join ( dlc_scorer . split ( \"_\" )[: - 1 ]) # ---- Insert ---- model_dict = { \"model_name\" : model_name , \"model_description\" : model_description , \"scorer\" : dlc_scorer , \"task\" : dlc_config [ \"Task\" ], \"date\" : dlc_config [ \"date\" ], \"iteration\" : dlc_config [ \"iteration\" ], \"snapshotindex\" : dlc_config [ \"snapshotindex\" ], \"shuffle\" : shuffle , \"trainingsetindex\" : int ( trainingsetindex ), \"project_path\" : project_path . relative_to ( root_dir ) . as_posix (), \"paramset_idx\" : paramset_idx , \"config_template\" : dlc_config , } # -- prompt for confirmation -- if prompt : print ( \"--- DLC Model specification to be inserted ---\" ) for k , v in model_dict . items (): if k != \"config_template\" : print ( \" \\t {} : {} \" . format ( k , v )) else : print ( \" \\t -- Template/Contents of config.yaml --\" ) for k , v in model_dict [ \"config_template\" ] . items (): print ( \" \\t\\t {} : {} \" . format ( k , v )) if ( prompt and dj . utils . user_choice ( \"Proceed with new DLC model insert?\" ) != \"yes\" ): print ( \"Canceled insert.\" ) return # ---- Save DJ-managed config ---- _ = dlc_reader . save_yaml ( project_path , dlc_config ) # ____ Insert into table ---- with cls . connection . transaction : cls . insert1 ( model_dict ) # Returns array, so check size for unambiguous truth value if BodyPart . extract_new_body_parts ( dlc_config , verbose = False ) . size > 0 : BodyPart . insert_from_config ( dlc_config , prompt = prompt ) cls . BodyPart . insert (( model_name , bp ) for bp in dlc_config [ \"bodyparts\" ])", "title": "insert_new_model()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.ModelEvaluation", "text": "Bases: dj . Computed Performance characteristics model calculated by deeplabcut.evaluate_network Attributes: Name Type Description Model foreign key Model name. train_iterations int Training iterations. train_error float Optional. Train error (px). test_error float Optional. Test error (px). p_cutoff float Optional. p-cutoff used. train_error_p float Optional. Train error with p-cutoff. test_error_p float Optional. Test error with p-cutoff. Source code in element_deeplabcut/model.py 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 @schema class ModelEvaluation ( dj . Computed ): \"\"\"Performance characteristics model calculated by `deeplabcut.evaluate_network` Attributes: Model (foreign key): Model name. train_iterations (int): Training iterations. train_error (float): Optional. Train error (px). test_error (float): Optional. Test error (px). p_cutoff (float): Optional. p-cutoff used. train_error_p (float): Optional. Train error with p-cutoff. test_error_p (float): Optional. Test error with p-cutoff.\"\"\" definition = \"\"\" -> Model --- train_iterations : int # Training iterations train_error=null : float # Train error (px) test_error=null : float # Test error (px) p_cutoff=null : float # p-cutoff used train_error_p=null : float # Train error with p-cutoff test_error_p=null : float # Test error with p-cutoff \"\"\" def make ( self , key ): \"\"\".populate() method will launch evaulation for each unique entry in Model.\"\"\" dlc_config , project_path , model_prefix , shuffle , trainingsetindex = ( Model & key ) . fetch1 ( \"config_template\" , \"project_path\" , \"model_prefix\" , \"shuffle\" , \"trainingsetindex\" , ) project_path = find_full_path ( get_dlc_root_data_dir (), project_path ) yml_path , _ = dlc_reader . read_yaml ( project_path ) evaluate_network ( yml_path , Shuffles = [ shuffle ], # this needs to be a list trainingsetindex = trainingsetindex , comparisonbodyparts = \"all\" , ) eval_folder = get_evaluation_folder ( trainFraction = dlc_config [ \"TrainingFraction\" ][ trainingsetindex ], shuffle = shuffle , cfg = dlc_config , modelprefix = model_prefix , ) eval_path = project_path / eval_folder assert eval_path . exists (), f \"Couldn't find evaluation folder: \\n { eval_path } \" eval_csvs = list ( eval_path . glob ( \"*csv\" )) max_modified_time = 0 for eval_csv in eval_csvs : modified_time = os . path . getmtime ( eval_csv ) if modified_time > max_modified_time : eval_csv_latest = eval_csv with open ( eval_csv_latest , newline = \"\" ) as f : results = list ( csv . DictReader ( f , delimiter = \",\" ))[ 0 ] # in testing, test_error_p returned empty string self . insert1 ( dict ( key , train_iterations = results [ \"Training iterations:\" ], train_error = results [ \" Train error(px)\" ], test_error = results [ \" Test error(px)\" ], p_cutoff = results [ \"p-cutoff used\" ], train_error_p = results [ \"Train error with p-cutoff\" ], test_error_p = results [ \"Test error with p-cutoff\" ], ) )", "title": "ModelEvaluation"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.ModelEvaluation.make", "text": ".populate() method will launch evaulation for each unique entry in Model. Source code in element_deeplabcut/model.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 def make ( self , key ): \"\"\".populate() method will launch evaulation for each unique entry in Model.\"\"\" dlc_config , project_path , model_prefix , shuffle , trainingsetindex = ( Model & key ) . fetch1 ( \"config_template\" , \"project_path\" , \"model_prefix\" , \"shuffle\" , \"trainingsetindex\" , ) project_path = find_full_path ( get_dlc_root_data_dir (), project_path ) yml_path , _ = dlc_reader . read_yaml ( project_path ) evaluate_network ( yml_path , Shuffles = [ shuffle ], # this needs to be a list trainingsetindex = trainingsetindex , comparisonbodyparts = \"all\" , ) eval_folder = get_evaluation_folder ( trainFraction = dlc_config [ \"TrainingFraction\" ][ trainingsetindex ], shuffle = shuffle , cfg = dlc_config , modelprefix = model_prefix , ) eval_path = project_path / eval_folder assert eval_path . exists (), f \"Couldn't find evaluation folder: \\n { eval_path } \" eval_csvs = list ( eval_path . glob ( \"*csv\" )) max_modified_time = 0 for eval_csv in eval_csvs : modified_time = os . path . getmtime ( eval_csv ) if modified_time > max_modified_time : eval_csv_latest = eval_csv with open ( eval_csv_latest , newline = \"\" ) as f : results = list ( csv . DictReader ( f , delimiter = \",\" ))[ 0 ] # in testing, test_error_p returned empty string self . insert1 ( dict ( key , train_iterations = results [ \"Training iterations:\" ], train_error = results [ \" Train error(px)\" ], test_error = results [ \" Test error(px)\" ], p_cutoff = results [ \"p-cutoff used\" ], train_error_p = results [ \"Train error with p-cutoff\" ], test_error_p = results [ \"Test error with p-cutoff\" ], ) )", "title": "make()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimationTask", "text": "Bases: dj . Manual Staging table for pairing of video recording and model before inference. Attributes: Name Type Description VideoRecording foreign key Video recording key. Model foreign key Model name. task_mode load or trigger Optional. Default load. Or trigger computation. pose_estimation_output_dir varchar(255) Optional. Output dir relative to get_dlc_root_data_dir. pose_estimation_params longblob Optional. Params for DLC's analyze_videos params, if not default. Source code in element_deeplabcut/model.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 @schema class PoseEstimationTask ( dj . Manual ): \"\"\"Staging table for pairing of video recording and model before inference. Attributes: VideoRecording (foreign key): Video recording key. Model (foreign key): Model name. task_mode (load or trigger): Optional. Default load. Or trigger computation. pose_estimation_output_dir ( varchar(255) ): Optional. Output dir relative to get_dlc_root_data_dir. pose_estimation_params (longblob): Optional. Params for DLC's analyze_videos params, if not default.\"\"\" definition = \"\"\" -> VideoRecording # Session -> Recording + File part table -> Model # Must specify a DLC project_path --- task_mode='load' : enum('load', 'trigger') # load results or trigger computation pose_estimation_output_dir='': varchar(255) # output dir relative to the root dir pose_estimation_params=null : longblob # analyze_videos params, if not default \"\"\" @classmethod def infer_output_dir ( cls , key : dict , relative : bool = False , mkdir : bool = False ): \"\"\"Return the expected pose_estimation_output_dir. Spaces in model name are replaced with hyphens. Based on convention: / video_dir / Device_{}_Recording_{}_Model_{} Args: key: DataJoint key specifying a pairing of VideoRecording and Model. relative (bool): Report directory relative to get_dlc_processed_data_dir(). mkdir (bool): Default False. Make directory if it doesn't exist. \"\"\" video_filepath = find_full_path ( get_dlc_root_data_dir (), ( VideoRecording . File & key ) . fetch ( \"file_path\" , limit = 1 )[ 0 ], ) root_dir = find_root_directory ( get_dlc_root_data_dir (), video_filepath . parent ) recording_key = VideoRecording & key device = \"-\" . join ( str ( v ) for v in ( _linking_module . Device & recording_key ) . fetch1 ( \"KEY\" ) . values () ) if get_dlc_processed_data_dir (): processed_dir = Path ( get_dlc_processed_data_dir ()) else : # if processed not provided, default to where video is processed_dir = root_dir output_dir = ( processed_dir / video_filepath . parent . relative_to ( root_dir ) / ( f 'device_ { device } _recording_ { key [ \"recording_id\" ] } _model_' + key [ \"model_name\" ] . replace ( \" \" , \"-\" ) ) ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def insert_estimation_task ( cls , key : dict , task_mode : str = \"trigger\" , params : dict = None , relative : bool = True , mkdir : bool = True , skip_duplicates : bool = False , ): \"\"\"Insert PoseEstimationTask in inferred output dir. Based on the convention / video_dir / device_{}_recording_{}_model_{} Args: key: DataJoint key specifying a pairing of VideoRecording and Model. task_mode (bool): Default 'trigger' computation. Or 'load' existing results. params (dict): Optional. Parameters passed to DLC's analyze_videos: videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve relative (bool): Report directory relative to get_dlc_processed_data_dir(). mkdir (bool): Default False. Make directory if it doesn't exist. \"\"\" output_dir = cls . infer_output_dir ( key , relative = relative , mkdir = mkdir ) cls . insert1 ( { ** key , \"task_mode\" : task_mode , \"pose_estimation_params\" : params , \"pose_estimation_output_dir\" : output_dir , }, skip_duplicates = skip_duplicates , )", "title": "PoseEstimationTask"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimationTask.infer_output_dir", "text": "Return the expected pose_estimation_output_dir. Spaces in model name are replaced with hyphens. Based on convention: / video_dir / Device_{} Recording {} Model {} Parameters: Name Type Description Default key dict DataJoint key specifying a pairing of VideoRecording and Model. required relative bool Report directory relative to get_dlc_processed_data_dir(). False mkdir bool Default False. Make directory if it doesn't exist. False Source code in element_deeplabcut/model.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 @classmethod def infer_output_dir ( cls , key : dict , relative : bool = False , mkdir : bool = False ): \"\"\"Return the expected pose_estimation_output_dir. Spaces in model name are replaced with hyphens. Based on convention: / video_dir / Device_{}_Recording_{}_Model_{} Args: key: DataJoint key specifying a pairing of VideoRecording and Model. relative (bool): Report directory relative to get_dlc_processed_data_dir(). mkdir (bool): Default False. Make directory if it doesn't exist. \"\"\" video_filepath = find_full_path ( get_dlc_root_data_dir (), ( VideoRecording . File & key ) . fetch ( \"file_path\" , limit = 1 )[ 0 ], ) root_dir = find_root_directory ( get_dlc_root_data_dir (), video_filepath . parent ) recording_key = VideoRecording & key device = \"-\" . join ( str ( v ) for v in ( _linking_module . Device & recording_key ) . fetch1 ( \"KEY\" ) . values () ) if get_dlc_processed_data_dir (): processed_dir = Path ( get_dlc_processed_data_dir ()) else : # if processed not provided, default to where video is processed_dir = root_dir output_dir = ( processed_dir / video_filepath . parent . relative_to ( root_dir ) / ( f 'device_ { device } _recording_ { key [ \"recording_id\" ] } _model_' + key [ \"model_name\" ] . replace ( \" \" , \"-\" ) ) ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) return output_dir . relative_to ( processed_dir ) if relative else output_dir", "title": "infer_output_dir()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimationTask.insert_estimation_task", "text": "Insert PoseEstimationTask in inferred output dir. Based on the convention / video_dir / device_{} recording {} model {} Parameters: Name Type Description Default key dict DataJoint key specifying a pairing of VideoRecording and Model. required task_mode bool Default 'trigger' computation. Or 'load' existing results. 'trigger' params dict Optional. Parameters passed to DLC's analyze_videos: videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve None relative bool Report directory relative to get_dlc_processed_data_dir(). True mkdir bool Default False. Make directory if it doesn't exist. True Source code in element_deeplabcut/model.py 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 @classmethod def insert_estimation_task ( cls , key : dict , task_mode : str = \"trigger\" , params : dict = None , relative : bool = True , mkdir : bool = True , skip_duplicates : bool = False , ): \"\"\"Insert PoseEstimationTask in inferred output dir. Based on the convention / video_dir / device_{}_recording_{}_model_{} Args: key: DataJoint key specifying a pairing of VideoRecording and Model. task_mode (bool): Default 'trigger' computation. Or 'load' existing results. params (dict): Optional. Parameters passed to DLC's analyze_videos: videotype, gputouse, save_as_csv, batchsize, cropping, TFGPUinference, dynamic, robust_nframes, allow_growth, use_shelve relative (bool): Report directory relative to get_dlc_processed_data_dir(). mkdir (bool): Default False. Make directory if it doesn't exist. \"\"\" output_dir = cls . infer_output_dir ( key , relative = relative , mkdir = mkdir ) cls . insert1 ( { ** key , \"task_mode\" : task_mode , \"pose_estimation_params\" : params , \"pose_estimation_output_dir\" : output_dir , }, skip_duplicates = skip_duplicates , )", "title": "insert_estimation_task()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimation", "text": "Bases: dj . Computed Results of pose estimation. Attributes: Name Type Description PoseEstimationTask foreign key Pose Estimation Task key. post_estimation_time datetime time of generation of this set of DLC results. Source code in element_deeplabcut/model.py 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 @schema class PoseEstimation ( dj . Computed ): \"\"\"Results of pose estimation. Attributes: PoseEstimationTask (foreign key): Pose Estimation Task key. post_estimation_time (datetime): time of generation of this set of DLC results. \"\"\" definition = \"\"\" -> PoseEstimationTask --- pose_estimation_time: datetime # time of generation of this set of DLC results \"\"\" class BodyPartPosition ( dj . Part ): \"\"\"Position of individual body parts by frame index Attributes: PoseEstimation (foreign key): Pose Estimation key. Model.BodyPart (foreign key): Body Part key. frame_index (longblob): Frame index in model. x_pos (longblob): X position. y_pos (longblob): Y position. z_pos (longblob): Optional. Z position. likelihood (longblob): Model confidence.\"\"\" definition = \"\"\" # uses DeepLabCut h5 output for body part position -> master -> Model.BodyPart --- frame_index : longblob # frame index in model x_pos : longblob y_pos : longblob z_pos=null : longblob likelihood : longblob \"\"\" def make ( self , key ): \"\"\".populate() method will launch training for each PoseEstimationTask\"\"\" # ID model and directories dlc_model = ( Model & key ) . fetch1 () task_mode , analyze_video_params , output_dir = ( PoseEstimationTask & key ) . fetch1 ( \"task_mode\" , \"pose_estimation_params\" , \"pose_estimation_output_dir\" ) analyze_video_params = analyze_video_params or {} output_dir = find_full_path ( get_dlc_root_data_dir (), output_dir ) video_filepaths = [ find_full_path ( get_dlc_root_data_dir (), fp ) . as_posix () for fp in ( VideoRecording . File & key ) . fetch ( \"file_path\" ) ] project_path = find_full_path ( get_dlc_root_data_dir (), dlc_model [ \"project_path\" ] ) # Triger PoseEstimation if task_mode == \"trigger\" : dlc_reader . do_pose_estimation ( video_filepaths , dlc_model , project_path , output_dir , ** analyze_video_params , ) dlc_result = dlc_reader . PoseEstimation ( output_dir ) creation_time = datetime . fromtimestamp ( dlc_result . creation_time ) . strftime ( \"%Y-%m- %d %H:%M:%S\" ) body_parts = [ { ** key , \"body_part\" : k , \"frame_index\" : np . arange ( dlc_result . nframes ), \"x_pos\" : v [ \"x\" ], \"y_pos\" : v [ \"y\" ], \"z_pos\" : v . get ( \"z\" ), \"likelihood\" : v [ \"likelihood\" ], } for k , v in dlc_result . data . items () ] self . insert1 ({ ** key , \"pose_estimation_time\" : creation_time }) self . BodyPartPosition . insert ( body_parts ) @classmethod def get_trajectory ( cls , key : dict , body_parts : list = \"all\" ) -> pd . DataFrame : \"\"\"Returns a pandas dataframe of coordinates of the specified body_part(s) Args: key (dict): A DataJoint query specifying one PoseEstimation entry. body_parts (list, optional): Body parts as a list. If \"all\", all joints Returns: df: multi index pandas dataframe with DLC scorer names, body_parts and x/y coordinates of each joint name for a camera_id, similar to output of DLC dataframe. If 2D, z is set of zeros \"\"\" model_name = key [ \"model_name\" ] if body_parts == \"all\" : body_parts = ( cls . BodyPartPosition & key ) . fetch ( \"body_part\" ) elif not isinstance ( body_parts , list ): body_parts = list ( body_parts ) df = None for body_part in body_parts : x_pos , y_pos , z_pos , likelihood = ( cls . BodyPartPosition & { \"body_part\" : body_part } ) . fetch1 ( \"x_pos\" , \"y_pos\" , \"z_pos\" , \"likelihood\" ) if not z_pos : z_pos = np . zeros_like ( x_pos ) a = np . vstack (( x_pos , y_pos , z_pos , likelihood )) a = a . T pdindex = pd . MultiIndex . from_product ( [[ model_name ], [ body_part ], [ \"x\" , \"y\" , \"z\" , \"likelihood\" ]], names = [ \"scorer\" , \"bodyparts\" , \"coords\" ], ) frame = pd . DataFrame ( a , columns = pdindex , index = range ( 0 , a . shape [ 0 ])) df = pd . concat ([ df , frame ], axis = 1 ) return df", "title": "PoseEstimation"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimation.BodyPartPosition", "text": "Bases: dj . Part Position of individual body parts by frame index Attributes: Name Type Description PoseEstimation foreign key Pose Estimation key. Model.BodyPart foreign key Body Part key. frame_index longblob Frame index in model. x_pos longblob X position. y_pos longblob Y position. z_pos longblob Optional. Z position. likelihood longblob Model confidence. Source code in element_deeplabcut/model.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 class BodyPartPosition ( dj . Part ): \"\"\"Position of individual body parts by frame index Attributes: PoseEstimation (foreign key): Pose Estimation key. Model.BodyPart (foreign key): Body Part key. frame_index (longblob): Frame index in model. x_pos (longblob): X position. y_pos (longblob): Y position. z_pos (longblob): Optional. Z position. likelihood (longblob): Model confidence.\"\"\" definition = \"\"\" # uses DeepLabCut h5 output for body part position -> master -> Model.BodyPart --- frame_index : longblob # frame index in model x_pos : longblob y_pos : longblob z_pos=null : longblob likelihood : longblob \"\"\"", "title": "BodyPartPosition"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimation.make", "text": ".populate() method will launch training for each PoseEstimationTask Source code in element_deeplabcut/model.py 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 def make ( self , key ): \"\"\".populate() method will launch training for each PoseEstimationTask\"\"\" # ID model and directories dlc_model = ( Model & key ) . fetch1 () task_mode , analyze_video_params , output_dir = ( PoseEstimationTask & key ) . fetch1 ( \"task_mode\" , \"pose_estimation_params\" , \"pose_estimation_output_dir\" ) analyze_video_params = analyze_video_params or {} output_dir = find_full_path ( get_dlc_root_data_dir (), output_dir ) video_filepaths = [ find_full_path ( get_dlc_root_data_dir (), fp ) . as_posix () for fp in ( VideoRecording . File & key ) . fetch ( \"file_path\" ) ] project_path = find_full_path ( get_dlc_root_data_dir (), dlc_model [ \"project_path\" ] ) # Triger PoseEstimation if task_mode == \"trigger\" : dlc_reader . do_pose_estimation ( video_filepaths , dlc_model , project_path , output_dir , ** analyze_video_params , ) dlc_result = dlc_reader . PoseEstimation ( output_dir ) creation_time = datetime . fromtimestamp ( dlc_result . creation_time ) . strftime ( \"%Y-%m- %d %H:%M:%S\" ) body_parts = [ { ** key , \"body_part\" : k , \"frame_index\" : np . arange ( dlc_result . nframes ), \"x_pos\" : v [ \"x\" ], \"y_pos\" : v [ \"y\" ], \"z_pos\" : v . get ( \"z\" ), \"likelihood\" : v [ \"likelihood\" ], } for k , v in dlc_result . data . items () ] self . insert1 ({ ** key , \"pose_estimation_time\" : creation_time }) self . BodyPartPosition . insert ( body_parts )", "title": "make()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.PoseEstimation.get_trajectory", "text": "Returns a pandas dataframe of coordinates of the specified body_part(s) Parameters: Name Type Description Default key dict A DataJoint query specifying one PoseEstimation entry. required body_parts list Body parts as a list. If \"all\", all joints 'all' Returns: Name Type Description df pd . DataFrame multi index pandas dataframe with DLC scorer names, body_parts and x/y coordinates of each joint name for a camera_id, similar to output of DLC dataframe. If 2D, z is set of zeros Source code in element_deeplabcut/model.py 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 @classmethod def get_trajectory ( cls , key : dict , body_parts : list = \"all\" ) -> pd . DataFrame : \"\"\"Returns a pandas dataframe of coordinates of the specified body_part(s) Args: key (dict): A DataJoint query specifying one PoseEstimation entry. body_parts (list, optional): Body parts as a list. If \"all\", all joints Returns: df: multi index pandas dataframe with DLC scorer names, body_parts and x/y coordinates of each joint name for a camera_id, similar to output of DLC dataframe. If 2D, z is set of zeros \"\"\" model_name = key [ \"model_name\" ] if body_parts == \"all\" : body_parts = ( cls . BodyPartPosition & key ) . fetch ( \"body_part\" ) elif not isinstance ( body_parts , list ): body_parts = list ( body_parts ) df = None for body_part in body_parts : x_pos , y_pos , z_pos , likelihood = ( cls . BodyPartPosition & { \"body_part\" : body_part } ) . fetch1 ( \"x_pos\" , \"y_pos\" , \"z_pos\" , \"likelihood\" ) if not z_pos : z_pos = np . zeros_like ( x_pos ) a = np . vstack (( x_pos , y_pos , z_pos , likelihood )) a = a . T pdindex = pd . MultiIndex . from_product ( [[ model_name ], [ body_part ], [ \"x\" , \"y\" , \"z\" , \"likelihood\" ]], names = [ \"scorer\" , \"bodyparts\" , \"coords\" ], ) frame = pd . DataFrame ( a , columns = pdindex , index = range ( 0 , a . shape [ 0 ])) df = pd . concat ([ df , frame ], axis = 1 ) return df", "title": "get_trajectory()"}, {"location": "api/element_deeplabcut/model/#element_deeplabcut.model.str_to_bool", "text": "Return whether the provided string represents true. Otherwise false. Parameters: Name Type Description Default value any Any input required Returns: Name Type Description bool bool True if value in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\") Source code in element_deeplabcut/model.py 765 766 767 768 769 770 771 772 773 774 775 776 777 778 def str_to_bool ( value ) -> bool : \"\"\"Return whether the provided string represents true. Otherwise false. Args: value (any): Any input Returns: bool (bool): True if value in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\") \"\"\" # Due to distutils equivalent depreciation in 3.10 # Adopted from github.com/PostHog/posthog/blob/master/posthog/utils.py if not value : return False return str ( value ) . lower () in ( \"y\" , \"yes\" , \"t\" , \"true\" , \"on\" , \"1\" )", "title": "str_to_bool()"}, {"location": "api/element_deeplabcut/train/", "text": "Code adapted from the Mathis Lab MIT License Copyright (c) 2022 Mackenzie Mathis DataJoint Schema for DeepLabCut 2.x, Supports 2D and 3D DLC via triangulation. activate ( train_schema_name , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activate this schema. Parameters: Name Type Description Default train_schema_name str schema name on the database server required create_schema bool when True (default), create schema in the database if it does not yet exist. True create_tables str when True (default), create schema tables in the database if they do not yet exist. True linking_module str a module (or name) containing the required dependencies. None Dependencies: Functions get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder. Source code in element_deeplabcut/train.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def activate ( train_schema_name : str , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None ): \"\"\"Activate this schema. Args: train_schema_name (str): schema name on the database server create_schema (bool): when True (default), create schema in the database if it does not yet exist. create_tables (str): when True (default), create schema tables in the database if they do not yet exist. linking_module (str): a module (or name) containing the required dependencies. Dependencies: Functions: get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" assert hasattr ( linking_module , \"get_dlc_root_data_dir\" ), \"The linking module must specify a lookup funtion for a root data directory\" global _linking_module _linking_module = linking_module # activate schema . activate ( train_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) get_dlc_root_data_dir () \u00b6 Pulls relevant func from parent namespace to specify root data dir(s). It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strongs for possible root data directories. Source code in element_deeplabcut/train.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_dlc_root_data_dir () -> list : \"\"\"Pulls relevant func from parent namespace to specify root data dir(s). It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strongs for possible root data directories. \"\"\" root_directories = _linking_module . get_dlc_root_data_dir () if isinstance ( root_directories , ( str , Path )): root_directories = [ root_directories ] if ( hasattr ( _linking_module , \"get_dlc_processed_data_dir\" ) and get_dlc_processed_data_dir () not in root_directories ): root_directories . append ( _linking_module . get_dlc_processed_data_dir ()) return root_directories get_dlc_processed_data_dir () \u00b6 Pulls relevant func from parent namespace. Defaults to DLC's project /videos/. Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default. Source code in element_deeplabcut/train.py 99 100 101 102 103 104 105 106 107 108 109 def get_dlc_processed_data_dir () -> str : \"\"\"Pulls relevant func from parent namespace. Defaults to DLC's project /videos/. Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default. \"\"\" if hasattr ( _linking_module , \"get_dlc_processed_data_dir\" ): return _linking_module . get_dlc_processed_data_dir () else : return get_dlc_root_data_dir ()[ 0 ] VideoSet \u00b6 Bases: dj . Manual Collection of videos included in a given training set. Attributes: Name Type Description video_set_id int Unique ID for collection of videos. Source code in element_deeplabcut/train.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @schema class VideoSet ( dj . Manual ): \"\"\"Collection of videos included in a given training set. Attributes: video_set_id (int): Unique ID for collection of videos.\"\"\" definition = \"\"\" # Set of vids in training set video_set_id: int \"\"\" class File ( dj . Part ): \"\"\"File IDs and paths in a given VideoSet Attributes: VideoSet (foreign key): VideoSet key. file_path ( varchar(255) ): Path to file on disk relative to root.\"\"\" definition = \"\"\" # Paths of training files (e.g., labeled pngs, CSV or video) -> master file_id: int --- file_path: varchar(255) \"\"\" File \u00b6 Bases: dj . Part File IDs and paths in a given VideoSet Attributes: Name Type Description VideoSet foreign key VideoSet key. file_path varchar(255) Path to file on disk relative to root. Source code in element_deeplabcut/train.py 126 127 128 129 130 131 132 133 134 135 136 137 138 class File ( dj . Part ): \"\"\"File IDs and paths in a given VideoSet Attributes: VideoSet (foreign key): VideoSet key. file_path ( varchar(255) ): Path to file on disk relative to root.\"\"\" definition = \"\"\" # Paths of training files (e.g., labeled pngs, CSV or video) -> master file_id: int --- file_path: varchar(255) \"\"\" TrainingParamSet \u00b6 Bases: dj . Lookup Parameters used to train a model Attributes: Name Type Description paramset_idx smallint Index uniqely identifying paramset. paramset_desc varchar(128) Description of paramset. param_set_hash uuid Hash identifying this paramset. params longblob Dictionary of all applicable parameters. Note longblob param_set_hash must be unique. Source code in element_deeplabcut/train.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 @schema class TrainingParamSet ( dj . Lookup ): \"\"\"Parameters used to train a model Attributes: paramset_idx (smallint): Index uniqely identifying paramset. paramset_desc ( varchar(128) ): Description of paramset. param_set_hash (uuid): Hash identifying this paramset. params (longblob): Dictionary of all applicable parameters. Note: param_set_hash must be unique.\"\"\" definition = \"\"\" # Parameters to specify a DLC model training instance # For DLC \u2264 v2.0, include scorer_lecacy = True in params paramset_idx : smallint --- paramset_desc: varchar(128) param_set_hash : uuid # hash identifying this parameterset unique index (param_set_hash) params : longblob # dictionary of all applicable parameters \"\"\" required_parameters = ( \"shuffle\" , \"trainingsetindex\" ) skipped_parameters = ( \"project_path\" , \"video_sets\" ) @classmethod def insert_new_params ( cls , paramset_desc : str , params : dict , paramset_idx : int = None ): \"\"\" Insert a new set of training parameters into dlc.TrainingParamSet. Args: paramset_desc (str): Description of parameter set to be inserted params (dict): Dictionary including all settings to specify model training. Must include shuffle & trainingsetindex b/c not in config.yaml. project_path and video_sets will be overwritten by config.yaml. Note that trainingsetindex is 0-indexed paramset_idx (int): optional, integer to represent parameters. \"\"\" for required_param in cls . required_parameters : assert required_param in params , ( \"Missing required parameter: \" + required_param ) for skipped_param in cls . skipped_parameters : if skipped_param in params : params . pop ( skipped_param ) if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} # If the specified param-set already exists if param_query : existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if existing_paramset_idx == int ( paramset_idx ): # If existing_idx same: return # job done else : cls . insert1 ( param_dict ) # if duplicate, will raise duplicate error insert_new_params ( paramset_desc , params , paramset_idx = None ) classmethod \u00b6 Insert a new set of training parameters into dlc.TrainingParamSet. Parameters: Name Type Description Default paramset_desc str Description of parameter set to be inserted required params dict Dictionary including all settings to specify model training. Must include shuffle & trainingsetindex b/c not in config.yaml. project_path and video_sets will be overwritten by config.yaml. Note that trainingsetindex is 0-indexed required paramset_idx int optional, integer to represent parameters. None Source code in element_deeplabcut/train.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 @classmethod def insert_new_params ( cls , paramset_desc : str , params : dict , paramset_idx : int = None ): \"\"\" Insert a new set of training parameters into dlc.TrainingParamSet. Args: paramset_desc (str): Description of parameter set to be inserted params (dict): Dictionary including all settings to specify model training. Must include shuffle & trainingsetindex b/c not in config.yaml. project_path and video_sets will be overwritten by config.yaml. Note that trainingsetindex is 0-indexed paramset_idx (int): optional, integer to represent parameters. \"\"\" for required_param in cls . required_parameters : assert required_param in params , ( \"Missing required parameter: \" + required_param ) for skipped_param in cls . skipped_parameters : if skipped_param in params : params . pop ( skipped_param ) if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} # If the specified param-set already exists if param_query : existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if existing_paramset_idx == int ( paramset_idx ): # If existing_idx same: return # job done else : cls . insert1 ( param_dict ) # if duplicate, will raise duplicate error TrainingTask \u00b6 Bases: dj . Manual Staging table for pairing videosets and training parameter sets Attributes: Name Type Description VideoSet foreign key VideoSet Key. TrainingParamSet foreign key TrainingParamSet key. training_id int Unique ID for training task. model_prefix varchar(32) Optional. Prefix for model files. project_path varchar(255) Optional. DLC's project_path in config relative to get_dlc_root_data_dir Source code in element_deeplabcut/train.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 @schema class TrainingTask ( dj . Manual ): \"\"\"Staging table for pairing videosets and training parameter sets Attributes: VideoSet (foreign key): VideoSet Key. TrainingParamSet (foreign key): TrainingParamSet key. training_id (int): Unique ID for training task. model_prefix ( varchar(32) ): Optional. Prefix for model files. project_path ( varchar(255) ): Optional. DLC's project_path in config relative to get_dlc_root_data_dir \"\"\" definition = \"\"\" # Specification for a DLC model training instance -> VideoSet # labeled video(s) for training -> TrainingParamSet training_id : int --- model_prefix='' : varchar(32) project_path='' : varchar(255) # DLC's project_path in config relative to root \"\"\" ModelTraining \u00b6 Bases: dj . Computed Automated Model training information. Attributes: Name Type Description TrainingTask foreign key TrainingTask key. latest_snapshot int unsigned Latest exact snapshot index (i.e., never -1). config_template longblob Stored full config file. Source code in element_deeplabcut/train.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 @schema class ModelTraining ( dj . Computed ): \"\"\"Automated Model training information. Attributes: TrainingTask (foreign key): TrainingTask key. latest_snapshot (int unsigned): Latest exact snapshot index (i.e., never -1). config_template (longblob): Stored full config file.\"\"\" definition = \"\"\" -> TrainingTask --- latest_snapshot: int unsigned # latest exact snapshot index (i.e., never -1) config_template: longblob # stored full config file \"\"\" # To continue from previous training snapshot, devs suggest editing pose_cfg.yml # https://github.com/DeepLabCut/DeepLabCut/issues/70 def make ( self , key ): \"\"\"Launch training for each train.TrainingTask training_id via `.populate()`.\"\"\" project_path , model_prefix = ( TrainingTask & key ) . fetch1 ( \"project_path\" , \"model_prefix\" ) project_path = find_full_path ( get_dlc_root_data_dir (), project_path ) # ---- Build and save DLC configuration (yaml) file ---- _ , dlc_config = dlc_reader . read_yaml ( project_path ) # load existing dlc_config . update (( TrainingParamSet & key ) . fetch1 ( \"params\" )) dlc_config . update ( { \"project_path\" : project_path . as_posix (), \"modelprefix\" : model_prefix , \"train_fraction\" : dlc_config [ \"TrainingFraction\" ][ int ( dlc_config [ \"trainingsetindex\" ]) ], \"training_filelist_datajoint\" : [ # don't overwrite origin video_sets find_full_path ( get_dlc_root_data_dir (), fp ) . as_posix () for fp in ( VideoSet . File & key ) . fetch ( \"file_path\" ) ], } ) # Write dlc config file to base project folder dlc_cfg_filepath = dlc_reader . save_yaml ( project_path , dlc_config ) # ---- Trigger DLC model training job ---- train_network_input_args = list ( inspect . signature ( train_network ) . parameters ) train_network_kwargs = { k : v for k , v in dlc_config . items () if k in train_network_input_args } for k in [ \"shuffle\" , \"trainingsetindex\" , \"maxiters\" ]: train_network_kwargs [ k ] = int ( train_network_kwargs [ k ]) try : train_network ( dlc_cfg_filepath , ** train_network_kwargs ) except KeyboardInterrupt : # Instructions indicate to train until interrupt print ( \"DLC training stopped via Keyboard Interrupt\" ) snapshots = list ( ( project_path / get_model_folder ( trainFraction = dlc_config [ \"train_fraction\" ], shuffle = dlc_config [ \"shuffle\" ], cfg = dlc_config , modelprefix = dlc_config [ \"modelprefix\" ], ) / \"train\" ) . glob ( \"*index*\" ) ) max_modified_time = 0 # DLC goes by snapshot magnitude when judging 'latest' for evaluation # Here, we mean most recently generated for snapshot in snapshots : modified_time = os . path . getmtime ( snapshot ) if modified_time > max_modified_time : latest_snapshot = int ( snapshot . stem [ 9 :]) max_modified_time = modified_time self . insert1 ( { ** key , \"latest_snapshot\" : latest_snapshot , \"config_template\" : dlc_config } ) make ( key ) \u00b6 Launch training for each train.TrainingTask training_id via .populate() . Source code in element_deeplabcut/train.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def make ( self , key ): \"\"\"Launch training for each train.TrainingTask training_id via `.populate()`.\"\"\" project_path , model_prefix = ( TrainingTask & key ) . fetch1 ( \"project_path\" , \"model_prefix\" ) project_path = find_full_path ( get_dlc_root_data_dir (), project_path ) # ---- Build and save DLC configuration (yaml) file ---- _ , dlc_config = dlc_reader . read_yaml ( project_path ) # load existing dlc_config . update (( TrainingParamSet & key ) . fetch1 ( \"params\" )) dlc_config . update ( { \"project_path\" : project_path . as_posix (), \"modelprefix\" : model_prefix , \"train_fraction\" : dlc_config [ \"TrainingFraction\" ][ int ( dlc_config [ \"trainingsetindex\" ]) ], \"training_filelist_datajoint\" : [ # don't overwrite origin video_sets find_full_path ( get_dlc_root_data_dir (), fp ) . as_posix () for fp in ( VideoSet . File & key ) . fetch ( \"file_path\" ) ], } ) # Write dlc config file to base project folder dlc_cfg_filepath = dlc_reader . save_yaml ( project_path , dlc_config ) # ---- Trigger DLC model training job ---- train_network_input_args = list ( inspect . signature ( train_network ) . parameters ) train_network_kwargs = { k : v for k , v in dlc_config . items () if k in train_network_input_args } for k in [ \"shuffle\" , \"trainingsetindex\" , \"maxiters\" ]: train_network_kwargs [ k ] = int ( train_network_kwargs [ k ]) try : train_network ( dlc_cfg_filepath , ** train_network_kwargs ) except KeyboardInterrupt : # Instructions indicate to train until interrupt print ( \"DLC training stopped via Keyboard Interrupt\" ) snapshots = list ( ( project_path / get_model_folder ( trainFraction = dlc_config [ \"train_fraction\" ], shuffle = dlc_config [ \"shuffle\" ], cfg = dlc_config , modelprefix = dlc_config [ \"modelprefix\" ], ) / \"train\" ) . glob ( \"*index*\" ) ) max_modified_time = 0 # DLC goes by snapshot magnitude when judging 'latest' for evaluation # Here, we mean most recently generated for snapshot in snapshots : modified_time = os . path . getmtime ( snapshot ) if modified_time > max_modified_time : latest_snapshot = int ( snapshot . stem [ 9 :]) max_modified_time = modified_time self . insert1 ( { ** key , \"latest_snapshot\" : latest_snapshot , \"config_template\" : dlc_config } )", "title": "train.py"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.activate", "text": "Activate this schema. Parameters: Name Type Description Default train_schema_name str schema name on the database server required create_schema bool when True (default), create schema in the database if it does not yet exist. True create_tables str when True (default), create schema tables in the database if they do not yet exist. True linking_module str a module (or name) containing the required dependencies. None Dependencies: Functions get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder. Source code in element_deeplabcut/train.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def activate ( train_schema_name : str , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None ): \"\"\"Activate this schema. Args: train_schema_name (str): schema name on the database server create_schema (bool): when True (default), create schema in the database if it does not yet exist. create_tables (str): when True (default), create schema tables in the database if they do not yet exist. linking_module (str): a module (or name) containing the required dependencies. Dependencies: Functions: get_dlc_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s). get_dlc_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" assert hasattr ( linking_module , \"get_dlc_root_data_dir\" ), \"The linking module must specify a lookup funtion for a root data directory\" global _linking_module _linking_module = linking_module # activate schema . activate ( train_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , )", "title": "activate()"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.get_dlc_root_data_dir", "text": "Pulls relevant func from parent namespace to specify root data dir(s). It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strongs for possible root data directories. Source code in element_deeplabcut/train.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_dlc_root_data_dir () -> list : \"\"\"Pulls relevant func from parent namespace to specify root data dir(s). It is recommended that all paths in DataJoint Elements stored as relative paths, with respect to some user-configured \"root\" director(y/ies). The root(s) may vary between data modalities and user machines. Returns a full path string or list of strongs for possible root data directories. \"\"\" root_directories = _linking_module . get_dlc_root_data_dir () if isinstance ( root_directories , ( str , Path )): root_directories = [ root_directories ] if ( hasattr ( _linking_module , \"get_dlc_processed_data_dir\" ) and get_dlc_processed_data_dir () not in root_directories ): root_directories . append ( _linking_module . get_dlc_processed_data_dir ()) return root_directories", "title": "get_dlc_root_data_dir()"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.get_dlc_processed_data_dir", "text": "Pulls relevant func from parent namespace. Defaults to DLC's project /videos/. Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default. Source code in element_deeplabcut/train.py 99 100 101 102 103 104 105 106 107 108 109 def get_dlc_processed_data_dir () -> str : \"\"\"Pulls relevant func from parent namespace. Defaults to DLC's project /videos/. Method in parent namespace should provide a string to a directory where DLC output files will be stored. If unspecified, output files will be stored in the session directory 'videos' folder, per DeepLabCut default. \"\"\" if hasattr ( _linking_module , \"get_dlc_processed_data_dir\" ): return _linking_module . get_dlc_processed_data_dir () else : return get_dlc_root_data_dir ()[ 0 ]", "title": "get_dlc_processed_data_dir()"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.VideoSet", "text": "Bases: dj . Manual Collection of videos included in a given training set. Attributes: Name Type Description video_set_id int Unique ID for collection of videos. Source code in element_deeplabcut/train.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @schema class VideoSet ( dj . Manual ): \"\"\"Collection of videos included in a given training set. Attributes: video_set_id (int): Unique ID for collection of videos.\"\"\" definition = \"\"\" # Set of vids in training set video_set_id: int \"\"\" class File ( dj . Part ): \"\"\"File IDs and paths in a given VideoSet Attributes: VideoSet (foreign key): VideoSet key. file_path ( varchar(255) ): Path to file on disk relative to root.\"\"\" definition = \"\"\" # Paths of training files (e.g., labeled pngs, CSV or video) -> master file_id: int --- file_path: varchar(255) \"\"\"", "title": "VideoSet"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.VideoSet.File", "text": "Bases: dj . Part File IDs and paths in a given VideoSet Attributes: Name Type Description VideoSet foreign key VideoSet key. file_path varchar(255) Path to file on disk relative to root. Source code in element_deeplabcut/train.py 126 127 128 129 130 131 132 133 134 135 136 137 138 class File ( dj . Part ): \"\"\"File IDs and paths in a given VideoSet Attributes: VideoSet (foreign key): VideoSet key. file_path ( varchar(255) ): Path to file on disk relative to root.\"\"\" definition = \"\"\" # Paths of training files (e.g., labeled pngs, CSV or video) -> master file_id: int --- file_path: varchar(255) \"\"\"", "title": "File"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.TrainingParamSet", "text": "Bases: dj . Lookup Parameters used to train a model Attributes: Name Type Description paramset_idx smallint Index uniqely identifying paramset. paramset_desc varchar(128) Description of paramset. param_set_hash uuid Hash identifying this paramset. params longblob Dictionary of all applicable parameters. Note longblob param_set_hash must be unique. Source code in element_deeplabcut/train.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 @schema class TrainingParamSet ( dj . Lookup ): \"\"\"Parameters used to train a model Attributes: paramset_idx (smallint): Index uniqely identifying paramset. paramset_desc ( varchar(128) ): Description of paramset. param_set_hash (uuid): Hash identifying this paramset. params (longblob): Dictionary of all applicable parameters. Note: param_set_hash must be unique.\"\"\" definition = \"\"\" # Parameters to specify a DLC model training instance # For DLC \u2264 v2.0, include scorer_lecacy = True in params paramset_idx : smallint --- paramset_desc: varchar(128) param_set_hash : uuid # hash identifying this parameterset unique index (param_set_hash) params : longblob # dictionary of all applicable parameters \"\"\" required_parameters = ( \"shuffle\" , \"trainingsetindex\" ) skipped_parameters = ( \"project_path\" , \"video_sets\" ) @classmethod def insert_new_params ( cls , paramset_desc : str , params : dict , paramset_idx : int = None ): \"\"\" Insert a new set of training parameters into dlc.TrainingParamSet. Args: paramset_desc (str): Description of parameter set to be inserted params (dict): Dictionary including all settings to specify model training. Must include shuffle & trainingsetindex b/c not in config.yaml. project_path and video_sets will be overwritten by config.yaml. Note that trainingsetindex is 0-indexed paramset_idx (int): optional, integer to represent parameters. \"\"\" for required_param in cls . required_parameters : assert required_param in params , ( \"Missing required parameter: \" + required_param ) for skipped_param in cls . skipped_parameters : if skipped_param in params : params . pop ( skipped_param ) if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} # If the specified param-set already exists if param_query : existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if existing_paramset_idx == int ( paramset_idx ): # If existing_idx same: return # job done else : cls . insert1 ( param_dict ) # if duplicate, will raise duplicate error", "title": "TrainingParamSet"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.TrainingParamSet.insert_new_params", "text": "Insert a new set of training parameters into dlc.TrainingParamSet. Parameters: Name Type Description Default paramset_desc str Description of parameter set to be inserted required params dict Dictionary including all settings to specify model training. Must include shuffle & trainingsetindex b/c not in config.yaml. project_path and video_sets will be overwritten by config.yaml. Note that trainingsetindex is 0-indexed required paramset_idx int optional, integer to represent parameters. None Source code in element_deeplabcut/train.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 @classmethod def insert_new_params ( cls , paramset_desc : str , params : dict , paramset_idx : int = None ): \"\"\" Insert a new set of training parameters into dlc.TrainingParamSet. Args: paramset_desc (str): Description of parameter set to be inserted params (dict): Dictionary including all settings to specify model training. Must include shuffle & trainingsetindex b/c not in config.yaml. project_path and video_sets will be overwritten by config.yaml. Note that trainingsetindex is 0-indexed paramset_idx (int): optional, integer to represent parameters. \"\"\" for required_param in cls . required_parameters : assert required_param in params , ( \"Missing required parameter: \" + required_param ) for skipped_param in cls . skipped_parameters : if skipped_param in params : params . pop ( skipped_param ) if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} # If the specified param-set already exists if param_query : existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if existing_paramset_idx == int ( paramset_idx ): # If existing_idx same: return # job done else : cls . insert1 ( param_dict ) # if duplicate, will raise duplicate error", "title": "insert_new_params()"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.TrainingTask", "text": "Bases: dj . Manual Staging table for pairing videosets and training parameter sets Attributes: Name Type Description VideoSet foreign key VideoSet Key. TrainingParamSet foreign key TrainingParamSet key. training_id int Unique ID for training task. model_prefix varchar(32) Optional. Prefix for model files. project_path varchar(255) Optional. DLC's project_path in config relative to get_dlc_root_data_dir Source code in element_deeplabcut/train.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 @schema class TrainingTask ( dj . Manual ): \"\"\"Staging table for pairing videosets and training parameter sets Attributes: VideoSet (foreign key): VideoSet Key. TrainingParamSet (foreign key): TrainingParamSet key. training_id (int): Unique ID for training task. model_prefix ( varchar(32) ): Optional. Prefix for model files. project_path ( varchar(255) ): Optional. DLC's project_path in config relative to get_dlc_root_data_dir \"\"\" definition = \"\"\" # Specification for a DLC model training instance -> VideoSet # labeled video(s) for training -> TrainingParamSet training_id : int --- model_prefix='' : varchar(32) project_path='' : varchar(255) # DLC's project_path in config relative to root \"\"\"", "title": "TrainingTask"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.ModelTraining", "text": "Bases: dj . Computed Automated Model training information. Attributes: Name Type Description TrainingTask foreign key TrainingTask key. latest_snapshot int unsigned Latest exact snapshot index (i.e., never -1). config_template longblob Stored full config file. Source code in element_deeplabcut/train.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 @schema class ModelTraining ( dj . Computed ): \"\"\"Automated Model training information. Attributes: TrainingTask (foreign key): TrainingTask key. latest_snapshot (int unsigned): Latest exact snapshot index (i.e., never -1). config_template (longblob): Stored full config file.\"\"\" definition = \"\"\" -> TrainingTask --- latest_snapshot: int unsigned # latest exact snapshot index (i.e., never -1) config_template: longblob # stored full config file \"\"\" # To continue from previous training snapshot, devs suggest editing pose_cfg.yml # https://github.com/DeepLabCut/DeepLabCut/issues/70 def make ( self , key ): \"\"\"Launch training for each train.TrainingTask training_id via `.populate()`.\"\"\" project_path , model_prefix = ( TrainingTask & key ) . fetch1 ( \"project_path\" , \"model_prefix\" ) project_path = find_full_path ( get_dlc_root_data_dir (), project_path ) # ---- Build and save DLC configuration (yaml) file ---- _ , dlc_config = dlc_reader . read_yaml ( project_path ) # load existing dlc_config . update (( TrainingParamSet & key ) . fetch1 ( \"params\" )) dlc_config . update ( { \"project_path\" : project_path . as_posix (), \"modelprefix\" : model_prefix , \"train_fraction\" : dlc_config [ \"TrainingFraction\" ][ int ( dlc_config [ \"trainingsetindex\" ]) ], \"training_filelist_datajoint\" : [ # don't overwrite origin video_sets find_full_path ( get_dlc_root_data_dir (), fp ) . as_posix () for fp in ( VideoSet . File & key ) . fetch ( \"file_path\" ) ], } ) # Write dlc config file to base project folder dlc_cfg_filepath = dlc_reader . save_yaml ( project_path , dlc_config ) # ---- Trigger DLC model training job ---- train_network_input_args = list ( inspect . signature ( train_network ) . parameters ) train_network_kwargs = { k : v for k , v in dlc_config . items () if k in train_network_input_args } for k in [ \"shuffle\" , \"trainingsetindex\" , \"maxiters\" ]: train_network_kwargs [ k ] = int ( train_network_kwargs [ k ]) try : train_network ( dlc_cfg_filepath , ** train_network_kwargs ) except KeyboardInterrupt : # Instructions indicate to train until interrupt print ( \"DLC training stopped via Keyboard Interrupt\" ) snapshots = list ( ( project_path / get_model_folder ( trainFraction = dlc_config [ \"train_fraction\" ], shuffle = dlc_config [ \"shuffle\" ], cfg = dlc_config , modelprefix = dlc_config [ \"modelprefix\" ], ) / \"train\" ) . glob ( \"*index*\" ) ) max_modified_time = 0 # DLC goes by snapshot magnitude when judging 'latest' for evaluation # Here, we mean most recently generated for snapshot in snapshots : modified_time = os . path . getmtime ( snapshot ) if modified_time > max_modified_time : latest_snapshot = int ( snapshot . stem [ 9 :]) max_modified_time = modified_time self . insert1 ( { ** key , \"latest_snapshot\" : latest_snapshot , \"config_template\" : dlc_config } )", "title": "ModelTraining"}, {"location": "api/element_deeplabcut/train/#element_deeplabcut.train.ModelTraining.make", "text": "Launch training for each train.TrainingTask training_id via .populate() . Source code in element_deeplabcut/train.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def make ( self , key ): \"\"\"Launch training for each train.TrainingTask training_id via `.populate()`.\"\"\" project_path , model_prefix = ( TrainingTask & key ) . fetch1 ( \"project_path\" , \"model_prefix\" ) project_path = find_full_path ( get_dlc_root_data_dir (), project_path ) # ---- Build and save DLC configuration (yaml) file ---- _ , dlc_config = dlc_reader . read_yaml ( project_path ) # load existing dlc_config . update (( TrainingParamSet & key ) . fetch1 ( \"params\" )) dlc_config . update ( { \"project_path\" : project_path . as_posix (), \"modelprefix\" : model_prefix , \"train_fraction\" : dlc_config [ \"TrainingFraction\" ][ int ( dlc_config [ \"trainingsetindex\" ]) ], \"training_filelist_datajoint\" : [ # don't overwrite origin video_sets find_full_path ( get_dlc_root_data_dir (), fp ) . as_posix () for fp in ( VideoSet . File & key ) . fetch ( \"file_path\" ) ], } ) # Write dlc config file to base project folder dlc_cfg_filepath = dlc_reader . save_yaml ( project_path , dlc_config ) # ---- Trigger DLC model training job ---- train_network_input_args = list ( inspect . signature ( train_network ) . parameters ) train_network_kwargs = { k : v for k , v in dlc_config . items () if k in train_network_input_args } for k in [ \"shuffle\" , \"trainingsetindex\" , \"maxiters\" ]: train_network_kwargs [ k ] = int ( train_network_kwargs [ k ]) try : train_network ( dlc_cfg_filepath , ** train_network_kwargs ) except KeyboardInterrupt : # Instructions indicate to train until interrupt print ( \"DLC training stopped via Keyboard Interrupt\" ) snapshots = list ( ( project_path / get_model_folder ( trainFraction = dlc_config [ \"train_fraction\" ], shuffle = dlc_config [ \"shuffle\" ], cfg = dlc_config , modelprefix = dlc_config [ \"modelprefix\" ], ) / \"train\" ) . glob ( \"*index*\" ) ) max_modified_time = 0 # DLC goes by snapshot magnitude when judging 'latest' for evaluation # Here, we mean most recently generated for snapshot in snapshots : modified_time = os . path . getmtime ( snapshot ) if modified_time > max_modified_time : latest_snapshot = int ( snapshot . stem [ 9 :]) max_modified_time = modified_time self . insert1 ( { ** key , \"latest_snapshot\" : latest_snapshot , \"config_template\" : dlc_config } )", "title": "make()"}, {"location": "api/element_deeplabcut/version/", "text": "Package metadata", "title": "version.py"}, {"location": "api/element_deeplabcut/export/nwb/", "text": "Portions of code adapted from DeepLabCut/DLC2NWB MIT License Copyright (c) 2022 Alexander Mathis DataJoint export methods for DeepLabCut 2.x dlc_session_to_nwb ( keys , use_element_session = True , session_kwargs = None ) \u00b6 Using keys from PoseEstimation table, save DLC's h5 output to NWB. Calls DLC2NWB to export NWB file using current h5 on disk. If use_element_session, calls NWB export function from Elements for lab, animal and session, passing session_kwargs. Saves output based on naming convention in DLC2NWB. If output path already exists, returns output path without making changes to the file. NOTE: does not support multianimal exports Parameters: Name Type Description Default keys list One or more keys from model.PoseEstimation required use_element_session bool Optional. If True, call NWB export from Element Session True session_kwargs dict Optional. Additional keyword args for Element Session export None Returns: Type Description str Output path of saved file Source code in element_deeplabcut/export/nwb.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def dlc_session_to_nwb ( keys : list , use_element_session : bool = True , session_kwargs : dict = None ) -> str : \"\"\"Using keys from PoseEstimation table, save DLC's h5 output to NWB. Calls DLC2NWB to export NWB file using current h5 on disk. If use_element_session, calls NWB export function from Elements for lab, animal and session, passing session_kwargs. Saves output based on naming convention in DLC2NWB. If output path already exists, returns output path without making changes to the file. NOTE: does not support multianimal exports Args: keys: One or more keys from model.PoseEstimation use_element_session: Optional. If True, call NWB export from Element Session session_kwargs: Optional. Additional keyword args for Element Session export Returns: Output path of saved file \"\"\" if not isinstance ( keys , abc . Sequence ): # Ensure list for following loop keys = [ keys ] for key in keys : write_file = True subject_id = key [ \"subject\" ] output_dir = model . PoseEstimationTask . infer_output_dir ( key ) config_file = str ( output_dir / \"dj_dlc_config.yaml\" ) video_name = Path (( model . VideoRecording . File & key ) . fetch1 ( \"file_path\" )) . stem h5file = next ( output_dir . glob ( f \" { video_name } *h5\" )) output_path = h5file . replace ( \".h5\" , f \"_ { subject_id } .nwb\" ) # DLC2NWB convention if Path ( output_path ) . exists (): logger . warning ( f \"Skipping { subject_id } . NWB already exists.\" ) write_file = False # Use standard DLC2NWB export if write_file and not use_element_session : output_path = convert_h5_to_nwb ( config_file , h5file , subject_id ) # Pass Element Session export items in export if write_file and use_element_session : from element_session.export.nwb import session_to_nwb session_nwb = session_to_nwb ( key , ** session_kwargs ) # call session export dlc_nwb = write_subject_to_nwb ( session_nwb , h5file , subject_id , config_file ) # warnings filter from DLC2NWB with warnings . catch_warnings (), NWBHDF5IO ( output_path , mode = \"w\" ) as io : warnings . filterwarnings ( \"ignore\" , category = DtypeConversionWarning ) io . write ( dlc_nwb ) return output_path", "title": "nwb.py"}, {"location": "api/element_deeplabcut/export/nwb/#element_deeplabcut.export.nwb.dlc_session_to_nwb", "text": "Using keys from PoseEstimation table, save DLC's h5 output to NWB. Calls DLC2NWB to export NWB file using current h5 on disk. If use_element_session, calls NWB export function from Elements for lab, animal and session, passing session_kwargs. Saves output based on naming convention in DLC2NWB. If output path already exists, returns output path without making changes to the file. NOTE: does not support multianimal exports Parameters: Name Type Description Default keys list One or more keys from model.PoseEstimation required use_element_session bool Optional. If True, call NWB export from Element Session True session_kwargs dict Optional. Additional keyword args for Element Session export None Returns: Type Description str Output path of saved file Source code in element_deeplabcut/export/nwb.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def dlc_session_to_nwb ( keys : list , use_element_session : bool = True , session_kwargs : dict = None ) -> str : \"\"\"Using keys from PoseEstimation table, save DLC's h5 output to NWB. Calls DLC2NWB to export NWB file using current h5 on disk. If use_element_session, calls NWB export function from Elements for lab, animal and session, passing session_kwargs. Saves output based on naming convention in DLC2NWB. If output path already exists, returns output path without making changes to the file. NOTE: does not support multianimal exports Args: keys: One or more keys from model.PoseEstimation use_element_session: Optional. If True, call NWB export from Element Session session_kwargs: Optional. Additional keyword args for Element Session export Returns: Output path of saved file \"\"\" if not isinstance ( keys , abc . Sequence ): # Ensure list for following loop keys = [ keys ] for key in keys : write_file = True subject_id = key [ \"subject\" ] output_dir = model . PoseEstimationTask . infer_output_dir ( key ) config_file = str ( output_dir / \"dj_dlc_config.yaml\" ) video_name = Path (( model . VideoRecording . File & key ) . fetch1 ( \"file_path\" )) . stem h5file = next ( output_dir . glob ( f \" { video_name } *h5\" )) output_path = h5file . replace ( \".h5\" , f \"_ { subject_id } .nwb\" ) # DLC2NWB convention if Path ( output_path ) . exists (): logger . warning ( f \"Skipping { subject_id } . NWB already exists.\" ) write_file = False # Use standard DLC2NWB export if write_file and not use_element_session : output_path = convert_h5_to_nwb ( config_file , h5file , subject_id ) # Pass Element Session export items in export if write_file and use_element_session : from element_session.export.nwb import session_to_nwb session_nwb = session_to_nwb ( key , ** session_kwargs ) # call session export dlc_nwb = write_subject_to_nwb ( session_nwb , h5file , subject_id , config_file ) # warnings filter from DLC2NWB with warnings . catch_warnings (), NWBHDF5IO ( output_path , mode = \"w\" ) as io : warnings . filterwarnings ( \"ignore\" , category = DtypeConversionWarning ) io . write ( dlc_nwb ) return output_path", "title": "dlc_session_to_nwb()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/", "text": "PoseEstimation \u00b6 Class for handling DLC pose estimation files. Source code in element_deeplabcut/readers/dlc_reader.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class PoseEstimation : \"\"\"Class for handling DLC pose estimation files.\"\"\" def __init__ ( self , dlc_dir : str = None , pkl_path : str = None , h5_path : str = None , yml_path : str = None , filename_prefix : str = \"\" , ): if dlc_dir is None : assert pkl_path and h5_path and yml_path , ( 'If \"dlc_dir\" is not provided, then pkl_path, h5_path, and yml_path ' + \"must be provided\" ) else : self . dlc_dir = Path ( dlc_dir ) assert self . dlc_dir . exists (), f \"Unable to find { dlc_dir } \" # meta file: pkl - info about this DLC run (input video, configuration, etc.) if pkl_path is None : pkl_paths = list ( self . dlc_dir . rglob ( f \" { filename_prefix } *meta.pickle\" )) assert len ( pkl_paths ) == 1 , ( \"Unable to find one unique .pickle file in: \" + f \" { dlc_dir } - Found: { len ( pkl_paths ) } \" ) self . pkl_path = pkl_paths [ 0 ] else : self . pkl_path = Path ( pkl_path ) assert self . pkl_path . exists () # data file: h5 - body part outputs from the DLC post estimation step if h5_path is None : h5_paths = list ( self . dlc_dir . rglob ( f \" { filename_prefix } *.h5\" )) assert len ( h5_paths ) == 1 , ( \"Unable to find one unique .h5 file in: \" + f \" { dlc_dir } - Found: { len ( h5_paths ) } \" ) self . h5_path = h5_paths [ 0 ] else : self . h5_path = Path ( h5_path ) assert self . h5_path . exists () assert ( self . pkl_path . stem == self . h5_path . stem + \"_meta\" ), f \"Mismatching h5 ( { self . h5_path . stem } ) and pickle { self . pkl_path . stem } \" # config file: yaml - configuration for invoking the DLC post estimation step if yml_path is None : yml_paths = list ( self . dlc_dir . glob ( f \" { filename_prefix } *.y*ml\" )) # If multiple, defer to the one we save. if len ( yml_paths ) > 1 : yml_paths = [ val for val in yml_paths if val . stem == \"dj_dlc_config\" ] assert len ( yml_paths ) == 1 , ( \"Unable to find one unique .yaml file in: \" + f \" { dlc_dir } - Found: { len ( yml_paths ) } \" ) self . yml_path = yml_paths [ 0 ] else : self . yml_path = Path ( yml_path ) assert self . yml_path . exists () self . _pkl = None self . _rawdata = None self . _yml = None self . _data = None train_idx = np . where ( ( np . array ( self . yml [ \"TrainingFraction\" ]) * 100 ) . astype ( int ) == int ( self . pkl [ \"training set fraction\" ] * 100 ) )[ 0 ][ 0 ] train_iter = int ( self . pkl [ \"Scorer\" ] . split ( \"_\" )[ - 1 ]) self . model = { \"Scorer\" : self . pkl [ \"Scorer\" ], \"Task\" : self . yml [ \"Task\" ], \"date\" : self . yml [ \"date\" ], \"iteration\" : self . pkl [ \"iteration (active-learning)\" ], \"shuffle\" : int ( re . search ( \"shuffle(\\d+)\" , self . pkl [ \"Scorer\" ]) . groups ()[ 0 ]), \"snapshotindex\" : self . yml [ \"snapshotindex\" ], \"trainingsetindex\" : train_idx , \"training_iteration\" : train_iter , } self . fps = self . pkl [ \"fps\" ] self . nframes = self . pkl [ \"nframes\" ] self . creation_time = self . h5_path . stat () . st_mtime @property def pkl ( self ): \"\"\"Pickle file contents\"\"\" if self . _pkl is None : with open ( self . pkl_path , \"rb\" ) as f : self . _pkl = pickle . load ( f ) return self . _pkl [ \"data\" ] @property # DLC aux_func has a read_config option, but it rewrites the proj path def yml ( self ): \"\"\"json-structured config.yaml file contents\"\"\" if self . _yml is None : with open ( self . yml_path , \"rb\" ) as f : self . _yml = yaml . safe_load ( f ) return self . _yml @property def rawdata ( self ): \"\"\"Raw data from h5 file\"\"\" if self . _rawdata is None : self . _rawdata = pd . read_hdf ( self . h5_path ) return self . _rawdata @property def data ( self ): \"\"\"Data from the h5 file, restructured as a dict\"\"\" if self . _data is None : self . _data = self . reformat_rawdata () return self . _data @property def df ( self ): \"\"\"Data as dataframe\"\"\" top_level = self . rawdata . columns . levels [ 0 ][ 0 ] return self . rawdata . get ( top_level ) @property def body_parts ( self ): \"\"\"Set of body parts present in data file\"\"\" return self . df . columns . levels [ 0 ] def reformat_rawdata ( self ): \"\"\"Transform raw h5 data into dict\"\"\" error_message = ( f \"Total frames from .h5 file ( { len ( self . rawdata ) } ) differs \" + f 'from .pickle ( { self . pkl [ \"nframes\" ] } )' ) assert len ( self . rawdata ) == self . pkl [ \"nframes\" ], error_message body_parts_position = {} for body_part in self . body_parts : body_parts_position [ body_part ] = { c : self . df . get ( body_part ) . get ( c ) . values for c in self . df . get ( body_part ) . columns } return body_parts_position pkl () property \u00b6 Pickle file contents Source code in element_deeplabcut/readers/dlc_reader.py 106 107 108 109 110 111 112 @property def pkl ( self ): \"\"\"Pickle file contents\"\"\" if self . _pkl is None : with open ( self . pkl_path , \"rb\" ) as f : self . _pkl = pickle . load ( f ) return self . _pkl [ \"data\" ] yml () property \u00b6 json-structured config.yaml file contents Source code in element_deeplabcut/readers/dlc_reader.py 114 115 116 117 118 119 120 @property # DLC aux_func has a read_config option, but it rewrites the proj path def yml ( self ): \"\"\"json-structured config.yaml file contents\"\"\" if self . _yml is None : with open ( self . yml_path , \"rb\" ) as f : self . _yml = yaml . safe_load ( f ) return self . _yml rawdata () property \u00b6 Raw data from h5 file Source code in element_deeplabcut/readers/dlc_reader.py 122 123 124 125 126 127 @property def rawdata ( self ): \"\"\"Raw data from h5 file\"\"\" if self . _rawdata is None : self . _rawdata = pd . read_hdf ( self . h5_path ) return self . _rawdata data () property \u00b6 Data from the h5 file, restructured as a dict Source code in element_deeplabcut/readers/dlc_reader.py 129 130 131 132 133 134 @property def data ( self ): \"\"\"Data from the h5 file, restructured as a dict\"\"\" if self . _data is None : self . _data = self . reformat_rawdata () return self . _data df () property \u00b6 Data as dataframe Source code in element_deeplabcut/readers/dlc_reader.py 136 137 138 139 140 @property def df ( self ): \"\"\"Data as dataframe\"\"\" top_level = self . rawdata . columns . levels [ 0 ][ 0 ] return self . rawdata . get ( top_level ) body_parts () property \u00b6 Set of body parts present in data file Source code in element_deeplabcut/readers/dlc_reader.py 142 143 144 145 @property def body_parts ( self ): \"\"\"Set of body parts present in data file\"\"\" return self . df . columns . levels [ 0 ] reformat_rawdata () \u00b6 Transform raw h5 data into dict Source code in element_deeplabcut/readers/dlc_reader.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def reformat_rawdata ( self ): \"\"\"Transform raw h5 data into dict\"\"\" error_message = ( f \"Total frames from .h5 file ( { len ( self . rawdata ) } ) differs \" + f 'from .pickle ( { self . pkl [ \"nframes\" ] } )' ) assert len ( self . rawdata ) == self . pkl [ \"nframes\" ], error_message body_parts_position = {} for body_part in self . body_parts : body_parts_position [ body_part ] = { c : self . df . get ( body_part ) . get ( c ) . values for c in self . df . get ( body_part ) . columns } return body_parts_position read_yaml ( fullpath , filename = '*' ) \u00b6 Return contents of yml in fullpath. If available, defer to DJ-saved version Parameters: Name Type Description Default fullpath str String or pathlib path. Directory with yaml files required filename str Filename, no extension. Permits wildcards. '*' Returns: Type Description tuple Tuple of (a) filepath as pathlib.PosixPath and (b) file contents as dict Source code in element_deeplabcut/readers/dlc_reader.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def read_yaml ( fullpath : str , filename : str = \"*\" ) -> tuple : \"\"\"Return contents of yml in fullpath. If available, defer to DJ-saved version Args: fullpath (str): String or pathlib path. Directory with yaml files filename (str, optional): Filename, no extension. Permits wildcards. Returns: Tuple of (a) filepath as pathlib.PosixPath and (b) file contents as dict \"\"\" from deeplabcut.utils.auxiliaryfunctions import read_config # Take the DJ-saved if there. If not, return list of available yml_paths = list ( Path ( fullpath ) . glob ( \"dj_dlc_config.yaml\" )) or sorted ( list ( Path ( fullpath ) . glob ( f \" { filename } .y*ml\" )) ) assert ( # If more than 1 and not DJ-saved, len ( yml_paths ) == 1 ), f \"Found more yaml files than expected: { len ( yml_paths ) } \\n { fullpath } \" return yml_paths [ 0 ], read_config ( yml_paths [ 0 ]) save_yaml ( output_dir , config_dict , filename = 'dj_dlc_config' , mkdir = True ) \u00b6 Save config_dict to output_path as filename.yaml. By default, preserves original. Parameters: Name Type Description Default output_dir str where to save yaml file required config_dict str dict of config params or element-deeplabcut model.Model dict required filename str default 'dj_dlc_config' or preserve original 'config' Set to 'config' to overwrite original file. If extension is included, removed and replaced with \"yaml\". 'dj_dlc_config' mkdir bool Optional, True. Make new directory if output_dir not exist True Returns: Type Description str path of saved file as string - due to DLC func preference for strings Source code in element_deeplabcut/readers/dlc_reader.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def save_yaml ( output_dir : str , config_dict : dict , filename : str = \"dj_dlc_config\" , mkdir : bool = True , ) -> str : \"\"\"Save config_dict to output_path as filename.yaml. By default, preserves original. Args: output_dir (str): where to save yaml file config_dict (str): dict of config params or element-deeplabcut model.Model dict filename (str, optional): default 'dj_dlc_config' or preserve original 'config' Set to 'config' to overwrite original file. If extension is included, removed and replaced with \"yaml\". mkdir (bool): Optional, True. Make new directory if output_dir not exist Returns: path of saved file as string - due to DLC func preference for strings \"\"\" from deeplabcut.utils.auxiliaryfunctions import write_config if \"config_template\" in config_dict : # if passed full model.Model dict config_dict = config_dict [ \"config_template\" ] if mkdir : output_dir . mkdir ( exist_ok = True ) if \".\" in filename : # if user provided extension, remove filename = filename . split ( \".\" )[ 0 ] output_filepath = Path ( output_dir ) / f \" { filename } .yaml\" write_config ( output_filepath , config_dict ) return str ( output_filepath ) do_pose_estimation ( video_filepaths , dlc_model , project_path , output_dir , videotype = '' , gputouse = None , save_as_csv = False , batchsize = None , cropping = None , TFGPUinference = True , dynamic = ( False , 0.5 , 10 ), robust_nframes = False , allow_growth = False , use_shelve = False ) \u00b6 Launch DLC's analyze_videos within element-deeplabcut. Also saves a copy of the current config in the output dir, with ensuring analyzed videos in the video_set. NOTE: Config-specificed cropping not supported when adding to config in this manner. Parameters: Name Type Description Default video_filepaths list list of videos to analyze required dlc_model dict element-deeplabcut dlc.Model required project_path str path to project config.yml required output_dir str where to save output BELOW FROM DLC'S DOCSTRING \u00b6 required videotype str, optional, default=\"\" Checks for the extension of the video in case the input to the video is a directory. Only videos with this extension are analyzed. If unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept. '' gputouse int or None, optional, default=None Indicates the GPU to use (see number in nvidia-smi ). If none, None . See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries None save_as_csv bool, optional, default=False Saves the predictions in a .csv file. False batchsize int or None, optional, default=None Change batch size for inference; if given overwrites pose_cfg.yaml None cropping list or None, optional, default=None List of cropping coordinates as [x1, x2, y1, y2]. Note that the same cropping parameters will then be used for all videos. If different video crops are desired, run analyze_videos on individual videos with the corresponding cropping coordinates. None TFGPUinference bool, optional, default=True Perform inference on GPU with TensorFlow code. Introduced in \"Pretraining boosts out-of-domain robustness for pose estimation\" by Alexander Mathis, Mert Y\u00fcksekg\u00f6n\u00fcl, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis. Source https://arxiv.org/abs/1909.11229 True dynamic tuple(bool, float, int) triple (state, detectiontreshold, margin If the state is true, then dynamic cropping will be performed. That means that if an object is detected (i.e. any body part > detectiontreshold), then object boundaries are computed according to the smallest/largest x position and smallest/largest y position of all body parts. This window is expanded by the margin and from then on only the posture within this crop is analyzed (until the object is lost, i.e. <detectiontreshold). The current position is utilized for updating the crop window for the next frame (this is why the margin is important and should be set large enough given the movement of the animal). (False, 0.5, 10) robust_nframes bool, optional, default=False Evaluate a video's number of frames in a robust manner. This option is slower (as the whole video is read frame-by-frame), but does not rely on metadata, hence its robustness against file corruption. False allow_growth bool, optional, default=False. For some smaller GPUs the memory issues happen. If True , the memory allocator does not pre-allocate the entire specified GPU memory region, instead starting small and growing as needed. See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2 False use_shelve bool, optional, default=False By default, data are dumped in a pickle file at the end of the video analysis. Otherwise, data are written to disk on the fly using a \"shelf\"; i.e., a pickle-based, persistent, database-like object by default, resulting in constant memory footprint. False Source code in element_deeplabcut/readers/dlc_reader.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 def do_pose_estimation ( video_filepaths : list , dlc_model : dict , project_path : str , output_dir : str , videotype = \"\" , gputouse = None , save_as_csv = False , batchsize = None , cropping = None , TFGPUinference = True , dynamic = ( False , 0.5 , 10 ), robust_nframes = False , allow_growth = False , use_shelve = False , ): \"\"\"Launch DLC's analyze_videos within element-deeplabcut. Also saves a copy of the current config in the output dir, with ensuring analyzed videos in the video_set. NOTE: Config-specificed cropping not supported when adding to config in this manner. Args: video_filepaths (list): list of videos to analyze dlc_model (dict): element-deeplabcut dlc.Model project_path (str): path to project config.yml output_dir (str): where to save output # BELOW FROM DLC'S DOCSTRING videotype (str, optional, default=\"\"): Checks for the extension of the video in case the input to the video is a directory. Only videos with this extension are analyzed. If unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept. gputouse (int or None, optional, default=None): Indicates the GPU to use (see number in ``nvidia-smi``). If none, ``None``. See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries save_as_csv (bool, optional, default=False): Saves the predictions in a .csv file. batchsize (int or None, optional, default=None): Change batch size for inference; if given overwrites ``pose_cfg.yaml`` cropping (list or None, optional, default=None): List of cropping coordinates as [x1, x2, y1, y2]. Note that the same cropping parameters will then be used for all videos. If different video crops are desired, run ``analyze_videos`` on individual videos with the corresponding cropping coordinates. TFGPUinference (bool, optional, default=True): Perform inference on GPU with TensorFlow code. Introduced in \"Pretraining boosts out-of-domain robustness for pose estimation\" by Alexander Mathis, Mert Y\u00fcksekg\u00f6n\u00fcl, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis. Source https://arxiv.org/abs/1909.11229 dynamic (tuple(bool, float, int) triple (state, detectiontreshold, margin)): If the state is true, then dynamic cropping will be performed. That means that if an object is detected (i.e. any body part > detectiontreshold), then object boundaries are computed according to the smallest/largest x position and smallest/largest y position of all body parts. This window is expanded by the margin and from then on only the posture within this crop is analyzed (until the object is lost, i.e. <detectiontreshold). The current position is utilized for updating the crop window for the next frame (this is why the margin is important and should be set large enough given the movement of the animal). robust_nframes (bool, optional, default=False): Evaluate a video's number of frames in a robust manner. This option is slower (as the whole video is read frame-by-frame), but does not rely on metadata, hence its robustness against file corruption. allow_growth (bool, optional, default=False.): For some smaller GPUs the memory issues happen. If ``True``, the memory allocator does not pre-allocate the entire specified GPU memory region, instead starting small and growing as needed. See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2 use_shelve (bool, optional, default=False): By default, data are dumped in a pickle file at the end of the video analysis. Otherwise, data are written to disk on the fly using a \"shelf\"; i.e., a pickle-based, persistent, database-like object by default, resulting in constant memory footprint. \"\"\" from deeplabcut.pose_estimation_tensorflow import analyze_videos # ---- Build and save DLC configuration (yaml) file ---- dlc_config = dlc_model [ \"config_template\" ] dlc_project_path = Path ( project_path ) dlc_config [ \"project_path\" ] = dlc_project_path . as_posix () # ---- Add current video to config --- for video_filepath in video_filepaths : if video_filepath not in dlc_config [ \"video_sets\" ]: root_dir = find_root_directory ( get_dlc_root_data_dir (), video_filepath ) relative_path = Path ( video_filepath ) . relative_to ( root_dir ) recording_id = ( model . VideoRecording . File & f 'file_path=\" { relative_path } \"' ) . fetch1 ( \"recording_id\" ) try : px_width , px_height = ( model . RecordingInfo & f 'recording_id=\" { recording_id } \"' ) . fetch1 ( \"px_width\" , \"px_height\" ) except DataJointError : logger . warn ( f \"Could not find RecordingInfo for { video_filepath . stem } \" + \" \\n\\t Using zeros for crop value in config.\" ) px_height , px_width = 0 , 0 dlc_config [ \"video_sets\" ] . update ( { str ( video_filepath ): { \"crop\" : f \"0, { px_width } , 0, { px_height } \" }} ) # ---- Write config files ---- # To output dir: Important for loading/parsing output in datajoint _ = save_yaml ( output_dir , dlc_config ) # To project dir: Required by DLC to run the analyze_videos if dlc_project_path != output_dir : config_filepath = save_yaml ( dlc_project_path , dlc_config ) # ---- Trigger DLC prediction job ---- analyze_videos ( config = config_filepath , videos = video_filepaths , shuffle = dlc_model [ \"shuffle\" ], trainingsetindex = dlc_model [ \"trainingsetindex\" ], destfolder = output_dir , modelprefix = dlc_model [ \"model_prefix\" ], videotype = videotype , gputouse = gputouse , save_as_csv = save_as_csv , batchsize = batchsize , cropping = cropping , TFGPUinference = TFGPUinference , dynamic = dynamic , robust_nframes = robust_nframes , allow_growth = allow_growth , use_shelve = use_shelve , )", "title": "dlc_reader.py"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation", "text": "Class for handling DLC pose estimation files. Source code in element_deeplabcut/readers/dlc_reader.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class PoseEstimation : \"\"\"Class for handling DLC pose estimation files.\"\"\" def __init__ ( self , dlc_dir : str = None , pkl_path : str = None , h5_path : str = None , yml_path : str = None , filename_prefix : str = \"\" , ): if dlc_dir is None : assert pkl_path and h5_path and yml_path , ( 'If \"dlc_dir\" is not provided, then pkl_path, h5_path, and yml_path ' + \"must be provided\" ) else : self . dlc_dir = Path ( dlc_dir ) assert self . dlc_dir . exists (), f \"Unable to find { dlc_dir } \" # meta file: pkl - info about this DLC run (input video, configuration, etc.) if pkl_path is None : pkl_paths = list ( self . dlc_dir . rglob ( f \" { filename_prefix } *meta.pickle\" )) assert len ( pkl_paths ) == 1 , ( \"Unable to find one unique .pickle file in: \" + f \" { dlc_dir } - Found: { len ( pkl_paths ) } \" ) self . pkl_path = pkl_paths [ 0 ] else : self . pkl_path = Path ( pkl_path ) assert self . pkl_path . exists () # data file: h5 - body part outputs from the DLC post estimation step if h5_path is None : h5_paths = list ( self . dlc_dir . rglob ( f \" { filename_prefix } *.h5\" )) assert len ( h5_paths ) == 1 , ( \"Unable to find one unique .h5 file in: \" + f \" { dlc_dir } - Found: { len ( h5_paths ) } \" ) self . h5_path = h5_paths [ 0 ] else : self . h5_path = Path ( h5_path ) assert self . h5_path . exists () assert ( self . pkl_path . stem == self . h5_path . stem + \"_meta\" ), f \"Mismatching h5 ( { self . h5_path . stem } ) and pickle { self . pkl_path . stem } \" # config file: yaml - configuration for invoking the DLC post estimation step if yml_path is None : yml_paths = list ( self . dlc_dir . glob ( f \" { filename_prefix } *.y*ml\" )) # If multiple, defer to the one we save. if len ( yml_paths ) > 1 : yml_paths = [ val for val in yml_paths if val . stem == \"dj_dlc_config\" ] assert len ( yml_paths ) == 1 , ( \"Unable to find one unique .yaml file in: \" + f \" { dlc_dir } - Found: { len ( yml_paths ) } \" ) self . yml_path = yml_paths [ 0 ] else : self . yml_path = Path ( yml_path ) assert self . yml_path . exists () self . _pkl = None self . _rawdata = None self . _yml = None self . _data = None train_idx = np . where ( ( np . array ( self . yml [ \"TrainingFraction\" ]) * 100 ) . astype ( int ) == int ( self . pkl [ \"training set fraction\" ] * 100 ) )[ 0 ][ 0 ] train_iter = int ( self . pkl [ \"Scorer\" ] . split ( \"_\" )[ - 1 ]) self . model = { \"Scorer\" : self . pkl [ \"Scorer\" ], \"Task\" : self . yml [ \"Task\" ], \"date\" : self . yml [ \"date\" ], \"iteration\" : self . pkl [ \"iteration (active-learning)\" ], \"shuffle\" : int ( re . search ( \"shuffle(\\d+)\" , self . pkl [ \"Scorer\" ]) . groups ()[ 0 ]), \"snapshotindex\" : self . yml [ \"snapshotindex\" ], \"trainingsetindex\" : train_idx , \"training_iteration\" : train_iter , } self . fps = self . pkl [ \"fps\" ] self . nframes = self . pkl [ \"nframes\" ] self . creation_time = self . h5_path . stat () . st_mtime @property def pkl ( self ): \"\"\"Pickle file contents\"\"\" if self . _pkl is None : with open ( self . pkl_path , \"rb\" ) as f : self . _pkl = pickle . load ( f ) return self . _pkl [ \"data\" ] @property # DLC aux_func has a read_config option, but it rewrites the proj path def yml ( self ): \"\"\"json-structured config.yaml file contents\"\"\" if self . _yml is None : with open ( self . yml_path , \"rb\" ) as f : self . _yml = yaml . safe_load ( f ) return self . _yml @property def rawdata ( self ): \"\"\"Raw data from h5 file\"\"\" if self . _rawdata is None : self . _rawdata = pd . read_hdf ( self . h5_path ) return self . _rawdata @property def data ( self ): \"\"\"Data from the h5 file, restructured as a dict\"\"\" if self . _data is None : self . _data = self . reformat_rawdata () return self . _data @property def df ( self ): \"\"\"Data as dataframe\"\"\" top_level = self . rawdata . columns . levels [ 0 ][ 0 ] return self . rawdata . get ( top_level ) @property def body_parts ( self ): \"\"\"Set of body parts present in data file\"\"\" return self . df . columns . levels [ 0 ] def reformat_rawdata ( self ): \"\"\"Transform raw h5 data into dict\"\"\" error_message = ( f \"Total frames from .h5 file ( { len ( self . rawdata ) } ) differs \" + f 'from .pickle ( { self . pkl [ \"nframes\" ] } )' ) assert len ( self . rawdata ) == self . pkl [ \"nframes\" ], error_message body_parts_position = {} for body_part in self . body_parts : body_parts_position [ body_part ] = { c : self . df . get ( body_part ) . get ( c ) . values for c in self . df . get ( body_part ) . columns } return body_parts_position", "title": "PoseEstimation"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.pkl", "text": "Pickle file contents Source code in element_deeplabcut/readers/dlc_reader.py 106 107 108 109 110 111 112 @property def pkl ( self ): \"\"\"Pickle file contents\"\"\" if self . _pkl is None : with open ( self . pkl_path , \"rb\" ) as f : self . _pkl = pickle . load ( f ) return self . _pkl [ \"data\" ]", "title": "pkl()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.yml", "text": "json-structured config.yaml file contents Source code in element_deeplabcut/readers/dlc_reader.py 114 115 116 117 118 119 120 @property # DLC aux_func has a read_config option, but it rewrites the proj path def yml ( self ): \"\"\"json-structured config.yaml file contents\"\"\" if self . _yml is None : with open ( self . yml_path , \"rb\" ) as f : self . _yml = yaml . safe_load ( f ) return self . _yml", "title": "yml()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.rawdata", "text": "Raw data from h5 file Source code in element_deeplabcut/readers/dlc_reader.py 122 123 124 125 126 127 @property def rawdata ( self ): \"\"\"Raw data from h5 file\"\"\" if self . _rawdata is None : self . _rawdata = pd . read_hdf ( self . h5_path ) return self . _rawdata", "title": "rawdata()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.data", "text": "Data from the h5 file, restructured as a dict Source code in element_deeplabcut/readers/dlc_reader.py 129 130 131 132 133 134 @property def data ( self ): \"\"\"Data from the h5 file, restructured as a dict\"\"\" if self . _data is None : self . _data = self . reformat_rawdata () return self . _data", "title": "data()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.df", "text": "Data as dataframe Source code in element_deeplabcut/readers/dlc_reader.py 136 137 138 139 140 @property def df ( self ): \"\"\"Data as dataframe\"\"\" top_level = self . rawdata . columns . levels [ 0 ][ 0 ] return self . rawdata . get ( top_level )", "title": "df()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.body_parts", "text": "Set of body parts present in data file Source code in element_deeplabcut/readers/dlc_reader.py 142 143 144 145 @property def body_parts ( self ): \"\"\"Set of body parts present in data file\"\"\" return self . df . columns . levels [ 0 ]", "title": "body_parts()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.PoseEstimation.reformat_rawdata", "text": "Transform raw h5 data into dict Source code in element_deeplabcut/readers/dlc_reader.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def reformat_rawdata ( self ): \"\"\"Transform raw h5 data into dict\"\"\" error_message = ( f \"Total frames from .h5 file ( { len ( self . rawdata ) } ) differs \" + f 'from .pickle ( { self . pkl [ \"nframes\" ] } )' ) assert len ( self . rawdata ) == self . pkl [ \"nframes\" ], error_message body_parts_position = {} for body_part in self . body_parts : body_parts_position [ body_part ] = { c : self . df . get ( body_part ) . get ( c ) . values for c in self . df . get ( body_part ) . columns } return body_parts_position", "title": "reformat_rawdata()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.read_yaml", "text": "Return contents of yml in fullpath. If available, defer to DJ-saved version Parameters: Name Type Description Default fullpath str String or pathlib path. Directory with yaml files required filename str Filename, no extension. Permits wildcards. '*' Returns: Type Description tuple Tuple of (a) filepath as pathlib.PosixPath and (b) file contents as dict Source code in element_deeplabcut/readers/dlc_reader.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def read_yaml ( fullpath : str , filename : str = \"*\" ) -> tuple : \"\"\"Return contents of yml in fullpath. If available, defer to DJ-saved version Args: fullpath (str): String or pathlib path. Directory with yaml files filename (str, optional): Filename, no extension. Permits wildcards. Returns: Tuple of (a) filepath as pathlib.PosixPath and (b) file contents as dict \"\"\" from deeplabcut.utils.auxiliaryfunctions import read_config # Take the DJ-saved if there. If not, return list of available yml_paths = list ( Path ( fullpath ) . glob ( \"dj_dlc_config.yaml\" )) or sorted ( list ( Path ( fullpath ) . glob ( f \" { filename } .y*ml\" )) ) assert ( # If more than 1 and not DJ-saved, len ( yml_paths ) == 1 ), f \"Found more yaml files than expected: { len ( yml_paths ) } \\n { fullpath } \" return yml_paths [ 0 ], read_config ( yml_paths [ 0 ])", "title": "read_yaml()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.save_yaml", "text": "Save config_dict to output_path as filename.yaml. By default, preserves original. Parameters: Name Type Description Default output_dir str where to save yaml file required config_dict str dict of config params or element-deeplabcut model.Model dict required filename str default 'dj_dlc_config' or preserve original 'config' Set to 'config' to overwrite original file. If extension is included, removed and replaced with \"yaml\". 'dj_dlc_config' mkdir bool Optional, True. Make new directory if output_dir not exist True Returns: Type Description str path of saved file as string - due to DLC func preference for strings Source code in element_deeplabcut/readers/dlc_reader.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def save_yaml ( output_dir : str , config_dict : dict , filename : str = \"dj_dlc_config\" , mkdir : bool = True , ) -> str : \"\"\"Save config_dict to output_path as filename.yaml. By default, preserves original. Args: output_dir (str): where to save yaml file config_dict (str): dict of config params or element-deeplabcut model.Model dict filename (str, optional): default 'dj_dlc_config' or preserve original 'config' Set to 'config' to overwrite original file. If extension is included, removed and replaced with \"yaml\". mkdir (bool): Optional, True. Make new directory if output_dir not exist Returns: path of saved file as string - due to DLC func preference for strings \"\"\" from deeplabcut.utils.auxiliaryfunctions import write_config if \"config_template\" in config_dict : # if passed full model.Model dict config_dict = config_dict [ \"config_template\" ] if mkdir : output_dir . mkdir ( exist_ok = True ) if \".\" in filename : # if user provided extension, remove filename = filename . split ( \".\" )[ 0 ] output_filepath = Path ( output_dir ) / f \" { filename } .yaml\" write_config ( output_filepath , config_dict ) return str ( output_filepath )", "title": "save_yaml()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.do_pose_estimation", "text": "Launch DLC's analyze_videos within element-deeplabcut. Also saves a copy of the current config in the output dir, with ensuring analyzed videos in the video_set. NOTE: Config-specificed cropping not supported when adding to config in this manner. Parameters: Name Type Description Default video_filepaths list list of videos to analyze required dlc_model dict element-deeplabcut dlc.Model required project_path str path to project config.yml required output_dir str where to save output", "title": "do_pose_estimation()"}, {"location": "api/element_deeplabcut/readers/dlc_reader/#element_deeplabcut.readers.dlc_reader.do_pose_estimation--below-from-dlcs-docstring", "text": "required videotype str, optional, default=\"\" Checks for the extension of the video in case the input to the video is a directory. Only videos with this extension are analyzed. If unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept. '' gputouse int or None, optional, default=None Indicates the GPU to use (see number in nvidia-smi ). If none, None . See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries None save_as_csv bool, optional, default=False Saves the predictions in a .csv file. False batchsize int or None, optional, default=None Change batch size for inference; if given overwrites pose_cfg.yaml None cropping list or None, optional, default=None List of cropping coordinates as [x1, x2, y1, y2]. Note that the same cropping parameters will then be used for all videos. If different video crops are desired, run analyze_videos on individual videos with the corresponding cropping coordinates. None TFGPUinference bool, optional, default=True Perform inference on GPU with TensorFlow code. Introduced in \"Pretraining boosts out-of-domain robustness for pose estimation\" by Alexander Mathis, Mert Y\u00fcksekg\u00f6n\u00fcl, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis. Source https://arxiv.org/abs/1909.11229 True dynamic tuple(bool, float, int) triple (state, detectiontreshold, margin If the state is true, then dynamic cropping will be performed. That means that if an object is detected (i.e. any body part > detectiontreshold), then object boundaries are computed according to the smallest/largest x position and smallest/largest y position of all body parts. This window is expanded by the margin and from then on only the posture within this crop is analyzed (until the object is lost, i.e. <detectiontreshold). The current position is utilized for updating the crop window for the next frame (this is why the margin is important and should be set large enough given the movement of the animal). (False, 0.5, 10) robust_nframes bool, optional, default=False Evaluate a video's number of frames in a robust manner. This option is slower (as the whole video is read frame-by-frame), but does not rely on metadata, hence its robustness against file corruption. False allow_growth bool, optional, default=False. For some smaller GPUs the memory issues happen. If True , the memory allocator does not pre-allocate the entire specified GPU memory region, instead starting small and growing as needed. See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2 False use_shelve bool, optional, default=False By default, data are dumped in a pickle file at the end of the video analysis. Otherwise, data are written to disk on the fly using a \"shelf\"; i.e., a pickle-based, persistent, database-like object by default, resulting in constant memory footprint. False Source code in element_deeplabcut/readers/dlc_reader.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 def do_pose_estimation ( video_filepaths : list , dlc_model : dict , project_path : str , output_dir : str , videotype = \"\" , gputouse = None , save_as_csv = False , batchsize = None , cropping = None , TFGPUinference = True , dynamic = ( False , 0.5 , 10 ), robust_nframes = False , allow_growth = False , use_shelve = False , ): \"\"\"Launch DLC's analyze_videos within element-deeplabcut. Also saves a copy of the current config in the output dir, with ensuring analyzed videos in the video_set. NOTE: Config-specificed cropping not supported when adding to config in this manner. Args: video_filepaths (list): list of videos to analyze dlc_model (dict): element-deeplabcut dlc.Model project_path (str): path to project config.yml output_dir (str): where to save output # BELOW FROM DLC'S DOCSTRING videotype (str, optional, default=\"\"): Checks for the extension of the video in case the input to the video is a directory. Only videos with this extension are analyzed. If unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept. gputouse (int or None, optional, default=None): Indicates the GPU to use (see number in ``nvidia-smi``). If none, ``None``. See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries save_as_csv (bool, optional, default=False): Saves the predictions in a .csv file. batchsize (int or None, optional, default=None): Change batch size for inference; if given overwrites ``pose_cfg.yaml`` cropping (list or None, optional, default=None): List of cropping coordinates as [x1, x2, y1, y2]. Note that the same cropping parameters will then be used for all videos. If different video crops are desired, run ``analyze_videos`` on individual videos with the corresponding cropping coordinates. TFGPUinference (bool, optional, default=True): Perform inference on GPU with TensorFlow code. Introduced in \"Pretraining boosts out-of-domain robustness for pose estimation\" by Alexander Mathis, Mert Y\u00fcksekg\u00f6n\u00fcl, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis. Source https://arxiv.org/abs/1909.11229 dynamic (tuple(bool, float, int) triple (state, detectiontreshold, margin)): If the state is true, then dynamic cropping will be performed. That means that if an object is detected (i.e. any body part > detectiontreshold), then object boundaries are computed according to the smallest/largest x position and smallest/largest y position of all body parts. This window is expanded by the margin and from then on only the posture within this crop is analyzed (until the object is lost, i.e. <detectiontreshold). The current position is utilized for updating the crop window for the next frame (this is why the margin is important and should be set large enough given the movement of the animal). robust_nframes (bool, optional, default=False): Evaluate a video's number of frames in a robust manner. This option is slower (as the whole video is read frame-by-frame), but does not rely on metadata, hence its robustness against file corruption. allow_growth (bool, optional, default=False.): For some smaller GPUs the memory issues happen. If ``True``, the memory allocator does not pre-allocate the entire specified GPU memory region, instead starting small and growing as needed. See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2 use_shelve (bool, optional, default=False): By default, data are dumped in a pickle file at the end of the video analysis. Otherwise, data are written to disk on the fly using a \"shelf\"; i.e., a pickle-based, persistent, database-like object by default, resulting in constant memory footprint. \"\"\" from deeplabcut.pose_estimation_tensorflow import analyze_videos # ---- Build and save DLC configuration (yaml) file ---- dlc_config = dlc_model [ \"config_template\" ] dlc_project_path = Path ( project_path ) dlc_config [ \"project_path\" ] = dlc_project_path . as_posix () # ---- Add current video to config --- for video_filepath in video_filepaths : if video_filepath not in dlc_config [ \"video_sets\" ]: root_dir = find_root_directory ( get_dlc_root_data_dir (), video_filepath ) relative_path = Path ( video_filepath ) . relative_to ( root_dir ) recording_id = ( model . VideoRecording . File & f 'file_path=\" { relative_path } \"' ) . fetch1 ( \"recording_id\" ) try : px_width , px_height = ( model . RecordingInfo & f 'recording_id=\" { recording_id } \"' ) . fetch1 ( \"px_width\" , \"px_height\" ) except DataJointError : logger . warn ( f \"Could not find RecordingInfo for { video_filepath . stem } \" + \" \\n\\t Using zeros for crop value in config.\" ) px_height , px_width = 0 , 0 dlc_config [ \"video_sets\" ] . update ( { str ( video_filepath ): { \"crop\" : f \"0, { px_width } , 0, { px_height } \" }} ) # ---- Write config files ---- # To output dir: Important for loading/parsing output in datajoint _ = save_yaml ( output_dir , dlc_config ) # To project dir: Required by DLC to run the analyze_videos if dlc_project_path != output_dir : config_filepath = save_yaml ( dlc_project_path , dlc_config ) # ---- Trigger DLC prediction job ---- analyze_videos ( config = config_filepath , videos = video_filepaths , shuffle = dlc_model [ \"shuffle\" ], trainingsetindex = dlc_model [ \"trainingsetindex\" ], destfolder = output_dir , modelprefix = dlc_model [ \"model_prefix\" ], videotype = videotype , gputouse = gputouse , save_as_csv = save_as_csv , batchsize = batchsize , cropping = cropping , TFGPUinference = TFGPUinference , dynamic = dynamic , robust_nframes = robust_nframes , allow_growth = allow_growth , use_shelve = use_shelve , )", "title": "BELOW FROM DLC'S DOCSTRING"}, {"location": "api/workflow_deeplabcut/ingest/", "text": "ingest_subjects ( subject_csv_path = './user_data/subjects.csv' , skip_duplicates = True , verbose = True ) \u00b6 Inserts ./user_data/subject.csv data into corresponding subject schema tables Parameters: Name Type Description Default subject_csv_path str relative path of subject csv './user_data/subjects.csv' skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Display number of entries inserted when ingesting True Source code in workflow_deeplabcut/ingest.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def ingest_subjects ( subject_csv_path = \"./user_data/subjects.csv\" , skip_duplicates = True , verbose = True , ): \"\"\"Inserts ./user_data/subject.csv data into corresponding subject schema tables Args: subject_csv_path (str): relative path of subject csv skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Display number of entries inserted when ingesting \"\"\" csvs = [ subject_csv_path ] tables = [ subject . Subject ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose ) ingest_sessions ( session_csv_path = './user_data/sessions.csv' , skip_duplicates = True , verbose = True ) \u00b6 Ingests to session schema from ./user_data/sessions.csv Parameters: Name Type Description Default session_csv_path str relative path of session csv './user_data/sessions.csv' skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Default True. Display number of entries inserted when ingesting True Source code in workflow_deeplabcut/ingest.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def ingest_sessions ( session_csv_path = \"./user_data/sessions.csv\" , skip_duplicates = True , verbose = True ): \"\"\"Ingests to session schema from ./user_data/sessions.csv Args: session_csv_path (str): relative path of session csv skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Default True. Display number of entries inserted when ingesting \"\"\" csvs = [ session_csv_path , session_csv_path , session_csv_path ] tables = [ session . Session (), session . SessionDirectory (), session . SessionNote ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose ) ingest_train_params ( config_params_csv_path , skip_duplicates = True , verbose = True ) \u00b6 Use provided path to load TrainingParamSet with relative path to config.yaml Parameters: Name Type Description Default config_params_csv_path str relative path of csv with config parameters required skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Default True. Display number of entries inserted when ingesting True Source code in workflow_deeplabcut/ingest.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def ingest_train_params ( config_params_csv_path , skip_duplicates = True , verbose = True ): \"\"\"Use provided path to load TrainingParamSet with relative path to config.yaml Args: config_params_csv_path (str): relative path of csv with config parameters skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Default True. Display number of entries inserted when ingesting \"\"\" if verbose : previous_length = len ( train . TrainingParamSet . fetch ()) with open ( config_params_csv_path , newline = \"\" ) as f : config_csv = list ( csv . DictReader ( f , delimiter = \",\" )) for line in config_csv : paramset_idx = line . pop ( \"paramset_idx\" ) if skip_duplicates and ( paramset_idx in list ( train . TrainingParamSet . fetch ( \"paramset_idx\" )) ): continue paramset_desc = line . pop ( \"paramset_desc\" ) try : config_path = find_full_path ( get_dlc_root_data_dir (), line . pop ( \"config_path\" ) ) except FileNotFoundError as e : if verbose : print ( f \"Skipping { paramset_desc } : \\n\\t { e } \" ) continue with open ( config_path , \"rb\" ) as y : params = yaml . safe_load ( y ) params . update ({ ** line }) train . TrainingParamSet . insert_new_params ( paramset_idx = paramset_idx , paramset_desc = paramset_desc , params = params ) if verbose : insert_length = len ( train . TrainingParamSet . fetch ()) - previous_length print ( f \" \\n ---- Inserting { insert_length } entry(s) into #model_training_param_set \" + \"----\" ) ingest_train_vids ( train_video_csv_path , verbose = False , ** kwargs ) \u00b6 Use provided CSV to insert into train.VideoSet and train.VideoSet.File Parameters: Name Type Description Default train_video_csv_path str relative path of csv with training video info required verbose bool Default False. Display number of entries inserted when ingesting False **kwargs dict Optional. Unused dict of keyword arguments. {} Source code in workflow_deeplabcut/ingest.py 86 87 88 89 90 91 92 93 94 95 96 97 def ingest_train_vids ( train_video_csv_path : str , verbose : bool = False , ** kwargs : dict ): \"\"\"Use provided CSV to insert into train.VideoSet and train.VideoSet.File Args: train_video_csv_path (str): relative path of csv with training video info verbose (bool): Default False. Display number of entries inserted when ingesting **kwargs (dict): Optional. Unused dict of keyword arguments. \"\"\" csvs = [ train_video_csv_path , train_video_csv_path ] tables = [ train . VideoSet (), train . VideoSet . File ()] # With current CSV organization, must skip vids, as primary key is duplicated ingest_csv_to_table ( csvs , tables , skip_duplicates = True , verbose = verbose ) ingest_model_vids ( model_video_csv_path , skip_duplicates = True , verbose = False ) \u00b6 Use provided CSV to insert into model.VideoRecording and VideoRecording.File Parameters: Name Type Description Default model_video_csv_path str relative path of csv with model video info required skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Default False. Display number of entries inserted when ingesting False Source code in workflow_deeplabcut/ingest.py 100 101 102 103 104 105 106 107 108 109 110 111 112 def ingest_model_vids ( model_video_csv_path : str , skip_duplicates : bool = True , verbose : bool = False ): \"\"\"Use provided CSV to insert into model.VideoRecording and VideoRecording.File Args: model_video_csv_path (str): relative path of csv with model video info skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Default False. Display number of entries inserted when ingesting \"\"\" csvs = [ model_video_csv_path , model_video_csv_path ] tables = [ model . VideoRecording (), model . VideoRecording . File ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose ) ingest_model ( model_model_csv_path , skip_duplicates = True , verbose = False ) \u00b6 Use provided CSV to insert into model.Model table Parameters: Name Type Description Default model_model_csv_path str relative path of csv with DLC Model info required skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Default False. Display number of entries inserted when ingesting False Source code in workflow_deeplabcut/ingest.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def ingest_model ( model_model_csv_path : str , skip_duplicates : bool = True , verbose : bool = False ): \"\"\"Use provided CSV to insert into model.Model table Args: model_model_csv_path (str): relative path of csv with DLC Model info skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Default False. Display number of entries inserted when ingesting \"\"\" # NOTE: not included in ingest_dlc_items because not yet included in notebooks with open ( model_model_csv_path , newline = \"\" ) as f : data = list ( csv . DictReader ( f , delimiter = \",\" )) if verbose : prev_len = len ( model . Model ()) for model_row in data : # replace relative path with full path model_row [ \"dlc_config\" ] = find_full_path ( get_dlc_root_data_dir (), model_row . pop ( \"config_relative_path\" ) ) model_row [ \"project_path\" ] = Path ( model_row [ \"dlc_config\" ]) . parent model_row [ \"prompt\" ] = str_to_bool ( model_row [ \"prompt\" ]) model_name = model_row [ \"model_name\" ] if skip_duplicates and model_name in model . Model . fetch ( \"model_name\" ): if verbose : print ( f \"Skipping model, name already exists: { model_name } \" ) continue else : model . Model . insert_new_model ( ** model_row ) if verbose : insert_len = len ( model . Model ()) - prev_len print ( f \" \\n ---- Inserting { insert_len } entry(s) into model ----\" ) ingest_dlc_items ( config_params_csv_path = './user_data/config_params.csv' , train_video_csv_path = './user_data/train_videosets.csv' , model_video_csv_path = './user_data/model_videos.csv' , skip_duplicates = False , verbose = True ) \u00b6 Ingests to DLC schema from CSVs Parameters: Name Type Description Default config_params_csv_path str Optional. Csv path for model training config and parameters './user_data/config_params.csv' train_video_csv_path str Optional. Csv path for list of training videosets './user_data/train_videosets.csv' model_video_csv_path str Optional. Csv path for list of modeling videos for pose estimation './user_data/model_videos.csv' Source code in workflow_deeplabcut/ingest.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def ingest_dlc_items ( config_params_csv_path : str = \"./user_data/config_params.csv\" , train_video_csv_path : str = \"./user_data/train_videosets.csv\" , model_video_csv_path : str = \"./user_data/model_videos.csv\" , skip_duplicates : bool = False , verbose : bool = True , ): \"\"\"Ingests to DLC schema from CSVs Args: config_params_csv_path (str): Optional. Csv path for model training config and parameters train_video_csv_path (str): Optional. Csv path for list of training videosets model_video_csv_path (str): Optional. Csv path for list of modeling videos for pose estimation \"\"\" ingest_train_params ( config_params_csv_path = config_params_csv_path , skip_duplicates = skip_duplicates , verbose = verbose , ) ingest_train_vids ( train_video_csv_path = train_video_csv_path , skip_duplicates = skip_duplicates , verbose = verbose , ) ingest_model_vids ( model_video_csv_path = model_video_csv_path , skip_duplicates = skip_duplicates , verbose = verbose , )", "title": "ingest.py"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_subjects", "text": "Inserts ./user_data/subject.csv data into corresponding subject schema tables Parameters: Name Type Description Default subject_csv_path str relative path of subject csv './user_data/subjects.csv' skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Display number of entries inserted when ingesting True Source code in workflow_deeplabcut/ingest.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def ingest_subjects ( subject_csv_path = \"./user_data/subjects.csv\" , skip_duplicates = True , verbose = True , ): \"\"\"Inserts ./user_data/subject.csv data into corresponding subject schema tables Args: subject_csv_path (str): relative path of subject csv skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Display number of entries inserted when ingesting \"\"\" csvs = [ subject_csv_path ] tables = [ subject . Subject ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose )", "title": "ingest_subjects()"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_sessions", "text": "Ingests to session schema from ./user_data/sessions.csv Parameters: Name Type Description Default session_csv_path str relative path of session csv './user_data/sessions.csv' skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Default True. Display number of entries inserted when ingesting True Source code in workflow_deeplabcut/ingest.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def ingest_sessions ( session_csv_path = \"./user_data/sessions.csv\" , skip_duplicates = True , verbose = True ): \"\"\"Ingests to session schema from ./user_data/sessions.csv Args: session_csv_path (str): relative path of session csv skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Default True. Display number of entries inserted when ingesting \"\"\" csvs = [ session_csv_path , session_csv_path , session_csv_path ] tables = [ session . Session (), session . SessionDirectory (), session . SessionNote ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose )", "title": "ingest_sessions()"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_train_params", "text": "Use provided path to load TrainingParamSet with relative path to config.yaml Parameters: Name Type Description Default config_params_csv_path str relative path of csv with config parameters required skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Default True. Display number of entries inserted when ingesting True Source code in workflow_deeplabcut/ingest.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def ingest_train_params ( config_params_csv_path , skip_duplicates = True , verbose = True ): \"\"\"Use provided path to load TrainingParamSet with relative path to config.yaml Args: config_params_csv_path (str): relative path of csv with config parameters skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Default True. Display number of entries inserted when ingesting \"\"\" if verbose : previous_length = len ( train . TrainingParamSet . fetch ()) with open ( config_params_csv_path , newline = \"\" ) as f : config_csv = list ( csv . DictReader ( f , delimiter = \",\" )) for line in config_csv : paramset_idx = line . pop ( \"paramset_idx\" ) if skip_duplicates and ( paramset_idx in list ( train . TrainingParamSet . fetch ( \"paramset_idx\" )) ): continue paramset_desc = line . pop ( \"paramset_desc\" ) try : config_path = find_full_path ( get_dlc_root_data_dir (), line . pop ( \"config_path\" ) ) except FileNotFoundError as e : if verbose : print ( f \"Skipping { paramset_desc } : \\n\\t { e } \" ) continue with open ( config_path , \"rb\" ) as y : params = yaml . safe_load ( y ) params . update ({ ** line }) train . TrainingParamSet . insert_new_params ( paramset_idx = paramset_idx , paramset_desc = paramset_desc , params = params ) if verbose : insert_length = len ( train . TrainingParamSet . fetch ()) - previous_length print ( f \" \\n ---- Inserting { insert_length } entry(s) into #model_training_param_set \" + \"----\" )", "title": "ingest_train_params()"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_train_vids", "text": "Use provided CSV to insert into train.VideoSet and train.VideoSet.File Parameters: Name Type Description Default train_video_csv_path str relative path of csv with training video info required verbose bool Default False. Display number of entries inserted when ingesting False **kwargs dict Optional. Unused dict of keyword arguments. {} Source code in workflow_deeplabcut/ingest.py 86 87 88 89 90 91 92 93 94 95 96 97 def ingest_train_vids ( train_video_csv_path : str , verbose : bool = False , ** kwargs : dict ): \"\"\"Use provided CSV to insert into train.VideoSet and train.VideoSet.File Args: train_video_csv_path (str): relative path of csv with training video info verbose (bool): Default False. Display number of entries inserted when ingesting **kwargs (dict): Optional. Unused dict of keyword arguments. \"\"\" csvs = [ train_video_csv_path , train_video_csv_path ] tables = [ train . VideoSet (), train . VideoSet . File ()] # With current CSV organization, must skip vids, as primary key is duplicated ingest_csv_to_table ( csvs , tables , skip_duplicates = True , verbose = verbose )", "title": "ingest_train_vids()"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_model_vids", "text": "Use provided CSV to insert into model.VideoRecording and VideoRecording.File Parameters: Name Type Description Default model_video_csv_path str relative path of csv with model video info required skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Default False. Display number of entries inserted when ingesting False Source code in workflow_deeplabcut/ingest.py 100 101 102 103 104 105 106 107 108 109 110 111 112 def ingest_model_vids ( model_video_csv_path : str , skip_duplicates : bool = True , verbose : bool = False ): \"\"\"Use provided CSV to insert into model.VideoRecording and VideoRecording.File Args: model_video_csv_path (str): relative path of csv with model video info skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Default False. Display number of entries inserted when ingesting \"\"\" csvs = [ model_video_csv_path , model_video_csv_path ] tables = [ model . VideoRecording (), model . VideoRecording . File ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose )", "title": "ingest_model_vids()"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_model", "text": "Use provided CSV to insert into model.Model table Parameters: Name Type Description Default model_model_csv_path str relative path of csv with DLC Model info required skip_duplicates bool Default True. Passed to DataJoint insert True verbose bool Default False. Display number of entries inserted when ingesting False Source code in workflow_deeplabcut/ingest.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def ingest_model ( model_model_csv_path : str , skip_duplicates : bool = True , verbose : bool = False ): \"\"\"Use provided CSV to insert into model.Model table Args: model_model_csv_path (str): relative path of csv with DLC Model info skip_duplicates (bool): Default True. Passed to DataJoint insert verbose (bool): Default False. Display number of entries inserted when ingesting \"\"\" # NOTE: not included in ingest_dlc_items because not yet included in notebooks with open ( model_model_csv_path , newline = \"\" ) as f : data = list ( csv . DictReader ( f , delimiter = \",\" )) if verbose : prev_len = len ( model . Model ()) for model_row in data : # replace relative path with full path model_row [ \"dlc_config\" ] = find_full_path ( get_dlc_root_data_dir (), model_row . pop ( \"config_relative_path\" ) ) model_row [ \"project_path\" ] = Path ( model_row [ \"dlc_config\" ]) . parent model_row [ \"prompt\" ] = str_to_bool ( model_row [ \"prompt\" ]) model_name = model_row [ \"model_name\" ] if skip_duplicates and model_name in model . Model . fetch ( \"model_name\" ): if verbose : print ( f \"Skipping model, name already exists: { model_name } \" ) continue else : model . Model . insert_new_model ( ** model_row ) if verbose : insert_len = len ( model . Model ()) - prev_len print ( f \" \\n ---- Inserting { insert_len } entry(s) into model ----\" )", "title": "ingest_model()"}, {"location": "api/workflow_deeplabcut/ingest/#workflow_deeplabcut.ingest.ingest_dlc_items", "text": "Ingests to DLC schema from CSVs Parameters: Name Type Description Default config_params_csv_path str Optional. Csv path for model training config and parameters './user_data/config_params.csv' train_video_csv_path str Optional. Csv path for list of training videosets './user_data/train_videosets.csv' model_video_csv_path str Optional. Csv path for list of modeling videos for pose estimation './user_data/model_videos.csv' Source code in workflow_deeplabcut/ingest.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def ingest_dlc_items ( config_params_csv_path : str = \"./user_data/config_params.csv\" , train_video_csv_path : str = \"./user_data/train_videosets.csv\" , model_video_csv_path : str = \"./user_data/model_videos.csv\" , skip_duplicates : bool = False , verbose : bool = True , ): \"\"\"Ingests to DLC schema from CSVs Args: config_params_csv_path (str): Optional. Csv path for model training config and parameters train_video_csv_path (str): Optional. Csv path for list of training videosets model_video_csv_path (str): Optional. Csv path for list of modeling videos for pose estimation \"\"\" ingest_train_params ( config_params_csv_path = config_params_csv_path , skip_duplicates = skip_duplicates , verbose = verbose , ) ingest_train_vids ( train_video_csv_path = train_video_csv_path , skip_duplicates = skip_duplicates , verbose = verbose , ) ingest_model_vids ( model_video_csv_path = model_video_csv_path , skip_duplicates = skip_duplicates , verbose = verbose , )", "title": "ingest_dlc_items()"}, {"location": "api/workflow_deeplabcut/load_demo_data/", "text": "download_djarchive_dlc_data ( target_directory = '/tmp/test_data/' ) \u00b6 Download DLC demo data from djarchive. Approx .3 GB Parameters: Name Type Description Default target_directory str Where to store the downloaded data. '/tmp/test_data/' Source code in workflow_deeplabcut/load_demo_data.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def download_djarchive_dlc_data ( target_directory : str = \"/tmp/test_data/\" ): \"\"\"Download DLC demo data from djarchive. Approx .3 GB Args: target_directory (str, optional): Where to store the downloaded data. \"\"\" import djarchive_client client = djarchive_client . client () os . makedirs ( target_directory , exist_ok = True ) client . download ( \"workflow-dlc-data\" , target_directory = target_directory , revision = \"v1\" ) update_pose_cfg ( project = 'from_top_tracking' , net_type = None , update_snapshot = 0 ) \u00b6 Updates weight paths to absolute. If update_snapshot, changes weights to snap # Parameters: Name Type Description Default project str Default from 'from_top_tracking'. Poject name/folder in dlc_root_data_dir 'from_top_tracking' net_type str Project net weights (e.g., resnet50) If project is 'from_top_tracking', 'mobilenet_v2_1.0' None update_snapshot int Default 0 = no. If -1, highest integer value available. If integer, look for that snapshot. 0 Source code in workflow_deeplabcut/load_demo_data.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def update_pose_cfg ( project : str = \"from_top_tracking\" , net_type : str = None , update_snapshot : int = 0 ): \"\"\"Updates weight paths to absolute. If update_snapshot, changes weights to snap # Args: project (str, optional): Default from 'from_top_tracking'. Poject name/folder in dlc_root_data_dir net_type (str, optional): Project net weights (e.g., resnet50) If project is 'from_top_tracking', 'mobilenet_v2_1.0' update_snapshot (int, optional): Default 0 = no. If -1, highest integer value available. If integer, look for that snapshot. \"\"\" project_path = find_full_path ( get_dlc_root_data_dir (), f \" { project } /\" ) if project == \"from_top_tracking\" : net_type == \"mobilenet_v2_1.0\" for phase in [ \"test\" , \"train\" ]: config_search = list ( project_path . rglob ( f \" { phase } /pose_cfg.yaml\" )) if not config_search : print ( f \"Couldn't find cfg for { phase } \" ) config_path = config_search [ 0 ] cfg = read_plainconfig ( config_path ) if update_snapshot and phase == \"train\" : # Get available snapshots snaps_on_disk = set ( [ int ( i . split ( \"-\" )[ 1 ]) for i in [ f . stem for f in list ( project_path . rglob ( \"snapshot-*\" ))] ] ) # If -1, take most recent if update_snapshot == - 1 : update_snapshot = snaps_on_disk . pop () # last in sorted set else : # Assert desired snapshot is available assert ( update_snapshot in snaps_on_disk ), f \"Couldn't find snapshot { update_snapshot } in { config_path . parent } \" # Set snaphot value cfg [ \"init_weights\" ] = str ( config_path . parent / f \"snapshot- { update_snapshot } \" ) else : init_weights = Path ( cfg [ \"init_weights\" ] ) # e.g., path/to/snapshot-1 (no ext) cfg [ \"init_weights\" ] = str ( find_full_path ( get_deeplabcut_path (), init_weights . parent ) / init_weights . name # need parent/name bc it isn't on disk ) if net_type : # if net_type explicitly provided, update cfg [ \"net_type\" ] = net_type # For train, pull datatype_set for next function if phase == \"train\" : augmenter_type = cfg . get ( \"dataset_type\" ) write_plainconfig ( config_path , cfg ) return augmenter_type setup_bare_project ( project = 'from_top_tracking' , net_type = None ) \u00b6 Adds absolute paths to config files and generates training-datasets folder Parameters: Name Type Description Default project str Default 'from_top_tracking'. DLC project folder 'from_top_tracking' net_type str Project net (e.g., resnet50) passed to creat_training_dataset. If 'from_top', 'mobilenet_v2_1.0' None Source code in workflow_deeplabcut/load_demo_data.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def setup_bare_project ( project : str = \"from_top_tracking\" , net_type : str = None ): \"\"\"Adds absolute paths to config files and generates training-datasets folder Args: project (str, optional): Default 'from_top_tracking'. DLC project folder net_type (str, optional): Project net (e.g., resnet50) passed to creat_training_dataset. If 'from_top', 'mobilenet_v2_1.0' \"\"\" from deeplabcut import create_training_dataset if \"from_top_tracking\" in project : # set net_type for example data net_type = \"mobilenet_v2_1.0\" if os . path . isabs ( project ) and Path ( project ) . exists (): project_path = Path ( project ) else : project_path = find_full_path ( get_dlc_root_data_dir (), f \" { project } /\" ) # ---- Write roots to project config ---- project_config_path = project_path / \"config.yaml\" project_cfg = read_plainconfig ( project_config_path ) project_cfg [ \"project_path\" ] = str ( project_path ) cfg_videoset_paths = {} # separate to not change during loop for video , value in project_cfg [ \"video_sets\" ] . items (): # add absolute video path cfg_videoset_paths [ os . path . join ( project_path , video )] = value project_cfg [ \"video_sets\" ] = cfg_videoset_paths # save new fullpaths write_plainconfig ( project_config_path , project_cfg ) # Update train/test pose_cfg, return augmenter type augmenter_type = update_pose_cfg ( project = project , net_type = net_type , update_snapshot =- 1 ) # ---- Create training dataset ---- # Folder deleted from publicly available data to cut down on size _ = create_training_dataset ( project_config_path , num_shuffles = 1 , net_type = net_type , augmenter_type = augmenter_type , posecfg_template = str ( next ( Path ( project_path ) . rglob ( \"train/pose_cfg.y?ml\" ))), ) shorten_video ( vid_path = 'from_top_tracking/videos/test.mp4' , output_path = None , first_n_sec = 2 ) \u00b6 Save the first 2 seconds of a video relative to dlc root dir. Parameters: Name Type Description Default vid_path str Default \"from_top_tracking/videos/test_full.mp4\". Path relative to get_dlc_root_data_dir() root directory 'from_top_tracking/videos/test.mp4' output_path str Destination relative to vid_path root. If none, adds '-Ns' to filename, where N in first_n_sec None first_n_sec int Default 2. # of seconds to extract from beginning of video 2 Source code in workflow_deeplabcut/load_demo_data.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def shorten_video ( vid_path : str = \"from_top_tracking/videos/test.mp4\" , output_path : str = None , first_n_sec : int = 2 , ): \"\"\"Save the first 2 seconds of a video relative to dlc root dir. Args: vid_path (str, optional): Default \"from_top_tracking/videos/test_full.mp4\". Path relative to get_dlc_root_data_dir() root directory output_path (str, optional): Destination relative to vid_path root. If none, adds '-Ns' to filename, where N in first_n_sec first_n_sec (int, optional): Default 2. # of seconds to extract from beginning of video \"\"\" if os . path . isabs ( vid_path ) and Path ( vid_path ) . exists (): vid_path_full = Path ( vid_path ) else : vid_path_full = find_full_path ( get_dlc_root_data_dir (), vid_path ) if not output_path : output_path_full = vid_path_full . with_name ( vid_path_full . stem + f \"- { first_n_sec } s\" + vid_path_full . suffix ) else : output_path_full = ( find_root_directory ( get_dlc_root_data_dir (), vid_path_full ) / output_path ) cmd = ( # adjust -ss 0 to start later f \"ffmpeg -n -hide_banner -loglevel error -ss 0 -t { first_n_sec } -i \" + f \" { vid_path_full } -vcodec copy -acodec copy { output_path_full } \" ) _ = os . system ( cmd ) revert_checkpoint_file ( project = 'from_top_tracking' , original_checkpoint = 'checkpoint_orig' ) \u00b6 Delete existing checkpoint file and replace with original_checkpoint Parameters: Name Type Description Default project str DLC project name. Defaults to \"from_top_tracking\". 'from_top_tracking' original_checkpoint str Original checkpoint file ot use in the revert, in the same directory as the checkpoint file. Defaults to \"checkpoint_orig\". 'checkpoint_orig' Source code in workflow_deeplabcut/load_demo_data.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def revert_checkpoint_file ( project = \"from_top_tracking\" , original_checkpoint = \"checkpoint_orig\" ): \"\"\"Delete existing checkpoint file and replace with original_checkpoint Args: project (str, optional): DLC project name. Defaults to \"from_top_tracking\". original_checkpoint (str, optional): Original checkpoint file ot use in the revert, in the same directory as the checkpoint file. Defaults to \"checkpoint_orig\". \"\"\" import shutil project_path = find_full_path ( get_dlc_root_data_dir (), f \" { project } /\" ) original_checkpoint_path = list ( project_path . rglob ( original_checkpoint )) assert ( len ( original_checkpoint_path ) == 1 ), f \"Found more than one original checkpoint: \\n { original_checkpoint_path } \" shutil . copy ( original_checkpoint_path [ 0 ], original_checkpoint_path [ 0 ] . parent / \"checkpoint\" )", "title": "load_demo_data.py"}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.download_djarchive_dlc_data", "text": "Download DLC demo data from djarchive. Approx .3 GB Parameters: Name Type Description Default target_directory str Where to store the downloaded data. '/tmp/test_data/' Source code in workflow_deeplabcut/load_demo_data.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def download_djarchive_dlc_data ( target_directory : str = \"/tmp/test_data/\" ): \"\"\"Download DLC demo data from djarchive. Approx .3 GB Args: target_directory (str, optional): Where to store the downloaded data. \"\"\" import djarchive_client client = djarchive_client . client () os . makedirs ( target_directory , exist_ok = True ) client . download ( \"workflow-dlc-data\" , target_directory = target_directory , revision = \"v1\" )", "title": "download_djarchive_dlc_data()"}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.update_pose_cfg", "text": "Updates weight paths to absolute. If update_snapshot, changes weights to snap # Parameters: Name Type Description Default project str Default from 'from_top_tracking'. Poject name/folder in dlc_root_data_dir 'from_top_tracking' net_type str Project net weights (e.g., resnet50) If project is 'from_top_tracking', 'mobilenet_v2_1.0' None update_snapshot int Default 0 = no. If -1, highest integer value available. If integer, look for that snapshot. 0 Source code in workflow_deeplabcut/load_demo_data.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def update_pose_cfg ( project : str = \"from_top_tracking\" , net_type : str = None , update_snapshot : int = 0 ): \"\"\"Updates weight paths to absolute. If update_snapshot, changes weights to snap # Args: project (str, optional): Default from 'from_top_tracking'. Poject name/folder in dlc_root_data_dir net_type (str, optional): Project net weights (e.g., resnet50) If project is 'from_top_tracking', 'mobilenet_v2_1.0' update_snapshot (int, optional): Default 0 = no. If -1, highest integer value available. If integer, look for that snapshot. \"\"\" project_path = find_full_path ( get_dlc_root_data_dir (), f \" { project } /\" ) if project == \"from_top_tracking\" : net_type == \"mobilenet_v2_1.0\" for phase in [ \"test\" , \"train\" ]: config_search = list ( project_path . rglob ( f \" { phase } /pose_cfg.yaml\" )) if not config_search : print ( f \"Couldn't find cfg for { phase } \" ) config_path = config_search [ 0 ] cfg = read_plainconfig ( config_path ) if update_snapshot and phase == \"train\" : # Get available snapshots snaps_on_disk = set ( [ int ( i . split ( \"-\" )[ 1 ]) for i in [ f . stem for f in list ( project_path . rglob ( \"snapshot-*\" ))] ] ) # If -1, take most recent if update_snapshot == - 1 : update_snapshot = snaps_on_disk . pop () # last in sorted set else : # Assert desired snapshot is available assert ( update_snapshot in snaps_on_disk ), f \"Couldn't find snapshot { update_snapshot } in { config_path . parent } \" # Set snaphot value cfg [ \"init_weights\" ] = str ( config_path . parent / f \"snapshot- { update_snapshot } \" ) else : init_weights = Path ( cfg [ \"init_weights\" ] ) # e.g., path/to/snapshot-1 (no ext) cfg [ \"init_weights\" ] = str ( find_full_path ( get_deeplabcut_path (), init_weights . parent ) / init_weights . name # need parent/name bc it isn't on disk ) if net_type : # if net_type explicitly provided, update cfg [ \"net_type\" ] = net_type # For train, pull datatype_set for next function if phase == \"train\" : augmenter_type = cfg . get ( \"dataset_type\" ) write_plainconfig ( config_path , cfg ) return augmenter_type", "title": "update_pose_cfg()"}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.setup_bare_project", "text": "Adds absolute paths to config files and generates training-datasets folder Parameters: Name Type Description Default project str Default 'from_top_tracking'. DLC project folder 'from_top_tracking' net_type str Project net (e.g., resnet50) passed to creat_training_dataset. If 'from_top', 'mobilenet_v2_1.0' None Source code in workflow_deeplabcut/load_demo_data.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def setup_bare_project ( project : str = \"from_top_tracking\" , net_type : str = None ): \"\"\"Adds absolute paths to config files and generates training-datasets folder Args: project (str, optional): Default 'from_top_tracking'. DLC project folder net_type (str, optional): Project net (e.g., resnet50) passed to creat_training_dataset. If 'from_top', 'mobilenet_v2_1.0' \"\"\" from deeplabcut import create_training_dataset if \"from_top_tracking\" in project : # set net_type for example data net_type = \"mobilenet_v2_1.0\" if os . path . isabs ( project ) and Path ( project ) . exists (): project_path = Path ( project ) else : project_path = find_full_path ( get_dlc_root_data_dir (), f \" { project } /\" ) # ---- Write roots to project config ---- project_config_path = project_path / \"config.yaml\" project_cfg = read_plainconfig ( project_config_path ) project_cfg [ \"project_path\" ] = str ( project_path ) cfg_videoset_paths = {} # separate to not change during loop for video , value in project_cfg [ \"video_sets\" ] . items (): # add absolute video path cfg_videoset_paths [ os . path . join ( project_path , video )] = value project_cfg [ \"video_sets\" ] = cfg_videoset_paths # save new fullpaths write_plainconfig ( project_config_path , project_cfg ) # Update train/test pose_cfg, return augmenter type augmenter_type = update_pose_cfg ( project = project , net_type = net_type , update_snapshot =- 1 ) # ---- Create training dataset ---- # Folder deleted from publicly available data to cut down on size _ = create_training_dataset ( project_config_path , num_shuffles = 1 , net_type = net_type , augmenter_type = augmenter_type , posecfg_template = str ( next ( Path ( project_path ) . rglob ( \"train/pose_cfg.y?ml\" ))), )", "title": "setup_bare_project()"}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.shorten_video", "text": "Save the first 2 seconds of a video relative to dlc root dir. Parameters: Name Type Description Default vid_path str Default \"from_top_tracking/videos/test_full.mp4\". Path relative to get_dlc_root_data_dir() root directory 'from_top_tracking/videos/test.mp4' output_path str Destination relative to vid_path root. If none, adds '-Ns' to filename, where N in first_n_sec None first_n_sec int Default 2. # of seconds to extract from beginning of video 2 Source code in workflow_deeplabcut/load_demo_data.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def shorten_video ( vid_path : str = \"from_top_tracking/videos/test.mp4\" , output_path : str = None , first_n_sec : int = 2 , ): \"\"\"Save the first 2 seconds of a video relative to dlc root dir. Args: vid_path (str, optional): Default \"from_top_tracking/videos/test_full.mp4\". Path relative to get_dlc_root_data_dir() root directory output_path (str, optional): Destination relative to vid_path root. If none, adds '-Ns' to filename, where N in first_n_sec first_n_sec (int, optional): Default 2. # of seconds to extract from beginning of video \"\"\" if os . path . isabs ( vid_path ) and Path ( vid_path ) . exists (): vid_path_full = Path ( vid_path ) else : vid_path_full = find_full_path ( get_dlc_root_data_dir (), vid_path ) if not output_path : output_path_full = vid_path_full . with_name ( vid_path_full . stem + f \"- { first_n_sec } s\" + vid_path_full . suffix ) else : output_path_full = ( find_root_directory ( get_dlc_root_data_dir (), vid_path_full ) / output_path ) cmd = ( # adjust -ss 0 to start later f \"ffmpeg -n -hide_banner -loglevel error -ss 0 -t { first_n_sec } -i \" + f \" { vid_path_full } -vcodec copy -acodec copy { output_path_full } \" ) _ = os . system ( cmd )", "title": "shorten_video()"}, {"location": "api/workflow_deeplabcut/load_demo_data/#workflow_deeplabcut.load_demo_data.revert_checkpoint_file", "text": "Delete existing checkpoint file and replace with original_checkpoint Parameters: Name Type Description Default project str DLC project name. Defaults to \"from_top_tracking\". 'from_top_tracking' original_checkpoint str Original checkpoint file ot use in the revert, in the same directory as the checkpoint file. Defaults to \"checkpoint_orig\". 'checkpoint_orig' Source code in workflow_deeplabcut/load_demo_data.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def revert_checkpoint_file ( project = \"from_top_tracking\" , original_checkpoint = \"checkpoint_orig\" ): \"\"\"Delete existing checkpoint file and replace with original_checkpoint Args: project (str, optional): DLC project name. Defaults to \"from_top_tracking\". original_checkpoint (str, optional): Original checkpoint file ot use in the revert, in the same directory as the checkpoint file. Defaults to \"checkpoint_orig\". \"\"\" import shutil project_path = find_full_path ( get_dlc_root_data_dir (), f \" { project } /\" ) original_checkpoint_path = list ( project_path . rglob ( original_checkpoint )) assert ( len ( original_checkpoint_path ) == 1 ), f \"Found more than one original checkpoint: \\n { original_checkpoint_path } \" shutil . copy ( original_checkpoint_path [ 0 ], original_checkpoint_path [ 0 ] . parent / \"checkpoint\" )", "title": "revert_checkpoint_file()"}, {"location": "api/workflow_deeplabcut/paths/", "text": "get_dlc_root_data_dir () \u00b6 Returns a list of root directories for Element DeepLabCut Source code in workflow_deeplabcut/paths.py 5 6 7 8 9 10 11 12 13 def get_dlc_root_data_dir () -> list : \"\"\"Returns a list of root directories for Element DeepLabCut\"\"\" dlc_root_dirs = dj . config . get ( \"custom\" , {}) . get ( \"dlc_root_data_dir\" ) if not dlc_root_dirs : return None elif not isinstance ( dlc_root_dirs , abc . Sequence ): return list ( dlc_root_dirs ) else : return dlc_root_dirs get_dlc_processed_data_dir () \u00b6 Returns an output directory relative to custom 'dlc_output_dir' root Source code in workflow_deeplabcut/paths.py 16 17 18 19 20 21 22 23 24 def get_dlc_processed_data_dir () -> str : \"\"\"Returns an output directory relative to custom 'dlc_output_dir' root\"\"\" from pathlib import Path dlc_output_dir = dj . config . get ( \"custom\" , {}) . get ( \"dlc_output_dir\" ) if dlc_output_dir : return Path ( dlc_output_dir ) else : return None", "title": "paths.py"}, {"location": "api/workflow_deeplabcut/paths/#workflow_deeplabcut.paths.get_dlc_root_data_dir", "text": "Returns a list of root directories for Element DeepLabCut Source code in workflow_deeplabcut/paths.py 5 6 7 8 9 10 11 12 13 def get_dlc_root_data_dir () -> list : \"\"\"Returns a list of root directories for Element DeepLabCut\"\"\" dlc_root_dirs = dj . config . get ( \"custom\" , {}) . get ( \"dlc_root_data_dir\" ) if not dlc_root_dirs : return None elif not isinstance ( dlc_root_dirs , abc . Sequence ): return list ( dlc_root_dirs ) else : return dlc_root_dirs", "title": "get_dlc_root_data_dir()"}, {"location": "api/workflow_deeplabcut/paths/#workflow_deeplabcut.paths.get_dlc_processed_data_dir", "text": "Returns an output directory relative to custom 'dlc_output_dir' root Source code in workflow_deeplabcut/paths.py 16 17 18 19 20 21 22 23 24 def get_dlc_processed_data_dir () -> str : \"\"\"Returns an output directory relative to custom 'dlc_output_dir' root\"\"\" from pathlib import Path dlc_output_dir = dj . config . get ( \"custom\" , {}) . get ( \"dlc_output_dir\" ) if dlc_output_dir : return Path ( dlc_output_dir ) else : return None", "title": "get_dlc_processed_data_dir()"}, {"location": "api/workflow_deeplabcut/pipeline/", "text": "get_dlc_root_data_dir () \u00b6 Returns a list of root directories for Element DeepLabCut Source code in workflow_deeplabcut/paths.py 5 6 7 8 9 10 11 12 13 def get_dlc_root_data_dir () -> list : \"\"\"Returns a list of root directories for Element DeepLabCut\"\"\" dlc_root_dirs = dj . config . get ( \"custom\" , {}) . get ( \"dlc_root_data_dir\" ) if not dlc_root_dirs : return None elif not isinstance ( dlc_root_dirs , abc . Sequence ): return list ( dlc_root_dirs ) else : return dlc_root_dirs get_dlc_processed_data_dir () \u00b6 Returns an output directory relative to custom 'dlc_output_dir' root Source code in workflow_deeplabcut/paths.py 16 17 18 19 20 21 22 23 24 def get_dlc_processed_data_dir () -> str : \"\"\"Returns an output directory relative to custom 'dlc_output_dir' root\"\"\" from pathlib import Path dlc_output_dir = dj . config . get ( \"custom\" , {}) . get ( \"dlc_output_dir\" ) if dlc_output_dir : return Path ( dlc_output_dir ) else : return None Device \u00b6 Bases: dj . Lookup Table for managing lab equipment. In Element DeepLabCut, this table is referenced by model.VideoRecording . The primary key is also used to generate inferred output directories when running pose estimation inference. Refer to the definition attribute for the table design. Attributes: Name Type Description device varchar(32) Device short name. modality varchar(64) Modality for which this device is used. description varchar(256) Optional. Description of device. Source code in workflow_deeplabcut/pipeline.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @lab . schema class Device ( dj . Lookup ): \"\"\"Table for managing lab equipment. In Element DeepLabCut, this table is referenced by `model.VideoRecording`. The primary key is also used to generate inferred output directories when running pose estimation inference. Refer to the `definition` attribute for the table design. Attributes: device ( varchar(32) ): Device short name. modality ( varchar(64) ): Modality for which this device is used. description ( varchar(256) ): Optional. Description of device. \"\"\" definition = \"\"\" device : varchar(32) --- modality : varchar(64) description=null : varchar(256) \"\"\" contents = [ [ \"Camera1\" , \"Pose Estimation\" , \"Panasonic HC-V380K\" ], [ \"Camera2\" , \"Pose Estimation\" , \"Panasonic HC-V770K\" ], ]", "title": "pipeline.py"}, {"location": "api/workflow_deeplabcut/pipeline/#workflow_deeplabcut.pipeline.get_dlc_root_data_dir", "text": "Returns a list of root directories for Element DeepLabCut Source code in workflow_deeplabcut/paths.py 5 6 7 8 9 10 11 12 13 def get_dlc_root_data_dir () -> list : \"\"\"Returns a list of root directories for Element DeepLabCut\"\"\" dlc_root_dirs = dj . config . get ( \"custom\" , {}) . get ( \"dlc_root_data_dir\" ) if not dlc_root_dirs : return None elif not isinstance ( dlc_root_dirs , abc . Sequence ): return list ( dlc_root_dirs ) else : return dlc_root_dirs", "title": "get_dlc_root_data_dir()"}, {"location": "api/workflow_deeplabcut/pipeline/#workflow_deeplabcut.pipeline.get_dlc_processed_data_dir", "text": "Returns an output directory relative to custom 'dlc_output_dir' root Source code in workflow_deeplabcut/paths.py 16 17 18 19 20 21 22 23 24 def get_dlc_processed_data_dir () -> str : \"\"\"Returns an output directory relative to custom 'dlc_output_dir' root\"\"\" from pathlib import Path dlc_output_dir = dj . config . get ( \"custom\" , {}) . get ( \"dlc_output_dir\" ) if dlc_output_dir : return Path ( dlc_output_dir ) else : return None", "title": "get_dlc_processed_data_dir()"}, {"location": "api/workflow_deeplabcut/pipeline/#workflow_deeplabcut.pipeline.Device", "text": "Bases: dj . Lookup Table for managing lab equipment. In Element DeepLabCut, this table is referenced by model.VideoRecording . The primary key is also used to generate inferred output directories when running pose estimation inference. Refer to the definition attribute for the table design. Attributes: Name Type Description device varchar(32) Device short name. modality varchar(64) Modality for which this device is used. description varchar(256) Optional. Description of device. Source code in workflow_deeplabcut/pipeline.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @lab . schema class Device ( dj . Lookup ): \"\"\"Table for managing lab equipment. In Element DeepLabCut, this table is referenced by `model.VideoRecording`. The primary key is also used to generate inferred output directories when running pose estimation inference. Refer to the `definition` attribute for the table design. Attributes: device ( varchar(32) ): Device short name. modality ( varchar(64) ): Modality for which this device is used. description ( varchar(256) ): Optional. Description of device. \"\"\" definition = \"\"\" device : varchar(32) --- modality : varchar(64) description=null : varchar(256) \"\"\" contents = [ [ \"Camera1\" , \"Pose Estimation\" , \"Panasonic HC-V380K\" ], [ \"Camera2\" , \"Pose Estimation\" , \"Panasonic HC-V770K\" ], ]", "title": "Device"}, {"location": "api/workflow_deeplabcut/process/", "text": "QuietStdOut \u00b6 Context for suppressing standard output Source code in workflow_deeplabcut/process.py 7 8 9 10 11 12 13 14 15 16 class QuietStdOut : \"\"\"Context for suppressing standard output\"\"\" def __enter__ ( self ): self . _original_stdout = sys . stdout sys . stdout = open ( os . devnull , \"w\" ) def __exit__ ( self , exc_type , exc_val , exc_tb ): sys . stdout . close () sys . stdout = self . _original_stdout run ( verbose = True , display_progress = True , reserve_jobs = False , suppress_errors = False ) \u00b6 Run all make methods from element-deeplabcut Parameters: Name Type Description Default verbose bool Print which table is in being populated. Default True. True display_progress bool tqdm progress bar. Defaults to True. True reserve_jobs bool Reserves job to populate in asynchronous fashion. Defaults to False. False suppress_errors bool Suppress errors that would halt execution. Defaults to False. False Source code in workflow_deeplabcut/process.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def run ( verbose : bool = True , display_progress : bool = True , reserve_jobs : bool = False , suppress_errors : bool = False , ): \"\"\"Run all `make` methods from element-deeplabcut Args: verbose (bool, optional): Print which table is in being populated. Default True. display_progress (bool, optional): tqdm progress bar. Defaults to True. reserve_jobs (bool, optional): Reserves job to populate in asynchronous fashion. Defaults to False. suppress_errors (bool, optional): Suppress errors that would halt execution. Defaults to False. \"\"\" populate_settings = { \"display_progress\" : display_progress , \"reserve_jobs\" : reserve_jobs , \"suppress_errors\" : suppress_errors , } tables = [ train . ModelTraining (), model . RecordingInfo (), model . ModelEvaluation (), model . PoseEstimation (), ] with nullcontext () if verbose else QuietStdOut (): for table in tables : print ( f \" \\n ---- Populating { table . table_name } ----\" ) table . populate ( ** populate_settings )", "title": "process.py"}, {"location": "api/workflow_deeplabcut/process/#workflow_deeplabcut.process.QuietStdOut", "text": "Context for suppressing standard output Source code in workflow_deeplabcut/process.py 7 8 9 10 11 12 13 14 15 16 class QuietStdOut : \"\"\"Context for suppressing standard output\"\"\" def __enter__ ( self ): self . _original_stdout = sys . stdout sys . stdout = open ( os . devnull , \"w\" ) def __exit__ ( self , exc_type , exc_val , exc_tb ): sys . stdout . close () sys . stdout = self . _original_stdout", "title": "QuietStdOut"}, {"location": "api/workflow_deeplabcut/process/#workflow_deeplabcut.process.run", "text": "Run all make methods from element-deeplabcut Parameters: Name Type Description Default verbose bool Print which table is in being populated. Default True. True display_progress bool tqdm progress bar. Defaults to True. True reserve_jobs bool Reserves job to populate in asynchronous fashion. Defaults to False. False suppress_errors bool Suppress errors that would halt execution. Defaults to False. False Source code in workflow_deeplabcut/process.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def run ( verbose : bool = True , display_progress : bool = True , reserve_jobs : bool = False , suppress_errors : bool = False , ): \"\"\"Run all `make` methods from element-deeplabcut Args: verbose (bool, optional): Print which table is in being populated. Default True. display_progress (bool, optional): tqdm progress bar. Defaults to True. reserve_jobs (bool, optional): Reserves job to populate in asynchronous fashion. Defaults to False. suppress_errors (bool, optional): Suppress errors that would halt execution. Defaults to False. \"\"\" populate_settings = { \"display_progress\" : display_progress , \"reserve_jobs\" : reserve_jobs , \"suppress_errors\" : suppress_errors , } tables = [ train . ModelTraining (), model . RecordingInfo (), model . ModelEvaluation (), model . PoseEstimation (), ] with nullcontext () if verbose else QuietStdOut (): for table in tables : print ( f \" \\n ---- Populating { table . table_name } ----\" ) table . populate ( ** populate_settings )", "title": "run()"}, {"location": "api/workflow_deeplabcut/version/", "text": "Package metadata Update the Docker image tag in docker-compose.yaml to match", "title": "version.py"}]}